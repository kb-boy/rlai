{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J-S64_dTGj1m",
        "outputId": "23e58468-1e7b-4b50-87df-543b13328355"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training REINFORCE using Advantage Function...\n",
            "Episode 500/2000 completed.\n",
            "Episode 1000/2000 completed.\n",
            "Episode 1500/2000 completed.\n",
            "Episode 2000/2000 completed.\n",
            "\n",
            "Final Policy (Advantage Method):\n",
            "-----------------\n",
            " T | ← | ← | ← \n",
            "-----------------\n",
            " ↑ | ↑ | ← | ↓ \n",
            "-----------------\n",
            " ↑ | → | → | ↓ \n",
            "-----------------\n",
            " ↑ | → | → | T \n",
            "-----------------\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# --- 1. ENVIRONMENT (4x4 Grid World) ---\n",
        "class GridWorld:\n",
        "    def __init__(self):\n",
        "        self.grid_size = 4\n",
        "        self.terminal_states = [0, 15]\n",
        "        self.actions = [0, 1, 2, 3] # UP, DOWN, LEFT, RIGHT\n",
        "\n",
        "    def step(self, state, action):\n",
        "        if state in self.terminal_states:\n",
        "            return state, 0, True\n",
        "\n",
        "        row, col = divmod(state, self.grid_size)\n",
        "        if action == 0:   row = max(row - 1, 0)\n",
        "        elif action == 1: row = min(row + 1, self.grid_size - 1)\n",
        "        elif action == 2: col = max(col - 1, 0)\n",
        "        elif action == 3: col = min(col + 1, self.grid_size - 1)\n",
        "\n",
        "        next_state = row * self.grid_size + col\n",
        "        reward = -1\n",
        "        done = next_state in self.terminal_states\n",
        "        return next_state, reward, done\n",
        "\n",
        "    def reset(self):\n",
        "        start_state = np.random.randint(0, 16)\n",
        "        while start_state in self.terminal_states:\n",
        "            start_state = np.random.randint(0, 16)\n",
        "        return start_state\n",
        "\n",
        "# --- 2. NEURAL NETWORKS ---\n",
        "class PolicyNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(PolicyNetwork, self).__init__()\n",
        "        self.fc1 = nn.Linear(16, 128)\n",
        "        self.fc2 = nn.Linear(128, 4)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return F.softmax(x, dim=-1)\n",
        "\n",
        "class ValueNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ValueNetwork, self).__init__()\n",
        "        self.fc1 = nn.Linear(16, 128)\n",
        "        self.fc2 = nn.Linear(128, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        return self.fc2(x)\n",
        "\n",
        "def state_to_tensor(state):\n",
        "    v = torch.zeros(16)\n",
        "    v[state] = 1.0\n",
        "    return v.unsqueeze(0)\n",
        "\n",
        "# --- 3. REINFORCE WITH ADVANTAGE ALGORITHM ---\n",
        "def train_reinforce_advantage():\n",
        "    env = GridWorld()\n",
        "\n",
        "    # Initialize Actor (Policy) and Critic (Value)\n",
        "    policy_net = PolicyNetwork()\n",
        "    value_net = ValueNetwork()\n",
        "\n",
        "    # Use small learning rate and gradient clipping for stability\n",
        "    policy_optimizer = optim.Adam(policy_net.parameters(), lr=0.0005)\n",
        "    value_optimizer = optim.Adam(value_net.parameters(), lr=0.0005)\n",
        "\n",
        "    num_episodes = 2000\n",
        "    gamma = 0.99\n",
        "\n",
        "    print(\"Training REINFORCE using Advantage Function...\")\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "\n",
        "        log_probs = []\n",
        "        values = []\n",
        "        rewards = []\n",
        "\n",
        "        # --- A. Collect Trajectory (Monte Carlo) ---\n",
        "        while not done:\n",
        "            state_t = state_to_tensor(state)\n",
        "\n",
        "            # 1. Get Policy prob and Value estimate\n",
        "            probs = policy_net(state_t)\n",
        "            value = value_net(state_t)\n",
        "\n",
        "            # 2. Sample Action\n",
        "            dist = torch.distributions.Categorical(probs)\n",
        "            action = dist.sample()\n",
        "\n",
        "            # 3. Take Step\n",
        "            next_state, reward, done = env.step(state, action.item())\n",
        "\n",
        "            log_probs.append(dist.log_prob(action))\n",
        "            values.append(value)\n",
        "            rewards.append(reward)\n",
        "\n",
        "            state = next_state\n",
        "            if len(rewards) > 100: break # Safety break\n",
        "\n",
        "        # --- B. Calculate Returns (G_t) ---\n",
        "        returns = []\n",
        "        G = 0\n",
        "        for r in reversed(rewards):\n",
        "            G = r + gamma * G\n",
        "            returns.insert(0, G)\n",
        "        returns = torch.tensor(returns, dtype=torch.float32)\n",
        "\n",
        "        # Normalize returns for numerical stability\n",
        "        if len(returns) > 1:\n",
        "            returns = (returns - returns.mean()) / (returns.std() + 1e-9)\n",
        "        else:\n",
        "            returns = returns - returns.mean()\n",
        "\n",
        "        # --- C. Calculate Advantage & Update ---\n",
        "        policy_loss = []\n",
        "        value_loss = []\n",
        "\n",
        "        for log_prob, value, G_t in zip(log_probs, values, returns):\n",
        "            # THE ADVANTAGE FUNCTION: A(s,a) = G_t - V(s)\n",
        "            # We detach() value because we don't want to update the ValueNet based on Policy loss\n",
        "            advantage = G_t - value.item()\n",
        "\n",
        "            # Policy Update: Increase prob of actions with positive Advantage\n",
        "            policy_loss.append(-log_prob * advantage)\n",
        "\n",
        "            # Value Update: Make V(s) closer to actual G_t\n",
        "            target = torch.tensor([G_t], dtype=torch.float32)\n",
        "            value_loss.append(F.mse_loss(value.view(-1), target))\n",
        "\n",
        "        # Backpropagation\n",
        "        policy_optimizer.zero_grad()\n",
        "        value_optimizer.zero_grad()\n",
        "\n",
        "        if policy_loss:\n",
        "            loss_p = torch.stack(policy_loss).sum()\n",
        "            loss_v = torch.stack(value_loss).sum()\n",
        "\n",
        "            loss_p.backward()\n",
        "            loss_v.backward()\n",
        "\n",
        "            # Gradient Clipping (Prevents Exploding Gradients/NaNs)\n",
        "            torch.nn.utils.clip_grad_norm_(policy_net.parameters(), 1.0)\n",
        "            torch.nn.utils.clip_grad_norm_(value_net.parameters(), 1.0)\n",
        "\n",
        "            policy_optimizer.step()\n",
        "            value_optimizer.step()\n",
        "\n",
        "        if (episode + 1) % 500 == 0:\n",
        "            print(f\"Episode {episode + 1}/{num_episodes} completed.\")\n",
        "\n",
        "    return policy_net\n",
        "\n",
        "# --- 4. VISUALIZE ---\n",
        "if __name__ == \"__main__\":\n",
        "    trained_policy = train_reinforce_advantage()\n",
        "\n",
        "    print(\"\\nFinal Policy (Advantage Method):\")\n",
        "    actions_map = {0: '↑', 1: '↓', 2: '←', 3: '→'}\n",
        "\n",
        "    output_grid = []\n",
        "    for s in range(16):\n",
        "        if s in [0, 15]:\n",
        "            output_grid.append(\" T \")\n",
        "            continue\n",
        "        st = state_to_tensor(s)\n",
        "        with torch.no_grad():\n",
        "            probs = trained_policy(st)\n",
        "            best_a = torch.argmax(probs).item()\n",
        "        output_grid.append(f\" {actions_map[best_a]} \")\n",
        "\n",
        "    print(\"-\" * 17)\n",
        "    for i in range(0, 16, 4):\n",
        "        print(\"|\".join(output_grid[i:i+4]))\n",
        "        print(\"-\" * 17)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77421d73"
      },
      "source": [
        "### Explanation of the REINFORCE with Advantage Algorithm\n",
        "\n",
        "This notebook implements the REINFORCE algorithm with an Advantage Function, a type of policy gradient method in Reinforcement Learning. It aims to train an agent to navigate a simple 4x4 GridWorld environment. The agent learns an optimal policy (how to act) and a value function (how good a state is) using neural networks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3e986959"
      },
      "source": [
        "#### 1. Environment: GridWorld\n",
        "\n",
        "The `GridWorld` class defines the environment where our agent will operate. It's a 4x4 grid.\n",
        "\n",
        "-   **States:** There are 16 possible states (0 to 15), representing each cell in the grid.\n",
        "-   **Terminal States:** States `0` (top-left) and `15` (bottom-right) are terminal states. Reaching them ends an episode.\n",
        "-   **Actions:** The agent can take four actions: UP (0), DOWN (1), LEFT (2), RIGHT (3).\n",
        "-   **`step(state, action)`:** This method simulates taking an action from a given state, returning the `next_state`, `reward` (-1 for each step, encouraging shortest paths), and `done` status.\n",
        "-   **`reset()`:** Starts a new episode from a random non-terminal state."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1b9ed405"
      },
      "source": [
        "#### 2. Neural Networks\n",
        "\n",
        "Two neural networks are defined, representing the **Actor** (Policy) and the **Critic** (Value) components of the algorithm.\n",
        "\n",
        "-   **`PolicyNetwork(nn.Module)` (Actor):**\n",
        "    -   Takes a 16-dimensional one-hot encoded state vector as input.\n",
        "    -   Outputs a probability distribution over the 4 possible actions using a `softmax` activation in the final layer.\n",
        "    -   The agent uses this network to decide which action to take in a given state.\n",
        "\n",
        "-   **`ValueNetwork(nn.Module)` (Critic):**\n",
        "    -   Also takes a 16-dimensional one-hot encoded state vector as input.\n",
        "    -   Outputs a single scalar value, which is its estimate of the *expected return* (total future reward) from that state.\n",
        "    -   This network helps to evaluate the quality of actions taken by the policy network.\n",
        "\n",
        "-   **`state_to_tensor(state)`:** A utility function to convert an integer state representation into a one-hot encoded PyTorch tensor, suitable for feeding into the neural networks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e85e8947"
      },
      "source": [
        "#### 3. REINFORCE with Advantage Algorithm\n",
        "\n",
        "The `train_reinforce_advantage()` function orchestrates the training process:\n",
        "\n",
        "-   **Initialization:**\n",
        "    -   Creates instances of `GridWorld`, `PolicyNetwork`, and `ValueNetwork`.\n",
        "    -   Sets up `Adam` optimizers for both networks with a small learning rate (`0.0005`) for stability.\n",
        "    -   Defines `num_episodes` (how many training runs) and `gamma` (discount factor for future rewards).\n",
        "\n",
        "-   **Episode Loop:** The training runs for a specified number of `num_episodes`.\n",
        "    -   For each episode, the environment is `reset()` to a starting state.\n",
        "    -   **A. Collect Trajectory (Monte Carlo):**\n",
        "        -   The agent interacts with the environment until a terminal state is reached or a safety break is triggered.\n",
        "        -   In each step:\n",
        "            1.  The current `state` is converted to a tensor.\n",
        "            2.  The `policy_net` predicts action probabilities, and the `value_net` estimates the state's value.\n",
        "            3.  An `action` is **sampled** from the policy's probability distribution (stochastic policy).\n",
        "            4.  The `env.step()` method is called to get the `next_state`, `reward`, and `done` status.\n",
        "            5.  The `log_prob` of the taken action, the `value` estimate, and the `reward` are stored.\n",
        "\n",
        "    -   **B. Calculate Returns (G_t):**\n",
        "        -   After an episode ends, the total discounted return (`G`) is calculated for each step in the collected trajectory.\n",
        "        -   `G_t` is the sum of discounted future rewards from time step `t` onwards. This is a Monte Carlo approach as it waits until the end of the episode to calculate returns.\n",
        "        -   The `returns` are then **normalized** to improve training stability.\n",
        "\n",
        "    -   **C. Calculate Advantage & Update:**\n",
        "        -   For each step in the trajectory, the **Advantage Function** `A(s,a) = G_t - V(s)` is calculated.\n",
        "            -   `G_t`: The actual return observed from that state.\n",
        "            -   `V(s)`: The value estimated by the `ValueNetwork` for that state.\n",
        "            -   The advantage tells us how much *better* or *worse* the observed return `G_t` was compared to what the critic *predicted* `V(s)`.\n",
        "        -   **Policy Update (Actor):**\n",
        "            -   The policy loss is `-log_prob * advantage`. If the advantage is positive (meaning the action led to better-than-expected returns), the `log_prob` (and thus the probability) of that action is increased.\n",
        "            -   The `value.item()` is `detach()`ed so that the policy loss does not affect the value network's gradients.\n",
        "        -   **Value Update (Critic):**\n",
        "            -   The value loss is calculated using Mean Squared Error (`F.mse_loss`) between the `value` predicted by the `ValueNetwork` and the actual observed `G_t` (target).\n",
        "            -   This teaches the value network to more accurately predict the expected future returns.\n",
        "\n",
        "    -   **Backpropagation:**\n",
        "        -   Gradients are cleared for both optimizers.\n",
        "        -   The `policy_loss` and `value_loss` are summed and `backward()` is called to compute gradients.\n",
        "        -   **Gradient Clipping:** `torch.nn.utils.clip_grad_norm_` is applied to prevent exploding gradients.\n",
        "        -   Optimizers take a `step()` to update the network weights.\n",
        "\n",
        "-   Training progress is printed every 500 episodes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52084b65"
      },
      "source": [
        "#### 4. Visualization\n",
        "\n",
        "After training, the `if __name__ == \"__main__\":` block executes:\n",
        "\n",
        "-   It calls `train_reinforce_advantage()` to get the `trained_policy` network.\n",
        "-   It then iterates through all 16 states of the grid.\n",
        "-   For each non-terminal state, it feeds the state to the `trained_policy` network (using `torch.no_grad()` to disable gradient calculations as we are only inferring).\n",
        "-   It identifies the action with the highest probability (`torch.argmax`) as the `best_a`.\n",
        "-   Finally, it prints a formatted 4x4 grid where 'T' denotes terminal states and arrows (↑, ↓, ←, →) indicate the optimal action the trained policy would take in that state."
      ]
    }
  ]
}