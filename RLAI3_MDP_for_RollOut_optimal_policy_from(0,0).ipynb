{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hmr3j4sj-KjH",
        "outputId": "f1527a4b-decc-44ea-8714-d58bd1dfde22"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computing Optimal Policy...\n",
            "\n",
            "--- Rolling out Optimal Policy from State 0 (0,0) ---\n",
            "Step 1: At 0 (0,0) -> Action: DOWN -> New State: 4\n",
            "Step 2: At 4 (1,0) -> Action: DOWN -> New State: 8\n",
            "Step 3: At 8 (2,0) -> Action: DOWN -> New State: 12\n",
            "Step 4: At 12 (3,0) -> Action: RIGHT -> New State: 13\n",
            "Step 5: At 13 (3,1) -> Action: RIGHT -> New State: 14\n",
            "Step 6: At 14 (3,2) -> Action: RIGHT -> New State: 15\n",
            "\n",
            "Goal Reached at State 15!\n",
            "Total Path: [0, 4, 8, 12, 13, 14, 15]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "class MDPGridWorld:\n",
        "    def __init__(self):\n",
        "        self.grid_size = 4\n",
        "        # For \"Rollout from (0,0)\", we treat (0,0) as Start\n",
        "        # and (3,3) [index 15] as the ONLY Goal/Terminal state.\n",
        "        self.terminal_states = [15]\n",
        "        self.actions = ['UP', 'DOWN', 'LEFT', 'RIGHT']\n",
        "        self.gamma = 1.0 # Discount factor\n",
        "\n",
        "    def get_next_state_reward(self, state, action):\n",
        "        if state in self.terminal_states:\n",
        "            return state, 0\n",
        "\n",
        "        row, col = divmod(state, self.grid_size)\n",
        "\n",
        "        # Move Logic\n",
        "        if action == 'UP':    row = max(row - 1, 0)\n",
        "        elif action == 'DOWN':  row = min(row + 1, self.grid_size - 1)\n",
        "        elif action == 'LEFT':  col = max(col - 1, 0)\n",
        "        elif action == 'RIGHT': col = min(col + 1, self.grid_size - 1)\n",
        "\n",
        "        next_state = row * self.grid_size + col\n",
        "        reward = -1 # Cost per step\n",
        "        return next_state, reward\n",
        "\n",
        "    def value_iteration(self):\n",
        "        \"\"\"1. PLAN: Calculate V* to find the best policy.\"\"\"\n",
        "        V = np.zeros(self.grid_size * self.grid_size)\n",
        "        theta = 1e-4\n",
        "\n",
        "        while True:\n",
        "            delta = 0\n",
        "            V_new = np.copy(V)\n",
        "            for s in range(self.grid_size * self.grid_size):\n",
        "                if s in self.terminal_states: continue\n",
        "\n",
        "                action_values = []\n",
        "                for a in self.actions:\n",
        "                    ns, r = self.get_next_state_reward(s, a)\n",
        "                    action_values.append(r + self.gamma * V[ns])\n",
        "\n",
        "                new_val = max(action_values)\n",
        "                V_new[s] = new_val\n",
        "                delta = max(delta, abs(new_val - V[s]))\n",
        "            V = V_new\n",
        "            if delta < theta: break\n",
        "        return V\n",
        "\n",
        "    def rollout(self, start_state, V):\n",
        "        \"\"\"2. ACT: Execute the policy from the start state.\"\"\"\n",
        "        print(f\"\\n--- Rolling out Optimal Policy from State {start_state} (0,0) ---\")\n",
        "\n",
        "        curr_state = start_state\n",
        "        steps = 0\n",
        "        path = [curr_state]\n",
        "\n",
        "        while curr_state not in self.terminal_states:\n",
        "            row, col = divmod(curr_state, self.grid_size)\n",
        "\n",
        "            # Find Best Action using V\n",
        "            best_action = None\n",
        "            best_val = -float('inf')\n",
        "\n",
        "            for action in self.actions:\n",
        "                ns, r = self.get_next_state_reward(curr_state, action)\n",
        "                val = r + self.gamma * V[ns]\n",
        "\n",
        "                if val > best_val:\n",
        "                    best_val = val\n",
        "                    best_action = action\n",
        "\n",
        "            # Execute the Move\n",
        "            next_state, _ = self.get_next_state_reward(curr_state, best_action)\n",
        "\n",
        "            print(f\"Step {steps+1}: At {curr_state} ({row},{col}) -> Action: {best_action} -> New State: {next_state}\")\n",
        "\n",
        "            curr_state = next_state\n",
        "            path.append(curr_state)\n",
        "            steps += 1\n",
        "\n",
        "            if steps > 20: # Safety break\n",
        "                print(\"Stuck in loop!\")\n",
        "                break\n",
        "\n",
        "        print(f\"\\nGoal Reached at State {curr_state}!\")\n",
        "        print(f\"Total Path: {path}\")\n",
        "\n",
        "# --- Main Execution ---\n",
        "if __name__ == \"__main__\":\n",
        "    world = MDPGridWorld()\n",
        "\n",
        "    # 1. First, we need the plan (Value Iteration)\n",
        "    print(\"Computing Optimal Policy...\")\n",
        "    optimal_values = world.value_iteration()\n",
        "\n",
        "    # 2. Now, we Roll Out (Simulate) the plan from (0,0)\n",
        "    world.rollout(start_state=0, V=optimal_values)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f036a8e4"
      },
      "source": [
        "This code defines a `MDPGridWorld` class that models a simple 4x4 grid world environment. It uses a common reinforcement learning algorithm called Value Iteration to find the optimal path from a starting point (0,0) to a goal state (3,3).\n",
        "\n",
        "1.  **`MDPGridWorld` Class**: This class sets up the grid environment. It defines the grid size (4x4), the terminal state (index 15, which is (3,3) in a 0-indexed grid), possible actions ('UP', 'DOWN', 'LEFT', 'RIGHT'), and a discount factor (`gamma`).\n",
        "\n",
        "2.  **`get_next_state_reward` Method**: This method simulates moving within the grid. Given a current `state` and an `action`, it calculates the `next_state` and the `reward`. In this grid, moving to any non-terminal state incurs a reward of -1 (a cost per step), and reaching the terminal state yields a reward of 0.\n",
        "\n",
        "3.  **`value_iteration` Method**: This is the 'planning' phase. It implements the Value Iteration algorithm to compute the optimal value function (V*). This function `V*` tells us the maximum expected future reward from each state, assuming we act optimally. It iteratively updates the value of each state until the values converge, meaning they don't change significantly anymore.\n",
        "\n",
        "4.  **`rollout` Method**: This is the 'acting' phase. Once the optimal value function `V` is computed, this method simulates an agent navigating the grid from a `start_state`. At each step, the agent chooses the action that leads to the state with the highest expected value, effectively following the optimal policy derived from `V`. It then prints the path taken until the goal is reached.\n",
        "\n",
        "5.  **Main Execution Block**: The code first creates an instance of `MDPGridWorld`, then calls `value_iteration()` to calculate the `optimal_values`. Finally, it calls `rollout(start_state=0, V=optimal_values)` to demonstrate the optimal path an agent would take starting from state 0 (which is (0,0)).\n",
        "\n",
        "In essence, the code first 'learns' the best way to navigate the grid by calculating the value of each state, and then 'shows' how an agent would use that knowledge to reach the goal efficiently."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0JK0KdNO-rlK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}