{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P3pbuHqRYh1E",
        "outputId": "c74f5e2c-5e05-4db4-a9ee-a489a304ade4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Multi-Agent System (Agents A & B)...\n",
            "\n",
            "--- Testing MARL Interaction ---\n",
            "Agent A: (0,0) -> (3,3)\n",
            "Agent B: (3,0) -> (0,3)\n",
            " A  .  .  . \n",
            " .  .  .  . \n",
            " .  .  .  . \n",
            " B  .  .  . \n",
            "------------\n",
            "Step 1: Agent A goes RIGHT, Agent B goes RIGHT\n",
            " .  A  .  . \n",
            " .  .  .  . \n",
            " .  .  .  . \n",
            " .  B  .  . \n",
            "------------\n",
            "Step 2: Agent A goes DOWN, Agent B goes UP\n",
            " .  .  .  . \n",
            " .  A  .  . \n",
            " .  B  .  . \n",
            " .  .  .  . \n",
            "------------\n",
            "Step 3: Agent A goes RIGHT, Agent B goes RIGHT\n",
            " .  .  .  . \n",
            " .  .  A  . \n",
            " .  .  B  . \n",
            " .  .  .  . \n",
            "------------\n",
            "Step 4: Agent A goes RIGHT, Agent B goes UP\n",
            " .  .  .  . \n",
            " .  .  B  A \n",
            " .  .  .  . \n",
            " .  .  .  . \n",
            "------------\n",
            "Step 5: Agent A goes DOWN, Agent B goes UP\n",
            " .  .  B  . \n",
            " .  .  .  . \n",
            " .  .  .  A \n",
            " .  .  .  . \n",
            "------------\n",
            "Step 6: Agent A goes DOWN, Agent B goes RIGHT\n",
            " .  .  B  . \n",
            " .  .  .  . \n",
            " .  .  .  A \n",
            " .  .  .  . \n",
            "------------\n",
            "Step 7: Agent A goes DOWN, Agent B goes RIGHT\n",
            " .  .  B  . \n",
            " .  .  .  . \n",
            " .  .  .  A \n",
            " .  .  .  . \n",
            "------------\n",
            "Step 8: Agent A goes DOWN, Agent B goes RIGHT\n",
            " .  .  B  . \n",
            " .  .  .  . \n",
            " .  .  .  A \n",
            " .  .  .  . \n",
            "------------\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# --- 1. MULTI-AGENT ENVIRONMENT ---\n",
        "class MultiAgentGridWorld:\n",
        "    def __init__(self):\n",
        "        self.grid_size = 4\n",
        "        self.actions = [0, 1, 2, 3] # UP, DOWN, LEFT, RIGHT\n",
        "\n",
        "        # Goals\n",
        "        self.goal_A = (3, 3) # Bottom-Right\n",
        "        self.goal_B = (0, 3) # Top-Right\n",
        "\n",
        "    def reset(self):\n",
        "        # Start positions\n",
        "        self.pos_A = (0, 0)\n",
        "        self.pos_B = (3, 0)\n",
        "\n",
        "        # Return combined state: (rowA, colA, rowB, colB)\n",
        "        return self.pos_A + self.pos_B\n",
        "\n",
        "    def step(self, action_A, action_B):\n",
        "        # 1. Calculate Proposed New Positions\n",
        "        new_pos_A = self._move(self.pos_A, action_A)\n",
        "        new_pos_B = self._move(self.pos_B, action_B)\n",
        "\n",
        "        reward_A = -1\n",
        "        reward_B = -1\n",
        "        done_A = False\n",
        "        done_B = False\n",
        "\n",
        "        # 2. Check for Collisions (Agents hitting each other)\n",
        "        if new_pos_A == new_pos_B:\n",
        "            # Crash! Both stay in place and get big penalty\n",
        "            reward_A = -10\n",
        "            reward_B = -10\n",
        "            new_pos_A = self.pos_A\n",
        "            new_pos_B = self.pos_B\n",
        "        else:\n",
        "            # 3. Check for Goals\n",
        "            if new_pos_A == self.goal_A:\n",
        "                reward_A = 100\n",
        "                done_A = True\n",
        "\n",
        "            if new_pos_B == self.goal_B:\n",
        "                reward_B = 100\n",
        "                done_B = True\n",
        "\n",
        "        # Update positions (if not done)\n",
        "        if not done_A: self.pos_A = new_pos_A\n",
        "        if not done_B: self.pos_B = new_pos_B\n",
        "\n",
        "        next_state = self.pos_A + self.pos_B\n",
        "\n",
        "        # Global Done: When BOTH finished\n",
        "        # (For simplicity in this simulation, we reset if ONE finishes to keep them training together,\n",
        "        # or we could wait. Here we'll treat episode as done if EITHER finishes for faster training cycles)\n",
        "        done = done_A or done_B\n",
        "\n",
        "        return next_state, reward_A, reward_B, done\n",
        "\n",
        "    def _move(self, pos, action):\n",
        "        r, c = pos\n",
        "        if action == 0:   r = max(r - 1, 0) # UP\n",
        "        elif action == 1: r = min(r + 1, self.grid_size - 1) # DOWN\n",
        "        elif action == 2: c = max(c - 1, 0) # LEFT\n",
        "        elif action == 3: c = min(c + 1, self.grid_size - 1) # RIGHT\n",
        "        return (r, c)\n",
        "\n",
        "# --- 2. INDEPENDENT Q-LEARNING ---\n",
        "def train_marl():\n",
        "    env = MultiAgentGridWorld()\n",
        "\n",
        "    # State Space: 4x4 for Agent A * 4x4 for Agent B = 256 states\n",
        "    # We map state tuple (r1, c1, r2, c2) to an index 0-255\n",
        "    def get_state_idx(state_tuple):\n",
        "        r1, c1, r2, c2 = state_tuple\n",
        "        # Flattening 4D coordinate to 1D index\n",
        "        return r1*64 + c1*16 + r2*4 + c2\n",
        "\n",
        "    # Two Independent Q-Tables\n",
        "    Q_A = np.zeros((256, 4))\n",
        "    Q_B = np.zeros((256, 4))\n",
        "\n",
        "    # Hyperparameters\n",
        "    episodes = 5000\n",
        "    alpha = 0.1\n",
        "    gamma = 0.95\n",
        "    epsilon = 0.1\n",
        "\n",
        "    print(\"Training Multi-Agent System (Agents A & B)...\")\n",
        "\n",
        "    for episode in range(episodes):\n",
        "        state = env.reset()\n",
        "        state_idx = get_state_idx(state)\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            # --- ACTION SELECTION (Epsilon-Greedy) ---\n",
        "            if np.random.rand() < epsilon:\n",
        "                act_A = np.random.choice(env.actions)\n",
        "            else:\n",
        "                act_A = np.argmax(Q_A[state_idx])\n",
        "\n",
        "            if np.random.rand() < epsilon:\n",
        "                act_B = np.random.choice(env.actions)\n",
        "            else:\n",
        "                act_B = np.argmax(Q_B[state_idx])\n",
        "\n",
        "            # --- STEP ---\n",
        "            next_state, rA, rB, done = env.step(act_A, act_B)\n",
        "            next_state_idx = get_state_idx(next_state)\n",
        "\n",
        "            # --- UPDATE Q-TABLES SEPARATELY ---\n",
        "\n",
        "            # Update Agent A\n",
        "            old_val_A = Q_A[state_idx, act_A]\n",
        "            next_max_A = np.max(Q_A[next_state_idx])\n",
        "            Q_A[state_idx, act_A] = old_val_A + alpha * (rA + gamma * next_max_A - old_val_A)\n",
        "\n",
        "            # Update Agent B\n",
        "            old_val_B = Q_B[state_idx, act_B]\n",
        "            next_max_B = np.max(Q_B[next_state_idx])\n",
        "            Q_B[state_idx, act_B] = old_val_B + alpha * (rB + gamma * next_max_B - old_val_B)\n",
        "\n",
        "            state_idx = next_state_idx\n",
        "\n",
        "            # Safety break\n",
        "            if rA == 100 or rB == 100:\n",
        "                break\n",
        "\n",
        "    return Q_A, Q_B, env\n",
        "\n",
        "# --- 3. ANALYSE / TEST ---\n",
        "if __name__ == \"__main__\":\n",
        "    qa, qb, env = train_marl()\n",
        "\n",
        "    print(\"\\n--- Testing MARL Interaction ---\")\n",
        "    print(\"Agent A: (0,0) -> (3,3)\")\n",
        "    print(\"Agent B: (3,0) -> (0,3)\")\n",
        "\n",
        "    state = env.reset()\n",
        "    state_idx = 0 # Calculated manually for (0,0,3,0)\n",
        "\n",
        "    # Helper to print grid\n",
        "    def print_grid(pos_a, pos_b):\n",
        "        grid = [[' . ' for _ in range(4)] for _ in range(4)]\n",
        "        grid[pos_a[0]][pos_a[1]] = ' A '\n",
        "        grid[pos_b[0]][pos_b[1]] = ' B '\n",
        "        if pos_a == pos_b: grid[pos_a[0]][pos_a[1]] = ' X ' # Collision\n",
        "        for row in grid:\n",
        "            print(\"\".join(row))\n",
        "        print(\"-\" * 12)\n",
        "\n",
        "    pos_A = (0,0)\n",
        "    pos_B = (3,0)\n",
        "\n",
        "    print_grid(pos_A, pos_B)\n",
        "\n",
        "    for step in range(8):\n",
        "        # Calculate state index from current positions\n",
        "        idx = pos_A[0]*64 + pos_A[1]*16 + pos_B[0]*4 + pos_B[1]\n",
        "\n",
        "        # Choose best actions\n",
        "        act_A = np.argmax(qa[idx])\n",
        "        act_B = np.argmax(qb[idx])\n",
        "\n",
        "        move_map = {0:'UP', 1:'DOWN', 2:'LEFT', 3:'RIGHT'}\n",
        "        print(f\"Step {step+1}: Agent A goes {move_map[act_A]}, Agent B goes {move_map[act_B]}\")\n",
        "\n",
        "        # Execute (using internal env logic manually to show steps)\n",
        "        # Note: In test, we assume they learned to avoid collision\n",
        "        new_state, _, _, _ = env.step(act_A, act_B)\n",
        "\n",
        "        # Extract positions from tuple (rA, cA, rB, cB)\n",
        "        pos_A = (new_state[0], new_state[1])\n",
        "        pos_B = (new_state[2], new_state[3])\n",
        "\n",
        "        print_grid(pos_A, pos_B)\n",
        "\n",
        "        if pos_A == (3,3) and pos_B == (0,3):\n",
        "            print(\"Both Agents Reached Goals!\")\n",
        "            break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "693713b5"
      },
      "source": [
        "### Understanding the Multi-Agent Reinforcement Learning Code\n",
        "\n",
        "This notebook implements a basic Multi-Agent Reinforcement Learning (MARL) system using Independent Q-Learning. The goal is to train two agents to navigate a 4x4 grid world to their respective goals while avoiding collisions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38da4efd"
      },
      "source": [
        "#### 1. MultiAgentGridWorld Environment\n",
        "\n",
        "This class defines the grid world environment where two agents, A and B, operate.\n",
        "\n",
        "*   **`__init__(self)`**: Initializes the grid size (4x4) and defines the possible actions (UP, DOWN, LEFT, RIGHT). It also sets the distinct goal locations for Agent A `(3, 3)` and Agent B `(0, 3)`.\n",
        "\n",
        "*   **`reset(self)`**: Resets the environment to its initial state. Both agents start at predefined positions: Agent A at `(0, 0)` (top-left) and Agent B at `(3, 0)` (bottom-left). It returns a combined state tuple `(rowA, colA, rowB, colB)`.\n",
        "\n",
        "*   **`step(self, action_A, action_B)`**: This is the core interaction function. Given actions for both agents, it calculates their new proposed positions.\n",
        "    *   **Collision Detection**: If both agents attempt to move to the same cell, a collision occurs. In this case, both agents receive a large negative reward (`-10`), and they remain in their original positions (they don't move).\n",
        "    *   **Goal Achievement**: If an agent reaches its designated goal, it receives a large positive reward (`100`), and its `done` flag is set to `True`.\n",
        "    *   **Reward**: A small negative reward (`-1`) is given at each step to encourage faster goal achievement.\n",
        "    *   **State Update**: The agents' positions are updated.\n",
        "    *   **Global `done`**: The episode is considered `done` if *either* agent reaches its goal, simplifying the training cycles.\n",
        "\n",
        "*   **`_move(self, pos, action)`**: A helper private method to calculate a new position for a single agent based on its current position and chosen action, ensuring it stays within grid boundaries."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dd1b69a6"
      },
      "source": [
        "#### 2. Independent Q-Learning Implementation (`train_marl` function)\n",
        "\n",
        "This function implements the training logic for the two agents using Independent Q-Learning. In this approach, each agent learns its own Q-table independently, treating the other agent as part of the environment.\n",
        "\n",
        "*   **`env = MultiAgentGridWorld()`**: Creates an instance of our environment.\n",
        "\n",
        "*   **State Representation**:\n",
        "    *   The environment's state is a tuple `(r1, c1, r2, c2)`.\n",
        "    *   `get_state_idx(state_tuple)`: This function converts the 4-element state tuple into a single integer index (0-255). This is crucial for indexing the Q-tables, as a 4x4 grid for two agents means 4*4*4*4 = 256 possible combined states.\n",
        "\n",
        "*   **Q-Tables (`Q_A`, `Q_B`)**: Two separate Q-tables are maintained, one for each agent. Each table has dimensions `(number_of_states, number_of_actions)`. `np.zeros((256, 4))` initializes these tables with zeros.\n",
        "\n",
        "*   **Hyperparameters**:\n",
        "    *   `episodes = 5000`: Number of training episodes.\n",
        "    *   `alpha = 0.1`: Learning rate, determining how much new information overrides old information.\n",
        "    *   `gamma = 0.95`: Discount factor, balancing the importance of immediate versus future rewards.\n",
        "    *   `epsilon = 0.1`: Exploration-exploitation trade-off parameter. During training, agents will sometimes take random actions (`epsilon` chance) to explore the environment, and sometimes take the best-known action (`1-epsilon` chance) to exploit what they've learned.\n",
        "\n",
        "*   **Training Loop**:\n",
        "    *   For each episode, the environment is reset.\n",
        "    *   **Action Selection (Epsilon-Greedy)**: Each agent independently decides its action. With probability `epsilon`, it chooses a random action. Otherwise, it chooses the action with the highest Q-value for its current state from its Q-table.\n",
        "    *   **`env.step(act_A, act_B)`**: Both agents' chosen actions are passed to the environment to get the next state, rewards, and `done` flag.\n",
        "    *   **Q-Table Update**: The core of Q-learning. For each agent, its Q-table is updated using the Q-learning formula:\n",
        "        `Q(s,a) = Q(s,a) + alpha * (reward + gamma * max(Q(s',a')) - Q(s,a))`\n",
        "        This updates the Q-value for the `(state, action)` pair based on the received reward and the maximum possible future reward from the `next_state`.\n",
        "    *   The loop continues until the episode is `done` (an agent reaches its goal). A safety break is also included if a goal is reached."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8514068a"
      },
      "source": [
        "#### 3. Analysis / Testing (`if __name__ == \"__main__\":` block)\n",
        "\n",
        "This section runs after the training is complete to demonstrate the learned policies of the agents. It shows how the agents behave in the environment using their trained Q-tables.\n",
        "\n",
        "*   **`qa, qb, env = train_marl()`**: Calls the training function to get the trained Q-tables and a fresh environment instance.\n",
        "\n",
        "*   **`print_grid(pos_a, pos_b)`**: A helper function to visualize the agents' positions on the grid. 'A' represents Agent A, 'B' represents Agent B, and 'X' indicates a collision.\n",
        "\n",
        "*   **Testing Loop**:\n",
        "    *   The environment is reset to the initial state.\n",
        "    *   In each step, the `get_state_idx` function is used to convert the current positions into a state index.\n",
        "    *   **Action Selection**: Unlike training, during testing, agents *always* choose the action with the highest Q-value (greedy policy) from their respective Q-tables, as `epsilon` is effectively 0.\n",
        "    *   **`env.step(act_A, act_B)`**: The chosen actions are executed in the environment.\n",
        "    *   The new positions are extracted and the grid is printed.\n",
        "    *   The loop continues for a fixed number of steps (`8` in this case) or until both agents reach their goals, demonstrating the learned cooperative/non-colliding behavior."
      ]
    }
  ]
}