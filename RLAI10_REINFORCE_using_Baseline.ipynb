{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mbVN7Fha_Ada",
        "outputId": "6b04d6a8-cd70-4dad-9cc8-6cfb56a3d239"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training REINFORCE with Baseline\n",
            "Episode 500/2000 completed.\n",
            "Episode 1000/2000 completed.\n",
            "Episode 1500/2000 completed.\n",
            "Episode 2000/2000 completed.\n",
            "\n",
            "Final Policy (REINFORCE):\n",
            "-----------------\n",
            " T | ← | ← | ↓ \n",
            "-----------------\n",
            " ↑ | ↑ | → | ↓ \n",
            "-----------------\n",
            " ↑ | → | → | ↓ \n",
            "-----------------\n",
            " → | → | → | T \n",
            "-----------------\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# --- 1. ENVIRONMENT (4x4 Grid World) ---\n",
        "class GridWorld:\n",
        "    def __init__(self):\n",
        "        self.grid_size = 4\n",
        "        self.terminal_states = [0, 15]\n",
        "        self.actions = [0, 1, 2, 3] # UP, DOWN, LEFT, RIGHT\n",
        "\n",
        "    def step(self, state, action):\n",
        "        if state in self.terminal_states:\n",
        "            return state, 0, True\n",
        "\n",
        "        row, col = divmod(state, self.grid_size)\n",
        "        if action == 0:   row = max(row - 1, 0)\n",
        "        elif action == 1: row = min(row + 1, self.grid_size - 1)\n",
        "        elif action == 2: col = max(col - 1, 0)\n",
        "        elif action == 3: col = min(col + 1, self.grid_size - 1)\n",
        "\n",
        "        next_state = row * self.grid_size + col\n",
        "        reward = -1\n",
        "        done = next_state in self.terminal_states\n",
        "        return next_state, reward, done\n",
        "\n",
        "    def reset(self):\n",
        "        start_state = np.random.randint(0, 16)\n",
        "        while start_state in self.terminal_states:\n",
        "            start_state = np.random.randint(0, 16)\n",
        "        return start_state\n",
        "\n",
        "# --- 2. NEURAL NETWORKS ---\n",
        "\n",
        "class PolicyNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(PolicyNetwork, self).__init__()\n",
        "        self.fc1 = nn.Linear(16, 128)\n",
        "        self.fc2 = nn.Linear(128, 4)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        # Numerical stability fix: prevents NaN in softmax\n",
        "        return F.softmax(x, dim=-1)\n",
        "\n",
        "class ValueNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ValueNetwork, self).__init__()\n",
        "        self.fc1 = nn.Linear(16, 128)\n",
        "        self.fc2 = nn.Linear(128, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        return self.fc2(x)\n",
        "\n",
        "def state_to_tensor(state):\n",
        "    v = torch.zeros(16)\n",
        "    v[state] = 1.0\n",
        "    return v.unsqueeze(0)\n",
        "\n",
        "# --- 3. REINFORCE ALGORITHM ---\n",
        "def train_reinforce_baseline():\n",
        "    env = GridWorld()\n",
        "\n",
        "    policy_net = PolicyNetwork()\n",
        "    value_net = ValueNetwork()\n",
        "\n",
        "    # Reduced learning rate slightly for stability\n",
        "    policy_optimizer = optim.Adam(policy_net.parameters(), lr=0.0005)\n",
        "    value_optimizer = optim.Adam(value_net.parameters(), lr=0.0005)\n",
        "\n",
        "    num_episodes = 2000\n",
        "    gamma = 0.99\n",
        "\n",
        "    print(\"Training REINFORCE with Baseline\")\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "\n",
        "        log_probs = []\n",
        "        values = []\n",
        "        rewards = []\n",
        "\n",
        "        # A. Generate Episode\n",
        "        while not done:\n",
        "            state_t = state_to_tensor(state)\n",
        "\n",
        "            probs = policy_net(state_t)\n",
        "            value = value_net(state_t)\n",
        "\n",
        "            # Create distribution\n",
        "            dist = torch.distributions.Categorical(probs)\n",
        "            action = dist.sample()\n",
        "\n",
        "            # Step\n",
        "            next_state, reward, done = env.step(state, action.item())\n",
        "\n",
        "            log_probs.append(dist.log_prob(action))\n",
        "            values.append(value)\n",
        "            rewards.append(reward)\n",
        "\n",
        "            state = next_state\n",
        "\n",
        "            # Safety break if agent gets stuck in a loop\n",
        "            if len(rewards) > 100:\n",
        "                break\n",
        "\n",
        "        # B. Calculate Returns\n",
        "        returns = []\n",
        "        G = 0\n",
        "        for r in reversed(rewards):\n",
        "            G = r + gamma * G\n",
        "            returns.insert(0, G)\n",
        "        returns = torch.tensor(returns, dtype=torch.float32)\n",
        "\n",
        "        # C. Normalize Returns (Safe Mode)\n",
        "        # Only normalize if we have more than 1 step, otherwise std is NaN\n",
        "        if len(returns) > 1:\n",
        "            returns = (returns - returns.mean()) / (returns.std() + 1e-9)\n",
        "        else:\n",
        "            returns = returns - returns.mean()\n",
        "\n",
        "        # D. Calculate Losses\n",
        "        policy_loss = []\n",
        "        value_loss = []\n",
        "\n",
        "        for log_prob, value, G_t in zip(log_probs, values, returns):\n",
        "            advantage = G_t - value.item()\n",
        "\n",
        "            # Policy Loss\n",
        "            policy_loss.append(-log_prob * advantage)\n",
        "\n",
        "            # Value Loss (Fixing the warning by using detach/clone logic if needed)\n",
        "            # We target the actual scalar G_t\n",
        "            target = torch.tensor([G_t], dtype=torch.float32)\n",
        "            value_loss.append(F.mse_loss(value.view(-1), target))\n",
        "\n",
        "        policy_optimizer.zero_grad()\n",
        "        value_optimizer.zero_grad()\n",
        "\n",
        "        # Check if lists are not empty (in case of immediate termination)\n",
        "        if policy_loss:\n",
        "            loss_p = torch.stack(policy_loss).sum()\n",
        "            loss_v = torch.stack(value_loss).sum()\n",
        "\n",
        "            loss_p.backward()\n",
        "            loss_v.backward()\n",
        "\n",
        "            # GRADIENT CLIPPING (The Fix for Exploding Gradients)\n",
        "            torch.nn.utils.clip_grad_norm_(policy_net.parameters(), 1.0)\n",
        "            torch.nn.utils.clip_grad_norm_(value_net.parameters(), 1.0)\n",
        "\n",
        "            policy_optimizer.step()\n",
        "            value_optimizer.step()\n",
        "\n",
        "        if (episode + 1) % 500 == 0:\n",
        "            print(f\"Episode {episode + 1}/{num_episodes} completed.\")\n",
        "\n",
        "    return policy_net\n",
        "\n",
        "# --- 4. VISUALIZE ---\n",
        "if __name__ == \"__main__\":\n",
        "    trained_policy = train_reinforce_baseline()\n",
        "\n",
        "    print(\"\\nFinal Policy (REINFORCE):\")\n",
        "    actions_map = {0: '↑', 1: '↓', 2: '←', 3: '→'}\n",
        "\n",
        "    output_grid = []\n",
        "    for s in range(16):\n",
        "        if s in [0, 15]:\n",
        "            output_grid.append(\" T \")\n",
        "            continue\n",
        "        st = state_to_tensor(s)\n",
        "        with torch.no_grad():\n",
        "            probs = trained_policy(st)\n",
        "            best_a = torch.argmax(probs).item()\n",
        "        output_grid.append(f\" {actions_map[best_a]} \")\n",
        "\n",
        "    print(\"-\" * 17)\n",
        "    for i in range(0, 16, 4):\n",
        "        print(\"|\".join(output_grid[i:i+4]))\n",
        "        print(\"-\" * 17)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6cf7ce9c"
      },
      "source": [
        "This code implements the REINFORCE reinforcement learning algorithm, enhanced with a baseline, to train an agent to navigate a 4x4 GridWorld environment. Let's break down its key components:\n",
        "\n",
        "1.  **Environment (GridWorld)**: This class defines a 4x4 grid. The agent can move Up, Down, Left, or Right. States 0 and 15 are terminal states. Each step incurs a reward of -1, encouraging the agent to reach a terminal state as quickly as possible. The `step` method calculates the next state, reward, and whether the episode is finished, while `reset` places the agent in a random non-terminal starting state.\n",
        "\n",
        "2.  **Neural Networks**: Two separate neural networks are defined using PyTorch:\n",
        "    *   **`PolicyNetwork`**: This network takes a state as input (represented as a 16-element one-hot encoded vector) and outputs a probability distribution over the four possible actions (Up, Down, Left, Right). It uses a ReLU activation in its hidden layer and a Softmax activation in its output layer to ensure the outputs are valid probabilities.\n",
        "    *   **`ValueNetwork`**: This network also takes a state as input and outputs a single value, representing the estimated value (expected cumulative reward) of being in that state. It uses a ReLU activation in its hidden layer.\n",
        "    *   **`state_to_tensor`**: A helper function to convert an integer state representation into a one-hot encoded PyTorch tensor suitable for network input.\n",
        "\n",
        "3.  **REINFORCE Algorithm (`train_reinforce_baseline`)**:\n",
        "    *   **Initialization**: Sets up the `GridWorld` environment, initializes both the `PolicyNetwork` and `ValueNetwork`, and configures their respective Adam optimizers with a learning rate of 0.0005. It also defines `num_episodes` (2000) and the discount factor `gamma` (0.99).\n",
        "    *   **Episode Generation (A)**: For each episode, the agent interacts with the environment. It uses the `PolicyNetwork` to sample an action, takes a step in the environment, and records the `log_probability` of the chosen action, the `value` predicted by the `ValueNetwork`, and the `reward` received. This continues until a terminal state is reached or a safety limit of 100 steps is exceeded.\n",
        "    *   **Return Calculation (B)**: After an episode ends, the code calculates the discounted cumulative rewards (returns, `G`) for each step in the episode. This means `G_t` is the sum of future discounted rewards starting from time `t`.\n",
        "    *   **Return Normalization (C)**: The calculated returns are then normalized (mean-subtracted and divided by standard deviation). This often helps stabilize training by making the target values for the networks more consistent.\n",
        "    *   **Loss Calculation (D)**:\n",
        "        *   **Advantage**: The `advantage` is calculated as the actual return (`G_t`) minus the estimated value of the state (`value.item()`). This term quantifies how much better or worse the actual outcome was compared to the network's prediction.\n",
        "        *   **Policy Loss**: The policy network is updated using the REINFORCE objective with a baseline. The loss is calculated as `-log_prob * advantage`. If the advantage is positive (meaning the action led to a better-than-expected outcome), the probability of that action is increased. If the advantage is negative, the probability is decreased.\n",
        "        *   **Value Loss**: The value network is updated using a Mean Squared Error (MSE) loss, comparing its predicted value for a state to the actual calculated return (`G_t`) for that state. This makes the value network a better predictor of future rewards.\n",
        "    *   **Optimization**: After calculating losses for all steps in the episode, both policy and value losses are backpropagated, and their respective optimizers update the network weights. Gradient clipping is applied to prevent exploding gradients, which can be a common issue in reinforcement learning.\n",
        "\n",
        "4.  **Visualization**: After training, the `if __name__ == \"__main__\":` block executes. It takes the `trained_policy` network and for each state in the grid (excluding terminal states), it uses the policy to determine the action with the highest probability. This best action is then printed in a grid format, showing the learned optimal path for the agent."
      ]
    }
  ]
}