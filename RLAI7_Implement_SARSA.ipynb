{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_1I4aklloSvO",
        "outputId": "77b0daa9-f7f8-4da2-e538-2e7ef4f6a76f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training with SARSA (5000 Episodes)...\n",
            "\n",
            "Final Policy (SARSA):\n",
            "-----------------\n",
            " T | ← | ← | ← \n",
            "-----------------\n",
            " ↑ | ← | → | ↓ \n",
            "-----------------\n",
            " ↑ | ↑ | → | ↓ \n",
            "-----------------\n",
            " → | → | → | T \n",
            "-----------------\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# --- 1. ENVIRONMENT ---\n",
        "class GridWorld:\n",
        "    def __init__(self):\n",
        "        self.grid_size = 4\n",
        "        self.terminal_states = [0, 15]\n",
        "        self.actions = [0, 1, 2, 3] # UP, DOWN, LEFT, RIGHT\n",
        "\n",
        "    def step(self, state, action):\n",
        "        if state in self.terminal_states:\n",
        "            return state, 0, True\n",
        "\n",
        "        row, col = divmod(state, self.grid_size)\n",
        "\n",
        "        if action == 0:   row = max(row - 1, 0) # UP\n",
        "        elif action == 1: row = min(row + 1, self.grid_size - 1) # DOWN\n",
        "        elif action == 2: col = max(col - 1, 0) # LEFT\n",
        "        elif action == 3: col = min(col + 1, self.grid_size - 1) # RIGHT\n",
        "\n",
        "        next_state = row * self.grid_size + col\n",
        "        reward = -1\n",
        "        done = next_state in self.terminal_states\n",
        "        return next_state, reward, done\n",
        "\n",
        "    def reset(self):\n",
        "        start_state = np.random.randint(0, 16)\n",
        "        while start_state in self.terminal_states:\n",
        "            start_state = np.random.randint(0, 16)\n",
        "        return start_state\n",
        "\n",
        "# --- 2. THE ALGORITHM: SARSA ---\n",
        "def sarsa_learning():\n",
        "    env = GridWorld()\n",
        "\n",
        "    # Parameters\n",
        "    num_episodes = 5000\n",
        "    alpha = 0.1   # Learning Rate\n",
        "    gamma = 1.0   # Discount Factor\n",
        "    epsilon = 0.1 # Exploration Rate\n",
        "\n",
        "    # Initialize Q-Table (16 States x 4 Actions)\n",
        "    Q = np.zeros((16, 4))\n",
        "\n",
        "    print(\"Training with SARSA (5000 Episodes)...\")\n",
        "\n",
        "    for _ in range(num_episodes):\n",
        "        state = env.reset()\n",
        "\n",
        "        # SARSA Step 1: Choose Action A (Epsilon-Greedy) BEFORE the loop\n",
        "        if np.random.rand() < epsilon:\n",
        "            action = np.random.choice(env.actions)\n",
        "        else:\n",
        "            action = np.argmax(Q[state])\n",
        "\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            # SARSA Step 2: Take Action A, observe R, S'\n",
        "            next_state, reward, done = env.step(state, action)\n",
        "\n",
        "            # SARSA Step 3: Choose Next Action A' (Epsilon-Greedy) based on S'\n",
        "            # Note: We pick the next action NOW, before updating\n",
        "            if np.random.rand() < epsilon:\n",
        "                next_action = np.random.choice(env.actions)\n",
        "            else:\n",
        "                next_action = np.argmax(Q[next_state])\n",
        "\n",
        "            # SARSA Step 4: Update Q(S, A) using Q(S', A')\n",
        "            # Formula: Q(s,a) = Q(s,a) + alpha * [ R + gamma * Q(s',a') - Q(s,a) ]\n",
        "\n",
        "            # Value of next state (0 if terminal)\n",
        "            q_next = 0 if done else Q[next_state, next_action]\n",
        "\n",
        "            target = reward + gamma * q_next\n",
        "            Q[state, action] += alpha * (target - Q[state, action])\n",
        "\n",
        "            # SARSA Step 5: Move to next state pair\n",
        "            state = next_state\n",
        "            action = next_action\n",
        "\n",
        "    return Q\n",
        "\n",
        "# --- 3. EXECUTION & RESULTS ---\n",
        "def print_policy(Q):\n",
        "    actions_map = {0: '↑', 1: '↓', 2: '←', 3: '→'}\n",
        "    print(\"\\nFinal Policy (SARSA):\")\n",
        "    print(\"-\" * 17)\n",
        "\n",
        "    grid_output = []\n",
        "    for s in range(16):\n",
        "        if s in [0, 15]:\n",
        "            grid_output.append(\" T \")\n",
        "            continue\n",
        "        best_action = np.argmax(Q[s])\n",
        "        grid_output.append(f\" {actions_map[best_action]} \")\n",
        "\n",
        "    for i in range(0, 16, 4):\n",
        "        print(\"|\".join(grid_output[i:i+4]))\n",
        "        print(\"-\" * 17)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    q_table = sarsa_learning()\n",
        "    print_policy(q_table)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10b130c1"
      },
      "source": [
        "This code implements the **SARSA (State-Action-Reward-State-Action) reinforcement learning algorithm** to find an optimal policy for navigating a **GridWorld environment**.\n",
        "\n",
        "Let's break it down:\n",
        "\n",
        "**1. `GridWorld` Class:**\n",
        "   - This class defines the environment. It's a 4x4 grid. `terminal_states` (0 and 15) are the goal states. `actions` are UP, DOWN, LEFT, RIGHT.\n",
        "   - The `step` method takes a current state and an action, then returns the `next_state`, the `reward` (always -1 for non-terminal steps), and whether the episode is `done` (if a terminal state is reached).\n",
        "   - The `reset` method initializes the agent to a random starting state that is not a terminal state.\n",
        "\n",
        "**2. `sarsa_learning()` Function:**\n",
        "   - This is the core of the SARSA algorithm.\n",
        "   - **Parameters:**\n",
        "     - `num_episodes`: How many training iterations to run.\n",
        "     - `alpha` (learning rate): How much to update the Q-value based on the new information.\n",
        "     - `gamma` (discount factor): How much future rewards are valued.\n",
        "     - `epsilon` (exploration rate): The probability of choosing a random action (exploration) instead of the best known action (exploitation).\n",
        "   - **Q-Table:** `Q = np.zeros((16, 4))` is initialized. This table stores the estimated maximum future rewards for taking a specific `action` in a specific `state`.\n",
        "   - **Learning Loop:** For each episode:\n",
        "     - The agent starts in a `reset` state.\n",
        "     - It chooses an `action` using an **epsilon-greedy policy** (mostly exploits the best known action, but sometimes explores randomly).\n",
        "     - It enters a `while not done` loop:\n",
        "       - Takes the chosen `action`, observes the `next_state`, `reward`, and whether the episode is `done`.\n",
        "       - Chooses the `next_action` also using an epsilon-greedy policy based on the `next_state`.\n",
        "       - **SARSA Update Rule:** The Q-value for the current `(state, action)` pair is updated using the observed `reward` and the Q-value of the `(next_state, next_action)` pair. This is the key difference from Q-learning: SARSA is 'on-policy', meaning it learns the value of the policy it's currently following.\n",
        "       - The `state` and `action` are then updated to `next_state` and `next_action` to continue the episode.\n",
        "   - It returns the learned `Q` table.\n",
        "\n",
        "**3. `print_policy(Q)` Function:**\n",
        "   - This function takes the final `Q` table and visualizes the optimal `policy` (the best action to take in each state) found by SARSA.\n",
        "   - For each non-terminal state, it finds the action with the highest Q-value and prints the corresponding arrow (↑, ↓, ←, →). Terminal states are marked 'T'.\n",
        "\n",
        "**4. `if __name__ == \"__main__\":` Block:**\n",
        "   - This ensures that `sarsa_learning()` is called to train the agent, and then `print_policy()` is called to display the results, when the script is run directly."
      ]
    }
  ]
}