{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AwfofvrQICn4",
        "outputId": "c2af956b-029b-4274-d999-17d0e8c20529"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training A2C for Continuous Control...\n",
            "Episode 100/episodes: Total Reward = -7.67\n",
            "Episode 200/episodes: Total Reward = 4.42\n",
            "Episode 300/episodes: Total Reward = 1.45\n",
            "Episode 400/episodes: Total Reward = 5.27\n",
            "Episode 500/episodes: Total Reward = 1.33\n",
            "Episode 600/episodes: Total Reward = 0.10\n",
            "Episode 700/episodes: Total Reward = -6.03\n",
            "Episode 800/episodes: Total Reward = -1.79\n",
            "Episode 900/episodes: Total Reward = -0.53\n",
            "Episode 1000/episodes: Total Reward = 3.38\n",
            "\n",
            "Testing Trained Policy (Moving from -2.0 to 0.0):\n",
            "Step 1: Pos -1.82 -> Action 1.00 -> New Pos -1.82\n",
            "Step 2: Pos -1.72 -> Action 1.00 -> New Pos -1.72\n",
            "Step 3: Pos -1.62 -> Action 1.00 -> New Pos -1.62\n",
            "Step 4: Pos -1.52 -> Action 1.00 -> New Pos -1.52\n",
            "Step 5: Pos -1.42 -> Action 1.00 -> New Pos -1.42\n",
            "Step 6: Pos -1.32 -> Action 1.00 -> New Pos -1.32\n",
            "Step 7: Pos -1.22 -> Action 1.00 -> New Pos -1.22\n",
            "Step 8: Pos -1.12 -> Action 1.00 -> New Pos -1.12\n",
            "Step 9: Pos -1.02 -> Action 1.00 -> New Pos -1.02\n",
            "Step 10: Pos -0.92 -> Action 1.00 -> New Pos -0.92\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.distributions import Normal\n",
        "\n",
        "# --- 1. CONTINUOUS ENVIRONMENT (Simple 1D Target Seeking) ---\n",
        "class ContinuousTargetEnv:\n",
        "    def __init__(self):\n",
        "        # State: Position on a line (Start at -2.0)\n",
        "        # Goal: Reach 0.0\n",
        "        self.state = np.array([-2.0], dtype=np.float32)\n",
        "        self.max_steps = 200\n",
        "        self.current_step = 0\n",
        "\n",
        "    def reset(self):\n",
        "        # Start at random position between -2 and -1\n",
        "        self.state = np.array([np.random.uniform(-2, -1)], dtype=np.float32)\n",
        "        self.current_step = 0\n",
        "        return self.state\n",
        "\n",
        "    def step(self, action):\n",
        "        # Action is a continuous force [-1, 1]\n",
        "        force = np.clip(action, -1.0, 1.0)\n",
        "\n",
        "        # Dynamics: Position += Force * speed\n",
        "        self.state[0] += force * 0.1\n",
        "\n",
        "        # Calculate Reward (Negative distance to goal 0.0)\n",
        "        dist = abs(self.state[0] - 0.0)\n",
        "        reward = -dist\n",
        "\n",
        "        # Check Done\n",
        "        self.current_step += 1\n",
        "        done = dist < 0.1 or self.current_step >= self.max_steps\n",
        "\n",
        "        # Bonus reward for finishing\n",
        "        if dist < 0.1:\n",
        "            reward += 10.0\n",
        "\n",
        "        return self.state, reward, done\n",
        "\n",
        "# --- 2. ACTOR-CRITIC NETWORK ---\n",
        "class ActorCritic(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ActorCritic, self).__init__()\n",
        "        # Common layer\n",
        "        self.fc1 = nn.Linear(1, 128)\n",
        "\n",
        "        # ACTOR HEAD (Outputs Mean `mu` and Std Dev `sigma`)\n",
        "        # Used to create a Normal Distribution (Gaussian)\n",
        "        self.mu_head = nn.Linear(128, 1)\n",
        "        self.sigma_head = nn.Linear(128, 1)\n",
        "\n",
        "        # CRITIC HEAD (Outputs Value V(s))\n",
        "        self.value_head = nn.Linear(128, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "\n",
        "        # Actor outputs\n",
        "        mu = torch.tanh(self.mu_head(x)) # Output between -1 and 1\n",
        "        sigma = F.softplus(self.sigma_head(x)) + 1e-5 # Always positive\n",
        "\n",
        "        # Critic output\n",
        "        value = self.value_head(x)\n",
        "\n",
        "        return mu, sigma, value\n",
        "\n",
        "# --- 3. A2C ALGORITHM (Continuous) ---\n",
        "def train_a2c_continuous():\n",
        "    env = ContinuousTargetEnv()\n",
        "    model = ActorCritic()\n",
        "\n",
        "    # We update both heads with one optimizer\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    episodes = 1000\n",
        "    gamma = 0.99\n",
        "\n",
        "    print(\"Training A2C for Continuous Control...\")\n",
        "\n",
        "    for episode in range(episodes):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        total_reward = 0\n",
        "\n",
        "        while not done:\n",
        "            state_t = torch.FloatTensor(state)\n",
        "\n",
        "            # 1. Forward pass\n",
        "            mu, sigma, value = model(state_t)\n",
        "\n",
        "            # 2. Sample Continuous Action from Normal Distribution\n",
        "            dist = Normal(mu, sigma)\n",
        "            action = dist.sample()\n",
        "\n",
        "            # Clip action to valid range for environment\n",
        "            action_numpy = action.detach().numpy()[0]\n",
        "\n",
        "            # 3. Take Step\n",
        "            next_state, reward, done = env.step(action_numpy)\n",
        "            total_reward += reward\n",
        "\n",
        "            # 4. Calculate Target (TD Target)\n",
        "            next_state_t = torch.FloatTensor(next_state)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                _, _, next_value = model(next_state_t)\n",
        "                # If done, next value is 0\n",
        "                target_value = reward + (0 if done else gamma * next_value.item())\n",
        "\n",
        "            # 5. Calculate Advantage\n",
        "            # Advantage = Target - Current_Prediction\n",
        "            advantage = target_value - value\n",
        "\n",
        "            # 6. Calculate Losses\n",
        "\n",
        "            # Critic Loss: MSE(Target, Predicted)\n",
        "            critic_loss = advantage.pow(2)\n",
        "\n",
        "            # Actor Loss: -log_prob * advantage\n",
        "            # (We detach advantage so we don't backprop through critic here)\n",
        "            log_prob = dist.log_prob(action)\n",
        "            actor_loss = -log_prob * advantage.detach()\n",
        "\n",
        "            # Total Loss\n",
        "            loss = actor_loss + critic_loss\n",
        "\n",
        "            # 7. Update\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            state = next_state\n",
        "\n",
        "        if (episode + 1) % 100 == 0:\n",
        "            print(f\"Episode {episode + 1}/episodes: Total Reward = {total_reward:.2f}\")\n",
        "\n",
        "    return model\n",
        "\n",
        "# --- 4. TEST ---\n",
        "if __name__ == \"__main__\":\n",
        "    trained_model = train_a2c_continuous()\n",
        "\n",
        "    print(\"\\nTesting Trained Policy (Moving from -2.0 to 0.0):\")\n",
        "    env = ContinuousTargetEnv()\n",
        "    state = env.reset()\n",
        "\n",
        "    for i in range(10):\n",
        "        state_t = torch.FloatTensor(state)\n",
        "        with torch.no_grad():\n",
        "            mu, sigma, _ = trained_model(state_t)\n",
        "            # In testing, we just use the Mean (mu) - no randomness\n",
        "            action = mu.item()\n",
        "\n",
        "        next_state, _, done = env.step(action)\n",
        "        print(f\"Step {i+1}: Pos {state[0]:.2f} -> Action {action:.2f} -> New Pos {next_state[0]:.2f}\")\n",
        "        state = next_state\n",
        "        if done: break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2b097451"
      },
      "source": [
        "## Explanation of the A2C for Continuous Control Implementation\n",
        "\n",
        "This notebook demonstrates an Actor-Critic (A2C) algorithm for a simple continuous control problem: an agent learning to move towards a target position on a 1D line."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5b13ac86"
      },
      "source": [
        "### 1. ContinuousTargetEnv: The Environment\n",
        "\n",
        "This class defines a simple 1D environment where an agent needs to reach a target position (0.0). It simulates the agent's movement based on continuous actions.\n",
        "\n",
        "*   **State:** A single float representing the agent's position on a line.\n",
        "*   **Action:** A continuous value between -1.0 and 1.0, representing a force applied to the agent.\n",
        "*   **Reward:** The negative absolute distance to the target (0.0), with a bonus for reaching close to the target.\n",
        "*   **Reset:** Initializes the agent's position randomly between -2 and -1.\n",
        "*   **Step:** Takes an action, updates the agent's position, calculates the reward, and determines if the episode is `done`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33f1e497"
      },
      "source": [
        "### 2. ActorCritic: The Neural Network Model\n",
        "\n",
        "This `nn.Module` implements the Actor-Critic architecture. It shares a common initial layer and then branches into two heads:\n",
        "\n",
        "*   **Actor Head:** Responsible for outputting the parameters of a continuous probability distribution (specifically, a Normal distribution) from which actions are sampled. It outputs:\n",
        "    *   `mu` (mean): The average action the agent believes is optimal.\n",
        "    *   `sigma` (standard deviation): The uncertainty or exploration radius around `mu`.\n",
        "*   **Critic Head:** Responsible for estimating the **value function** V(s), which represents the expected future reward from a given state `s`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a69fe893"
      },
      "source": [
        "### 3. train_a2c_continuous: The A2C Training Algorithm\n",
        "\n",
        "This function implements the Asynchronous Advantage Actor-Critic (A2C) algorithm for continuous action spaces. A2C is a policy gradient method that aims to optimize the agent's policy (how it chooses actions) by using a critic to estimate the value of states.\n",
        "\n",
        "**Key Concepts:**\n",
        "\n",
        "*   **Policy Gradient:** Methods that directly optimize the policy by estimating the gradient of the expected return.\n",
        "*   **Actor-Critic:** Combines two components:\n",
        "    *   **Actor:** The policy network that selects actions.\n",
        "    *   **Critic:** The value network that estimates the value function.\n",
        "*   **Advantage:** The advantage function `A(s, a) = Q(s, a) - V(s)` measures how much better a particular action `a` is than the average action from state `s`. In A2C, it's often approximated as `R + gamma * V(s') - V(s)` (TD error).\n",
        "*   **Continuous Actions:** Instead of outputting discrete probabilities for each action, the actor outputs parameters (mean and standard deviation) of a continuous probability distribution (e.g., Normal distribution), and actions are sampled from this distribution.\n",
        "\n",
        "**Training Steps:**\n",
        "\n",
        "1.  **Forward Pass:** The current state is fed into the `ActorCritic` model to get `mu`, `sigma` (from actor), and `value` (from critic).\n",
        "2.  **Sample Action:** An action is sampled from the Normal distribution defined by `mu` and `sigma`.\n",
        "3.  **Take Step:** The sampled action is executed in the environment, yielding `next_state`, `reward`, and `done` flag.\n",
        "4.  **Calculate TD Target:** The target value for the critic is calculated using the Bellman equation: `Reward + gamma * Value(next_state)`. If the episode is done, `Value(next_state)` is 0.\n",
        "5.  **Calculate Advantage:** The advantage is the difference between the TD target and the current critic's prediction (`target_value - value`).\n",
        "6.  **Calculate Losses:**\n",
        "    *   **Critic Loss:** Typically Mean Squared Error between the predicted `value` and the `target_value` (`advantage.pow(2)`).\n",
        "    *   **Actor Loss:** Calculated as `-log_prob * advantage`. The `log_prob` encourages actions that lead to higher advantage, and the `advantage.detach()` ensures that the critic's gradient doesn't flow through the actor.\n",
        "7.  **Update:** The total loss (actor loss + critic loss) is backpropagated, and the optimizer updates the model's weights."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51d7a633"
      },
      "source": [
        "### 4. Testing the Trained Policy\n",
        "\n",
        "After training, the `if __name__ == '__main__':` block tests the performance of the learned policy.\n",
        "\n",
        "*   During testing, actions are typically not sampled randomly but are deterministically chosen as the mean (`mu`) of the distribution predicted by the actor. This is because the agent should exploit what it has learned.\n",
        "*   The agent starts at a random negative position and attempts to reach 0.0. The output shows the agent's position, the chosen action, and the new position over several steps."
      ]
    }
  ]
}