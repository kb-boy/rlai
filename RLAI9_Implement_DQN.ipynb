{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lra2-mPG7IV4",
        "outputId": "725f66fb-a2d1-415a-eaa1-918fb4489c7d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training DQN (this may take a moment)...\n",
            "Episode 200/1000 completed.\n",
            "Episode 400/1000 completed.\n",
            "Episode 600/1000 completed.\n",
            "Episode 800/1000 completed.\n",
            "Episode 1000/1000 completed.\n",
            "\n",
            "Visualizing DQN Policy:\n",
            "-----------------\n",
            " T | ← | ← | ← \n",
            "-----------------\n",
            " ↑ | ← | ↓ | ↓ \n",
            "-----------------\n",
            " ↑ | ↓ | ↓ | ↓ \n",
            "-----------------\n",
            " ↑ | → | → | T \n",
            "-----------------\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import random\n",
        "from collections import deque\n",
        "\n",
        "# --- 1. ENVIRONMENT ---\n",
        "class GridWorld:\n",
        "    def __init__(self):\n",
        "        self.grid_size = 4\n",
        "        self.terminal_states = [0, 15]\n",
        "        self.actions = [0, 1, 2, 3] # UP, DOWN, LEFT, RIGHT\n",
        "\n",
        "    def step(self, state, action):\n",
        "        if state in self.terminal_states:\n",
        "            return state, 0, True\n",
        "\n",
        "        row, col = divmod(state, self.grid_size)\n",
        "\n",
        "        if action == 0:   row = max(row - 1, 0)\n",
        "        elif action == 1: row = min(row + 1, self.grid_size - 1)\n",
        "        elif action == 2: col = max(col - 1, 0)\n",
        "        elif action == 3: col = min(col + 1, self.grid_size - 1)\n",
        "\n",
        "        next_state = row * self.grid_size + col\n",
        "        reward = -1\n",
        "        done = next_state in self.terminal_states\n",
        "        return next_state, reward, done\n",
        "\n",
        "    def reset(self):\n",
        "        start_state = np.random.randint(0, 16)\n",
        "        while start_state in self.terminal_states:\n",
        "            start_state = np.random.randint(0, 16)\n",
        "        return start_state\n",
        "\n",
        "# --- 2. THE NEURAL NETWORK ---\n",
        "class QNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(QNetwork, self).__init__()\n",
        "        # Input: 16 (One-hot encoding of the state)\n",
        "        # Hidden: 128 neurons\n",
        "        # Output: 4 (Q-values for UP, DOWN, LEFT, RIGHT)\n",
        "        self.fc1 = nn.Linear(16, 128)\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        self.fc3 = nn.Linear(64, 4)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        return self.fc3(x)\n",
        "\n",
        "# --- 3. HELPER: ONE-HOT ENCODING ---\n",
        "def state_to_tensor(state):\n",
        "    # Converts state integer (e.g., 5) to one-hot vector [0,0,0,0,0,1,0...]\n",
        "    v = torch.zeros(16)\n",
        "    v[state] = 1.0\n",
        "    return v.unsqueeze(0) # Add batch dimension\n",
        "\n",
        "# --- 4. THE ALGORITHM: DQN Training ---\n",
        "def train_dqn():\n",
        "    env = GridWorld()\n",
        "\n",
        "    # Hyperparameters\n",
        "    episodes = 1000\n",
        "    gamma = 0.99\n",
        "    epsilon = 1.0\n",
        "    epsilon_decay = 0.995\n",
        "    epsilon_min = 0.1\n",
        "    learning_rate = 0.001\n",
        "    batch_size = 32\n",
        "\n",
        "    # Initialize Networks\n",
        "    policy_net = QNetwork()\n",
        "    optimizer = optim.Adam(policy_net.parameters(), lr=learning_rate)\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    # Replay Buffer (Memory)\n",
        "    memory = deque(maxlen=2000)\n",
        "\n",
        "    print(\"Training DQN (this may take a moment)...\")\n",
        "\n",
        "    for episode in range(episodes):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            state_tensor = state_to_tensor(state)\n",
        "\n",
        "            # A. Select Action (Epsilon-Greedy)\n",
        "            if random.random() < epsilon:\n",
        "                action = random.choice(env.actions)\n",
        "            else:\n",
        "                with torch.no_grad():\n",
        "                    q_values = policy_net(state_tensor)\n",
        "                    action = torch.argmax(q_values).item()\n",
        "\n",
        "            # B. Step\n",
        "            next_state, reward, done = env.step(state, action)\n",
        "\n",
        "            # C. Store in Memory\n",
        "            memory.append((state, action, reward, next_state, done))\n",
        "            state = next_state\n",
        "\n",
        "            # D. Train (Experience Replay)\n",
        "            if len(memory) > batch_size:\n",
        "                minibatch = random.sample(memory, batch_size)\n",
        "\n",
        "                # Prepare batch data\n",
        "                states_b = torch.cat([state_to_tensor(x[0]) for x in minibatch])\n",
        "                next_states_b = torch.cat([state_to_tensor(x[3]) for x in minibatch])\n",
        "\n",
        "                # Get current Q values\n",
        "                q_preds = policy_net(states_b)\n",
        "\n",
        "                # Calculate Target Q values\n",
        "                with torch.no_grad():\n",
        "                    q_next = policy_net(next_states_b)\n",
        "\n",
        "                target_q_values = q_preds.clone()\n",
        "\n",
        "                for i, (s, a, r, ns, d) in enumerate(minibatch):\n",
        "                    # Bellman Update: R + gamma * max(Q(s'))\n",
        "                    target = r\n",
        "                    if not d:\n",
        "                        target += gamma * torch.max(q_next[i]).item()\n",
        "                    target_q_values[i][a] = target\n",
        "\n",
        "                # Gradient Descent\n",
        "                loss = criterion(q_preds, target_q_values)\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "        # Decay Epsilon\n",
        "        if epsilon > epsilon_min:\n",
        "            epsilon *= epsilon_decay\n",
        "\n",
        "        if (episode+1) % 200 == 0:\n",
        "            print(f\"Episode {episode+1}/{episodes} completed.\")\n",
        "\n",
        "    return policy_net\n",
        "\n",
        "# --- 5. TEST THE TRAINED MODEL ---\n",
        "if __name__ == \"__main__\":\n",
        "    trained_model = train_dqn()\n",
        "\n",
        "    print(\"\\nVisualizing DQN Policy:\")\n",
        "    print(\"-\" * 17)\n",
        "    actions_map = {0: '↑', 1: '↓', 2: '←', 3: '→'}\n",
        "    env = GridWorld()\n",
        "\n",
        "    output_grid = []\n",
        "    for s in range(16):\n",
        "        if s in [0, 15]:\n",
        "            output_grid.append(\" T \")\n",
        "            continue\n",
        "\n",
        "        st = state_to_tensor(s)\n",
        "        with torch.no_grad():\n",
        "            q = trained_model(st)\n",
        "            best_a = torch.argmax(q).item()\n",
        "        output_grid.append(f\" {actions_map[best_a]} \")\n",
        "\n",
        "    for i in range(0, 16, 4):\n",
        "        print(\"|\".join(output_grid[i:i+4]))\n",
        "        print(\"-\" * 17)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ad9ea4e5"
      },
      "source": [
        "This code implements a Deep Q-Network (DQN) to solve a simple GridWorld environment. Let's break it down section by section:\n",
        "\n",
        "1.  **GridWorld Environment (`GridWorld` class):** This class defines the environment. It's a 4x4 grid. States `0` and `15` are terminal (end states). Actions `0, 1, 2, 3` correspond to Up, Down, Left, Right. The `step` method takes a state and action, returns the next state, a reward (always -1 per step, encouraging shorter paths), and whether the episode is done. The `reset` method puts the agent in a random non-terminal starting state.\n",
        "\n",
        "2.  **The Neural Network (`QNetwork` class):** This is a simple feedforward neural network built with PyTorch. It takes a one-hot encoded state (16 inputs for a 4x4 grid) and outputs 4 Q-values, one for each possible action (Up, Down, Left, Right). These Q-values estimate the expected future reward for taking a particular action in a given state.\n",
        "\n",
        "3.  **One-Hot Encoding Helper (`state_to_tensor` function):** This function converts an integer state (e.g., state 5) into a one-hot vector (a tensor with a 1 at index 5 and 0s everywhere else). This format is suitable for input to the neural network.\n",
        "\n",
        "4.  **DQN Training Algorithm (`train_dqn` function):** This is the core of the reinforcement learning agent:\n",
        "    *   **Hyperparameters:** Defines settings like `episodes` (how many training runs), `gamma` (discount factor for future rewards), `epsilon` (for exploration-exploitation trade-off), `learning_rate`, and `batch_size` (for experience replay).\n",
        "    *   **Network Initialization:** Creates the `policy_net` (the Q-network), an `optimizer` (Adam) to update its weights, and a `criterion` (MSELoss) to measure prediction error.\n",
        "    *   **Replay Buffer (`deque`):** A memory where the agent stores its experiences (state, action, reward, next_state, done). This allows the agent to learn from past interactions in a more stable way by sampling random batches.\n",
        "    *   **Training Loop:** Iterates through many episodes:\n",
        "        *   **Epsilon-Greedy Action Selection:** The agent chooses an action. With probability `epsilon`, it takes a random action (exploration). Otherwise, it chooses the action with the highest Q-value predicted by the `policy_net` (exploitation).\n",
        "        *   **Environment Step:** The agent takes the chosen action in the environment, getting a `next_state`, `reward`, and `done` flag.\n",
        "        *   **Store in Memory:** The experience tuple is added to the `memory` buffer.\n",
        "        *   **Experience Replay (Training):** Once enough experiences are in memory, a random `minibatch` is sampled. The network is then trained:\n",
        "            *   It predicts Q-values for the states in the `minibatch` (`q_preds`).\n",
        "            *   It calculates `target_q_values` using the Bellman equation (reward + discounted max Q-value of the next state). This is the 'correct' Q-value the network should predict.\n",
        "            *   The `MSELoss` between `q_preds` and `target_q_values` is calculated, and the network's weights are updated via backpropagation and the optimizer.\n",
        "        *   **Epsilon Decay:** `epsilon` gradually decreases over episodes, making the agent explore less and exploit more as it learns.\n",
        "\n",
        "5.  **Test the Trained Model (`if __name__ == \"__main__\":` block):** After training, this section evaluates the learned policy. For each non-terminal state in the grid, it uses the `trained_model` to predict the best action (the one with the highest Q-value) and then prints a grid visualizing the optimal action for each state (e.g., '↑', '↓', '←', '→'). 'T' denotes a terminal state.\n",
        "\n",
        "In essence, this code demonstrates how a neural network can learn to navigate an environment by approximating the optimal action-value function through trial and error, guided by a replay buffer and an epsilon-greedy exploration strategy."
      ]
    }
  ]
}