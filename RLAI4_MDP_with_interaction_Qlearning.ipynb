{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7FWBV0NODV1Q",
        "outputId": "7dfadaf1-f60d-49cd-f84e-9c1e2203967d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training with Q-Learning (5000 Episodes)...\n",
            "\n",
            "Learned Policy (from Q-Table):\n",
            "-----------------\n",
            " T | ← | ← | ↓ \n",
            "-----------------\n",
            " ↑ | ← | → | ↓ \n",
            "-----------------\n",
            " ↑ | → | → | ↓ \n",
            "-----------------\n",
            " ↑ | → | → | T \n",
            "-----------------\n",
            "\n",
            "Example Q-Values for State 1 (Next to Top-Left Goal):\n",
            "UP: -1.97, DOWN: -2.93, LEFT: -1.00, RIGHT: -2.94\n",
            "(Notice 'LEFT' should have the highest value because it leads to the goal)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# --- 1. THE ENVIRONMENT (Unknown to the agent initially) ---\n",
        "class GridWorld:\n",
        "    def __init__(self):\n",
        "        self.grid_size = 4\n",
        "        self.terminal_states = [0, 15] # Top-Left, Bottom-Right\n",
        "        self.actions = [0, 1, 2, 3] # 0:UP, 1:DOWN, 2:LEFT, 3:RIGHT\n",
        "\n",
        "    def step(self, state, action):\n",
        "        if state in self.terminal_states:\n",
        "            return state, 0, True\n",
        "\n",
        "        row, col = divmod(state, self.grid_size)\n",
        "\n",
        "        # Action Logic\n",
        "        if action == 0:   row = max(row - 1, 0) # UP\n",
        "        elif action == 1: row = min(row + 1, self.grid_size - 1) # DOWN\n",
        "        elif action == 2: col = max(col - 1, 0) # LEFT\n",
        "        elif action == 3: col = min(col + 1, self.grid_size - 1) # RIGHT\n",
        "\n",
        "        next_state = row * self.grid_size + col\n",
        "        reward = -1 # Penalty for each step\n",
        "        done = next_state in self.terminal_states\n",
        "\n",
        "        return next_state, reward, done\n",
        "\n",
        "    def reset(self):\n",
        "        start_state = np.random.randint(0, 16)\n",
        "        while start_state in self.terminal_states:\n",
        "            start_state = np.random.randint(0, 16)\n",
        "        return start_state\n",
        "\n",
        "# --- 2. THE ALGORITHM (Q-Learning) ---\n",
        "def q_learning():\n",
        "    env = GridWorld()\n",
        "\n",
        "    # Parameters\n",
        "    num_episodes = 5000\n",
        "    alpha = 0.1   # Learning Rate (How fast we accept new info)\n",
        "    gamma = 0.99  # Discount Factor (Importance of future rewards)\n",
        "    epsilon = 0.1 # Exploration Rate (Chance to try random move)\n",
        "\n",
        "    # Initialize Q-Table: 16 States x 4 Actions\n",
        "    # Q[s, a] stores the value of taking action 'a' in state 's'\n",
        "    Q = np.zeros((16, 4))\n",
        "\n",
        "    print(\"Training with Q-Learning (5000 Episodes)...\")\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            # A. Choose Action (Epsilon-Greedy)\n",
        "            if np.random.rand() < epsilon:\n",
        "                action = np.random.choice(env.actions) # Explore (Random)\n",
        "            else:\n",
        "                action = np.argmax(Q[state]) # Exploit (Best known action)\n",
        "\n",
        "            # B. Take Action & Observe\n",
        "            next_state, reward, done = env.step(state, action)\n",
        "\n",
        "            # C. Update Q-Value (The Bellman Update Rule)\n",
        "            # Old Value\n",
        "            old_value = Q[state, action]\n",
        "            # Best possible value from next state\n",
        "            next_max = np.max(Q[next_state])\n",
        "\n",
        "            # Formula: Q(s,a) = Q(s,a) + alpha * [Reward + gamma * max(Q(s')) - Q(s,a)]\n",
        "            new_value = old_value + alpha * (reward + gamma * next_max - old_value)\n",
        "\n",
        "            Q[state, action] = new_value\n",
        "\n",
        "            state = next_state\n",
        "\n",
        "    return Q\n",
        "\n",
        "# --- 3. DISPLAY RESULTS ---\n",
        "def print_results(Q):\n",
        "    actions_map = {0: '↑', 1: '↓', 2: '←', 3: '→'}\n",
        "    print(\"\\nLearned Policy (from Q-Table):\")\n",
        "    print(\"-\" * 17)\n",
        "\n",
        "    grid_output = []\n",
        "    for s in range(16):\n",
        "        if s in [0, 15]: # Terminal\n",
        "            grid_output.append(\" T \")\n",
        "            continue\n",
        "\n",
        "        # The best action is the one with the highest Q-value for this state\n",
        "        best_action_idx = np.argmax(Q[s])\n",
        "        grid_output.append(f\" {actions_map[best_action_idx]} \")\n",
        "\n",
        "    # Print nicely\n",
        "    for i in range(0, 16, 4):\n",
        "        print(\"|\".join(grid_output[i:i+4]))\n",
        "        print(\"-\" * 17)\n",
        "\n",
        "    print(\"\\nExample Q-Values for State 1 (Next to Top-Left Goal):\")\n",
        "    print(f\"UP: {Q[1,0]:.2f}, DOWN: {Q[1,1]:.2f}, LEFT: {Q[1,2]:.2f}, RIGHT: {Q[1,3]:.2f}\")\n",
        "    print(\"(Notice 'LEFT' should have the highest value because it leads to the goal)\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    final_Q = q_learning()\n",
        "    print_results(final_Q)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c93ba22a"
      },
      "source": [
        "This code implements Q-learning, a fundamental reinforcement learning algorithm, to train an agent to navigate a simple 4x4 grid world. Let's break it down into three main parts:\n",
        "\n",
        "1.  **The `GridWorld` Environment**: This class defines the problem the agent needs to solve. It's a 4x4 grid where the agent can move Up, Down, Left, or Right. States 0 (top-left) and 15 (bottom-right) are terminal states (goals). Every step the agent takes incurs a reward of -1, encouraging it to find the shortest path to a goal. The `step` method calculates the next state, reward, and whether the episode is `done` based on the agent's action.\n",
        "\n",
        "2.  **The `q_learning` Algorithm**: This is the core of the reinforcement learning process.\n",
        "    *   It initializes `Q`, a 16x4 table (16 states, 4 actions), to store the estimated 'quality' (future reward) of taking a specific action in a specific state.\n",
        "    *   **Parameters**: `alpha` (learning rate) determines how much new information overrides old information. `gamma` (discount factor) emphasizes the importance of future rewards. `epsilon` (exploration rate) balances between exploring new actions and exploiting known good actions.\n",
        "    *   **Training Loop**: Over 5000 episodes, the agent interacts with the environment. In each step:\n",
        "        *   **Action Selection**: It uses an \"epsilon-greedy\" strategy: with `epsilon` probability, it picks a random action (exploration); otherwise, it chooses the action with the highest Q-value for the current state (exploitation).\n",
        "        *   **Q-Value Update**: After observing the `reward` and `next_state` from its action, it updates the Q-value for the `(state, action)` pair using the Bellman equation. This formula (`Q(s,a) = Q(s,a) + alpha * [Reward + gamma * max(Q(s')) - Q(s,a)]`) iteratively improves the agent's knowledge of which actions are best in each state.\n",
        "\n",
        "3.  **`print_results`**: This function visualizes the learned policy. For each non-terminal state, it determines the best action by finding the action with the highest Q-value and prints an arrow indicating that direction. Terminal states are marked with 'T'. It also shows an example of Q-values for a specific state to illustrate the learned values."
      ]
    }
  ]
}