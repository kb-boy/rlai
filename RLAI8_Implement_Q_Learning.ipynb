{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oXwCLptjrZ7K",
        "outputId": "b917f935-c2aa-43bb-9a4a-12f2a2c2d6f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training with Q-Learning (5000 Episodes)...\n",
            "\n",
            "Final Optimal Policy (Q-Learning):\n",
            "-----------------\n",
            " T | ← | ← | ↓ \n",
            "-----------------\n",
            " ↑ | ← | ↓ | ↓ \n",
            "-----------------\n",
            " ↑ | → | → | ↓ \n",
            "-----------------\n",
            " ↑ | → | → | T \n",
            "-----------------\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# --- 1. ENVIRONMENT (4x4 Grid World) ---\n",
        "class GridWorld:\n",
        "    def __init__(self):\n",
        "        self.grid_size = 4\n",
        "        self.terminal_states = [0, 15] # Top-Left, Bottom-Right\n",
        "        self.actions = [0, 1, 2, 3] # UP, DOWN, LEFT, RIGHT\n",
        "\n",
        "    def step(self, state, action):\n",
        "        if state in self.terminal_states:\n",
        "            return state, 0, True\n",
        "\n",
        "        row, col = divmod(state, self.grid_size)\n",
        "\n",
        "        # Move Logic\n",
        "        if action == 0:   row = max(row - 1, 0) # UP\n",
        "        elif action == 1: row = min(row + 1, self.grid_size - 1) # DOWN\n",
        "        elif action == 2: col = max(col - 1, 0) # LEFT\n",
        "        elif action == 3: col = min(col + 1, self.grid_size - 1) # RIGHT\n",
        "\n",
        "        next_state = row * self.grid_size + col\n",
        "        reward = -1\n",
        "        done = next_state in self.terminal_states\n",
        "        return next_state, reward, done\n",
        "\n",
        "    def reset(self):\n",
        "        start_state = np.random.randint(0, 16)\n",
        "        while start_state in self.terminal_states:\n",
        "            start_state = np.random.randint(0, 16)\n",
        "        return start_state\n",
        "\n",
        "# --- 2. THE ALGORITHM: Q-Learning ---\n",
        "def q_learning():\n",
        "    env = GridWorld()\n",
        "\n",
        "    # Parameters\n",
        "    num_episodes = 5000\n",
        "    alpha = 0.1   # Learning Rate\n",
        "    gamma = 1.0   # Discount Factor (1.0 because we want shortest path)\n",
        "    epsilon = 0.1 # Exploration Rate\n",
        "\n",
        "    # Initialize Q-Table (16 States x 4 Actions)\n",
        "    Q = np.zeros((16, 4))\n",
        "\n",
        "    print(\"Training with Q-Learning (5000 Episodes)...\")\n",
        "\n",
        "    for _ in range(num_episodes):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            # 1. Choose Action (Epsilon-Greedy)\n",
        "            if np.random.rand() < epsilon:\n",
        "                action = np.random.choice(env.actions) # Explore\n",
        "            else:\n",
        "                action = np.argmax(Q[state]) # Exploit\n",
        "\n",
        "            # 2. Take Action\n",
        "            next_state, reward, done = env.step(state, action)\n",
        "\n",
        "            # 3. Update Q-Value (Off-Policy)\n",
        "            # We use max(Q[next_state]) regardless of what action we actually take next\n",
        "            best_next_action_val = 0 if done else np.max(Q[next_state])\n",
        "\n",
        "            # Update Rule: Q(S,A) = Q(S,A) + alpha * [ R + gamma * max_a Q(S',a) - Q(S,A) ]\n",
        "            td_target = reward + gamma * best_next_action_val\n",
        "            Q[state, action] += alpha * (td_target - Q[state, action])\n",
        "\n",
        "            state = next_state\n",
        "\n",
        "    return Q\n",
        "\n",
        "# --- 3. EXECUTION & RESULTS ---\n",
        "def print_policy(Q):\n",
        "    actions_map = {0: '↑', 1: '↓', 2: '←', 3: '→'}\n",
        "    print(\"\\nFinal Optimal Policy (Q-Learning):\")\n",
        "    print(\"-\" * 17)\n",
        "\n",
        "    grid_output = []\n",
        "    for s in range(16):\n",
        "        if s in [0, 15]:\n",
        "            grid_output.append(\" T \")\n",
        "            continue\n",
        "        # Simply pick the best action from the learned table\n",
        "        best_action = np.argmax(Q[s])\n",
        "        grid_output.append(f\" {actions_map[best_action]} \")\n",
        "\n",
        "    for i in range(0, 16, 4):\n",
        "        print(\"|\".join(grid_output[i:i+4]))\n",
        "        print(\"-\" * 17)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    q_table = q_learning()\n",
        "    print_policy(q_table)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3e4a5a0"
      },
      "source": [
        "### Code Explanation\n",
        "\n",
        "This code implements a Q-learning algorithm to find an optimal policy for navigating a 4x4 grid world. Let's break it down:\n",
        "\n",
        "1.  **`GridWorld` Class**: This defines the environment. It's a 4x4 grid where `0` and `15` are terminal states (goals). It defines possible actions (UP, DOWN, LEFT, RIGHT) and a `step` function that determines the `next_state`, `reward` (-1 for each step, encouraging shortest paths), and whether the episode is `done`.\n",
        "\n",
        "2.  **`q_learning` Function**: This is the core reinforcement learning algorithm. It initializes a Q-table (state-action value table) to zeros. It then runs for a specified number of `episodes`:\n",
        "    *   **Epsilon-Greedy Action Selection**: In each step, it either explores (picks a random action with probability `epsilon`) or exploits (picks the action with the highest Q-value for the current state with probability `1 - epsilon`).\n",
        "    *   **Q-Value Update**: After taking an action and observing the `next_state` and `reward`, it updates the Q-value for the current `(state, action)` pair using the Bellman equation. This update incorporates the `learning rate (alpha)` and `discount factor (gamma)`.\n",
        "\n",
        "3.  **`print_policy` Function**: After Q-learning is complete, this function takes the learned Q-table and visualizes the optimal policy. For each non-terminal state, it determines the action with the highest Q-value and prints it as an arrow, showing the agent's preferred move from that state.\n",
        "\n",
        "In essence, the code trains an agent to find the shortest path from any starting state to a terminal state in the 4x4 grid."
      ]
    }
  ]
}