{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VTTfMjqotCT3",
        "outputId": "0bb3d1db-6be1-4ffa-8cca-9db387d7abd9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Monte Carlo Policy Evaluation (5000 Episodes)...\n",
            "\n",
            "\n",
            "Resulting Value Function (V):\n",
            "[[  0.  -14.4 -20.5 -22.3]\n",
            " [-14.  -18.2 -19.8 -19.8]\n",
            " [-20.1 -20.3 -18.4 -14.5]\n",
            " [-22.4 -20.  -13.8   0. ]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# 1. THE ENVIRONMENT (4x4 Grid)\n",
        "class GridWorld:\n",
        "    def __init__(self):\n",
        "        self.grid_size = 4\n",
        "        # Terminal states: Top-Left (0) and Bottom-Right (15)\n",
        "        self.terminal_states = [0, 15]\n",
        "        self.actions = ['UP', 'DOWN', 'LEFT', 'RIGHT']\n",
        "\n",
        "    def step(self, state, action):\n",
        "        if state in self.terminal_states:\n",
        "            return state, 0, True\n",
        "\n",
        "        # Move logic\n",
        "        row, col = divmod(state, self.grid_size)\n",
        "        if action == 'UP':    row = max(row - 1, 0)\n",
        "        elif action == 'DOWN':  row = min(row + 1, self.grid_size - 1)\n",
        "        elif action == 'LEFT':  col = max(col - 1, 0)\n",
        "        elif action == 'RIGHT': col = min(col + 1, self.grid_size - 1)\n",
        "\n",
        "        next_state = row * self.grid_size + col\n",
        "        reward = -1  # Standard penalty for each step\n",
        "        done = next_state in self.terminal_states\n",
        "        return next_state, reward, done\n",
        "\n",
        "    def reset(self):\n",
        "        # Start anywhere except the terminal states\n",
        "        start_state = np.random.randint(0, 16)\n",
        "        while start_state in self.terminal_states:\n",
        "            start_state = np.random.randint(0, 16)\n",
        "        return start_state\n",
        "\n",
        "# 2. THE POLICY (Random)\n",
        "def generate_episode(env):\n",
        "    episode = []\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    while not done:\n",
        "        # Random Policy: 25% chance for any direction\n",
        "        action = np.random.choice(env.actions)\n",
        "        next_state, reward, done = env.step(state, action)\n",
        "        episode.append((state, action, reward))\n",
        "        state = next_state\n",
        "    return episode\n",
        "\n",
        "# 3. THE ALGORITHM (First-Visit Monte Carlo Policy Evaluation)\n",
        "def mc_policy_evaluation(env, num_episodes=5000):\n",
        "    # Initialize Values to 0\n",
        "    V = np.zeros(env.grid_size * env.grid_size)\n",
        "\n",
        "    # Store all returns for every state\n",
        "    returns = {s: [] for s in range(env.grid_size * env.grid_size)}\n",
        "\n",
        "    for _ in range(num_episodes):\n",
        "        episode = generate_episode(env)\n",
        "        G = 0\n",
        "\n",
        "        # Work backwards from the end of the episode\n",
        "        for idx in range(len(episode) - 1, -1, -1):\n",
        "            state, action, reward = episode[idx]\n",
        "            G = G + reward # Gamma is 1.0, so G = G + R\n",
        "\n",
        "            # \"First-Visit\" Check:\n",
        "            # Only count the return if this was the first time\n",
        "            # we visited this state in this specific episode.\n",
        "            previous_states = [x[0] for x in episode[:idx]]\n",
        "            if state not in previous_states:\n",
        "                returns[state].append(G)\n",
        "                V[state] = np.mean(returns[state]) # Average the returns\n",
        "    return V\n",
        "\n",
        "# --- Main Driver ---\n",
        "if __name__ == \"__main__\":\n",
        "    env = GridWorld()\n",
        "    print(\"Running Monte Carlo Policy Evaluation (5000 Episodes)...\\n\")\n",
        "\n",
        "    # Run the algorithm\n",
        "    values = mc_policy_evaluation(env)\n",
        "\n",
        "    print(\"\\nResulting Value Function (V):\")\n",
        "    # Reshape to 4x4 for easy reading\n",
        "    print(np.round(values.reshape(4, 4), 1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f760ea7a"
      },
      "source": [
        "The code consists of three main parts:  \n",
        "1.  **The Environment (`GridWorld` class)**: Defines the 4x4 grid, actions, rewards, and how the state changes.  \n",
        "2.  **The Policy (`generate_episode` function)**: Describes how an agent behaves in the environment. Here, it's a simple random policy.  \n",
        "3.  **The Algorithm (`mc_policy_evaluation` function)**: Implements the First-Visit Monte Carlo method to estimate the value function of the random policy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "db9b8bc3"
      },
      "source": [
        "### 1. The Environment (`GridWorld` Class)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5e137b17"
      },
      "source": [
        "class GridWorld:\n",
        "    def __init__(self):\n",
        "        self.grid_size = 4\n",
        "        # Terminal states: Top-Left (0) and Bottom-Right (15)\n",
        "        self.terminal_states = [0, 15]\n",
        "        self.actions = ['UP', 'DOWN', 'LEFT', 'RIGHT']\n",
        "\n",
        "    def step(self, state, action):\n",
        "        if state in self.terminal_states:\n",
        "            return state, 0, True\n",
        "\n",
        "        # Move logic\n",
        "        row, col = divmod(state, self.grid_size)\n",
        "        if action == 'UP':    row = max(row - 1, 0)\n",
        "        elif action == 'DOWN':  row = min(row + 1, self.grid_size - 1)\n",
        "        elif action == 'LEFT':  col = max(col - 1, 0)\n",
        "        elif action == 'RIGHT': col = min(col + 1, self.grid_size - 1)\n",
        "\n",
        "        next_state = row * self.grid_size + col\n",
        "        reward = -1  # Standard penalty for each step\n",
        "        done = next_state in self.terminal_states\n",
        "        return next_state, reward, done\n",
        "\n",
        "    def reset(self):\n",
        "        # Start anywhere except the terminal states\n",
        "        start_state = np.random.randint(0, 16)\n",
        "        while start_state in self.terminal_states:\n",
        "            start_state = np.random.randint(0, 16)\n",
        "        return start_state"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "622b6f6f"
      },
      "source": [
        "This class defines the grid-world environment:\n",
        "*   It's a 4x4 grid, with states numbered 0 to 15.\n",
        "*   States 0 (top-left) and 15 (bottom-right) are *terminal states* where the episode ends.\n",
        "*   `actions` are 'UP', 'DOWN', 'LEFT', 'RIGHT'.\n",
        "*   `step(state, action)`: Takes a current state and an action, then returns the `next_state`, the `reward` (-1 for each step, 0 in terminal states), and whether the episode is `done`.\n",
        "*   `reset()`: Starts a new episode from a random non-terminal state."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "161b22a8"
      },
      "source": [
        "### 2. The Policy (`generate_episode` Function)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5537daf"
      },
      "source": [
        "def generate_episode(env):\n",
        "    episode = []\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    while not done:\n",
        "        # Random Policy: 25% chance for any direction\n",
        "        action = np.random.choice(env.actions)\n",
        "        next_state, reward, done = env.step(state, action)\n",
        "        episode.append((state, action, reward))\n",
        "        state = next_state\n",
        "    return episode"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15614fe6"
      },
      "source": [
        "This function simulates one full episode using a *random policy*:\n",
        "*   It starts from a `reset()` state.\n",
        "*   In each step, the agent chooses an action randomly (each action has a 25% chance).\n",
        "*   It records the sequence of `(state, action, reward)` tuples until a terminal state is reached. This sequence is called an `episode`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3495e42"
      },
      "source": [
        "### 3. The Algorithm (`mc_policy_evaluation` Function)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "09a6b89b"
      },
      "source": [
        "def mc_policy_evaluation(env, num_episodes=5000):\n",
        "    # Initialize Values to 0\n",
        "    V = np.zeros(env.grid_size * env.grid_size)\n",
        "\n",
        "    # Store all returns for every state\n",
        "    returns = {s: [] for s in range(env.grid_size * env.grid_size)}\n",
        "\n",
        "    for _ in range(num_episodes):\n",
        "        episode = generate_episode(env)\n",
        "        G = 0\n",
        "\n",
        "        # Work backwards from the end of the episode\n",
        "        for idx in range(len(episode) - 1, -1, -1):\n",
        "            state, action, reward = episode[idx]\n",
        "            G = G + reward # Gamma is 1.0, so G = G + R\n",
        "\n",
        "            # \"First-Visit\" Check:\n",
        "            # Only count the return if this was the first time\n",
        "            # we visited this state in this specific episode.\n",
        "            previous_states = [x[0] for x in episode[:idx]]\n",
        "            if state not in previous_states:\n",
        "                returns[state].append(G)\n",
        "                V[state] = np.mean(returns[state]) # Average the returns\n",
        "    return V"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93e55c78"
      },
      "source": [
        "This is the core of the Monte Carlo policy evaluation:\n",
        "*   **Goal**: To estimate the *value function* (`V`) for the random policy, which means calculating the expected total future reward (return) from each state.\n",
        "*   **Initialization**: `V` is initialized to zeros, and `returns` is a dictionary to store all observed returns for each state.\n",
        "*   **Episode Generation**: It runs `num_episodes` (default 5000), generating a full episode for each run.\n",
        "*   **Calculating Returns (`G`)**: For each episode, it iterates *backwards* from the end to calculate the return `G`. Since the discount factor (gamma) is implicitly 1.0, `G` is simply the sum of future rewards.\n",
        "*   **First-Visit MC**: For each state encountered in an episode, it only considers the *first time* that state was visited in that specific episode to calculate its return. This is the \"first-visit\" aspect of the algorithm.\n",
        "*   **Averaging Returns**: After calculating the return `G` for a state (on its first visit in an episode), it adds `G` to the list of `returns` for that state. The value `V[state]` is then updated by averaging all the returns observed for that state so far.\n",
        "*   By running many episodes and averaging the returns, `V` converges to the true value function of the random policy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2fc7f6c0"
      },
      "source": [
        "### Main Driver"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ad8493f8"
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "    env = GridWorld()\n",
        "    print(\"Running Monte Carlo Policy Evaluation (5000 Episodes)...\n",
        "\")\n",
        "\n",
        "    # Run the algorithm\n",
        "    values = mc_policy_evaluation(env)\n",
        "\n",
        "    print(\"\n",
        "Resulting Value Function (V):\")\n",
        "    # Reshape to 4x4 for easy reading\n",
        "    print(np.round(values.reshape(4, 4), 1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2f5824e0"
      },
      "source": [
        "This block initializes the `GridWorld` environment, calls the `mc_policy_evaluation` function to get the value function, and then prints the results in a 4x4 grid format for readability."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40fda8d7"
      },
      "source": [
        "The output `values` array, reshaped into a 4x4 grid, represents the **estimated expected return** (total future reward) if you start from that specific state and follow the random policy until a terminal state is reached.  \n",
        "\n",
        "Let's break down what these numbers mean:\n",
        "\n",
        "*   **Terminal States (0 and 15):**\n",
        "    *   `0.0` at the top-left `(0,0)` and bottom-right `(3,3)` positions. These are your terminal states. Once the agent reaches a terminal state, the episode ends, and no further rewards are accumulated. Hence, their value is 0.\n",
        "\n",
        "*   **Negative Values:**\n",
        "    *   All other states have negative values. This is because the `reward` for each step taken in the environment is `-1`. Therefore, the value of a state reflects the expected number of steps it will take, on average, to reach a terminal state from that particular starting state, multiplied by -1.\n",
        "\n",
        "*   **Interpretation of Magnitude:**\n",
        "    *   **Less Negative Values (e.g., -14.4, -13.8):** States closer to a terminal state (like those adjacent to the top-left `0` or bottom-right `15` cells) tend to have less negative values. This indicates that, on average, it takes fewer steps to reach a terminal state from these positions under a random policy.\n",
        "    *   **More Negative Values (e.g., -22.4, -20.5):** States further away from both terminal states (typically in the middle of the grid, or further along a path that tends to wander) have more negative values. This suggests that it takes a greater number of steps, on average, to reach a terminal state from these positions with a purely random movement strategy."
      ]
    }
  ]
}