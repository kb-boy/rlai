{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xxASCDoQIFkr",
        "outputId": "39386e8c-c4a4-4310-cb87-fcbad9c93dcf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running TD(0) Learning for 5000 episodes...\n",
            "\n",
            "State-Value Function V(s) learned via TD(0):\n",
            "------------------------------\n",
            "[[  0.   -15.46 -20.66 -23.22]\n",
            " [-14.83 -19.04 -20.04 -19.77]\n",
            " [-21.01 -19.24 -15.65  -9.54]\n",
            " [-21.42 -18.69 -14.07   0.  ]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# --- 1. ENVIRONMENT (4x4 Grid World) ---\n",
        "class GridWorld:\n",
        "    def __init__(self):\n",
        "        self.grid_size = 4\n",
        "        self.terminal_states = [0, 15] # Top-Left, Bottom-Right\n",
        "        self.actions = ['UP', 'DOWN', 'LEFT', 'RIGHT']\n",
        "\n",
        "    def step(self, state, action):\n",
        "        if state in self.terminal_states:\n",
        "            return state, 0, True\n",
        "\n",
        "        row, col = divmod(state, self.grid_size)\n",
        "\n",
        "        if action == 'UP':    row = max(row - 1, 0)\n",
        "        elif action == 'DOWN':  row = min(row + 1, self.grid_size - 1)\n",
        "        elif action == 'LEFT':  col = max(col - 1, 0)\n",
        "        elif action == 'RIGHT': col = min(col + 1, self.grid_size - 1)\n",
        "\n",
        "        next_state = row * self.grid_size + col\n",
        "        reward = -1\n",
        "        done = next_state in self.terminal_states\n",
        "        return next_state, reward, done\n",
        "\n",
        "    def reset(self):\n",
        "        start_state = np.random.randint(0, 16)\n",
        "        while start_state in self.terminal_states:\n",
        "            start_state = np.random.randint(0, 16)\n",
        "        return start_state\n",
        "\n",
        "# --- 2. THE ALGORITHM: TD(0) Prediction ---\n",
        "def td_zero_learning(env, num_episodes=5000, alpha=0.1, gamma=1.0):\n",
        "    \"\"\"\n",
        "    TD(0) evaluates the value of states V(s) under a random policy.\n",
        "    It updates V(s) after every single step.\n",
        "    \"\"\"\n",
        "    # Initialize V(s) to 0\n",
        "    V = np.zeros(env.grid_size * env.grid_size)\n",
        "\n",
        "    print(f\"Running TD(0) Learning for {num_episodes} episodes...\")\n",
        "\n",
        "    for _ in range(num_episodes):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            # Policy: Random Walk (Standard for evaluation)\n",
        "            action = np.random.choice(env.actions)\n",
        "\n",
        "            # Take step\n",
        "            next_state, reward, done = env.step(state, action)\n",
        "\n",
        "            # --- THE TD(0) UPDATE RULE ---\n",
        "            # 1. Prediction: What we thought V(s) was\n",
        "            current_value = V[state]\n",
        "\n",
        "            # 2. Target: Reward + Discounted Value of Next State\n",
        "            # (If next state is terminal, its value is 0)\n",
        "            next_state_value = 0 if done else V[next_state]\n",
        "            td_target = reward + gamma * next_state_value\n",
        "\n",
        "            # 3. Update: Nudge current value towards the target\n",
        "            # V(s) = V(s) + alpha * [ R + gamma*V(s') - V(s) ]\n",
        "            V[state] = current_value + alpha * (td_target - current_value)\n",
        "\n",
        "            state = next_state\n",
        "\n",
        "    return V\n",
        "\n",
        "# --- 3. EXECUTION ---\n",
        "if __name__ == \"__main__\":\n",
        "    env = GridWorld()\n",
        "\n",
        "    # Run TD(0)\n",
        "    v_values = td_zero_learning(env)\n",
        "\n",
        "    print(\"\\nState-Value Function V(s) learned via TD(0):\")\n",
        "    print(\"-\" * 30)\n",
        "\n",
        "    # Reshape and print\n",
        "    print(np.round(v_values.reshape(4, 4), 2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4591d0f"
      },
      "source": [
        "### Explanation of the TD(0) Learning Code\n",
        "\n",
        "This code implements a basic reinforcement learning example using the TD(0) (Temporal Difference(0)) prediction algorithm in a 4x4 GridWorld environment. Here's a breakdown:\n",
        "\n",
        "1.  **GridWorld Class**: This class defines the environment. It's a 4x4 grid where states are represented by integers from 0 to 15. States 0 (top-left) and 15 (bottom-right) are terminal states. The `step` method takes a current state and an action (UP, DOWN, LEFT, RIGHT) and returns the next state, a reward (which is -1 for every non-terminal step), and whether the next state is terminal. The `reset` method initializes the agent to a random non-terminal starting state.\n",
        "\n",
        "2.  **`td_zero_learning` Function**: This function implements the TD(0) algorithm to *evaluate* the value of each state (`V(s)`) under a given policy. In this case, the policy is a random walk, meaning at each step, the agent chooses an action randomly. The core of TD(0) is its update rule: `V(s) = V(s) + alpha * [ R + gamma*V(s') - V(s) ]`. It updates the estimated value of the current state (`V[state]`) based on the immediate reward (`R`), the discounted value of the next state (`gamma*V[next_state]`), and its current prediction (`V[state]`).\n",
        "\n",
        "3.  **Execution Block**: The `if __name__ == \"__main__\":` block creates an instance of the `GridWorld` environment, runs the `td_zero_learning` function for 5000 episodes, and then prints the learned state-value function `V(s)` for all states, reshaped into a 4x4 grid for better visualization."
      ]
    }
  ]
}