{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r9RngrdFevU6",
        "outputId": "d1358a5f-6c23-4b89-9ca8-577eee5c7654"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running TD(0.5) Learning...\n",
            "\n",
            "State-Value Function V(s) via TD(0.5):\n",
            "------------------------------\n",
            "[[  0.   -12.85 -18.76 -20.62]\n",
            " [-12.41 -15.71 -19.13 -19.47]\n",
            " [-17.95 -19.31 -17.2  -13.9 ]\n",
            " [-20.94 -19.75 -13.09   0.  ]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# --- 1. ENVIRONMENT ---\n",
        "class GridWorld:\n",
        "    def __init__(self):\n",
        "        self.grid_size = 4\n",
        "        self.terminal_states = [0, 15]\n",
        "        self.actions = ['UP', 'DOWN', 'LEFT', 'RIGHT']\n",
        "\n",
        "    def step(self, state, action):\n",
        "        if state in self.terminal_states:\n",
        "            return state, 0, True\n",
        "\n",
        "        row, col = divmod(state, self.grid_size)\n",
        "\n",
        "        if action == 'UP':    row = max(row - 1, 0)\n",
        "        elif action == 'DOWN':  row = min(row + 1, self.grid_size - 1)\n",
        "        elif action == 'LEFT':  col = max(col - 1, 0)\n",
        "        elif action == 'RIGHT': col = min(col + 1, self.grid_size - 1)\n",
        "\n",
        "        next_state = row * self.grid_size + col\n",
        "        reward = -1\n",
        "        done = next_state in self.terminal_states\n",
        "        return next_state, reward, done\n",
        "\n",
        "    def reset(self):\n",
        "        start_state = np.random.randint(0, 16)\n",
        "        while start_state in self.terminal_states:\n",
        "            start_state = np.random.randint(0, 16)\n",
        "        return start_state\n",
        "\n",
        "# --- 2. THE ALGORITHM: TD(Lambda) ---\n",
        "def td_lambda_learning(env, lam=0.5, num_episodes=5000, alpha=0.1, gamma=1.0):\n",
        "    \"\"\"\n",
        "    TD(Lambda) uses 'Eligibility Traces' (E) to update past states.\n",
        "    lam (lambda): 0 = TD(0), 1 = Monte Carlo.\n",
        "    \"\"\"\n",
        "    # Initialize V(s)\n",
        "    V = np.zeros(env.grid_size * env.grid_size)\n",
        "\n",
        "    print(f\"Running TD({lam}) Learning...\")\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        # Reset Eligibility Traces at start of every episode\n",
        "        E = np.zeros(env.grid_size * env.grid_size)\n",
        "\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            # Policy: Random Walk\n",
        "            action = np.random.choice(env.actions)\n",
        "            next_state, reward, done = env.step(state, action)\n",
        "\n",
        "            # 1. Calculate TD Error (delta)\n",
        "            # The difference between what we expect and what we got\n",
        "            target = reward + (0 if done else gamma * V[next_state])\n",
        "            delta = target - V[state]\n",
        "\n",
        "            # 2. Increment Trace for CURRENT state\n",
        "            # \"I was just here, so I deserve credit/blame for this reward\"\n",
        "            E[state] += 1\n",
        "\n",
        "            # 3. Update V for ALL states based on their Trace\n",
        "            # V(s) = V(s) + alpha * delta * E(s)\n",
        "            # (Vectorized update for efficiency)\n",
        "            V += alpha * delta * E\n",
        "\n",
        "            # 4. Decay Traces for ALL states\n",
        "            # Memories fade over time: E = gamma * lambda * E\n",
        "            E *= gamma * lam\n",
        "\n",
        "            state = next_state\n",
        "\n",
        "    return V\n",
        "\n",
        "# --- 3. EXECUTION ---\n",
        "if __name__ == \"__main__\":\n",
        "    env = GridWorld()\n",
        "\n",
        "    # Run TD(Lambda) with lambda = 0.5\n",
        "    v_values = td_lambda_learning(env, lam=0.5)\n",
        "\n",
        "    print(f\"\\nState-Value Function V(s) via TD(0.5):\")\n",
        "    print(\"-\" * 30)\n",
        "    print(np.round(v_values.reshape(4, 4), 2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01d067ec"
      },
      "source": [
        "### Explanation of the TD(Lambda) Code\n",
        "\n",
        "This code implements the TD(Lambda) reinforcement learning algorithm to estimate the state-value function for a simple 4x4 grid world environment. Let's break it down into its three main parts:\n",
        "\n",
        "1.  **GridWorld Environment:** This `GridWorld` class defines a 4x4 grid. States are numbered 0 to 15. It has two terminal states (0 and 15) and four possible actions (UP, DOWN, LEFT, RIGHT). The `step` method takes a state and an action, returning the next state, a reward of -1 for each step (encouraging shorter paths), and whether the episode is done. The `reset` method places the agent in a random non-terminal starting state.\n",
        "\n",
        "2.  **TD(Lambda) Algorithm:** The `td_lambda_learning` function is the core of the reinforcement learning agent. It aims to learn the value `V(s)` for each state `s`, representing the expected future reward from that state. It initializes a value function `V` (all zeros) and then iterates over many episodes:\n",
        "    *   **Eligibility Traces (`E`):** At the beginning of each episode, eligibility traces are reset. These traces keep a memory of recently visited states, giving them credit or blame for rewards received later.\n",
        "    *   **Random Walk Policy:** The agent explores the environment by choosing actions randomly.\n",
        "    *   **TD Error Calculation:** For each step, it calculates the TD error (`delta`), which is the difference between the expected return from the current state and the actual observed return (reward + discounted value of the next state).\n",
        "    *   **Updating Traces:** The trace for the current state is incremented.\n",
        "    *   **Value Function Update:** The value function `V` for all states is updated based on their eligibility trace and the TD error. States with higher eligibility traces (meaning they were visited more recently or frequently) are updated more significantly.\n",
        "    *   **Decaying Traces:** Eligibility traces for all states are decayed over time, reflecting that older memories fade. The `lam` (lambda) parameter controls this decay, effectively blending between TD(0) (immediate reward consideration, `lam=0`) and Monte Carlo (full episode consideration, `lam=1`).\n",
        "\n",
        "3.  **Execution:** This section creates an instance of the `GridWorld` environment and then calls the `td_lambda_learning` function with a lambda value of 0.5. Finally, it prints the learned state-value function `V(s)` for all states, reshaped into a 4x4 grid for easy visualization."
      ]
    }
  ]
}