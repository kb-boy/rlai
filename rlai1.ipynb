{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nGsE6SmzCk11",
        "outputId": "36a12bc2-ce3b-4eed-c6f8-f206d6ee4771"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Monte Carlo Policy Evaluation (5000 Episodes)...\n",
            "\n",
            "wait wait wait ....\n",
            "\n",
            "Resulting Value Function (V):\n",
            "[[  0.  -13.6 -19.7 -22.1]\n",
            " [-13.9 -17.6 -19.6 -20. ]\n",
            " [-19.7 -19.3 -17.9 -14.4]\n",
            " [-21.9 -19.7 -13.5   0. ]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# 1. THE ENVIRONMENT (4x4 Grid)\n",
        "class GridWorld:\n",
        "    def __init__(self):\n",
        "        self.grid_size = 4\n",
        "        # Terminal states: Top-Left (0) and Bottom-Right (15)\n",
        "        self.terminal_states = [0, 15]\n",
        "        self.actions = ['UP', 'DOWN', 'LEFT', 'RIGHT']\n",
        "\n",
        "    def step(self, state, action):\n",
        "        if state in self.terminal_states:\n",
        "            return state, 0, True\n",
        "\n",
        "        # Move logic\n",
        "        row, col = divmod(state, self.grid_size)\n",
        "        if action == 'UP':    row = max(row - 1, 0)\n",
        "        elif action == 'DOWN':  row = min(row + 1, self.grid_size - 1)\n",
        "        elif action == 'LEFT':  col = max(col - 1, 0)\n",
        "        elif action == 'RIGHT': col = min(col + 1, self.grid_size - 1)\n",
        "\n",
        "        next_state = row * self.grid_size + col\n",
        "        reward = -1  # Standard penalty for each step\n",
        "        done = next_state in self.terminal_states\n",
        "        return next_state, reward, done\n",
        "\n",
        "    def reset(self):\n",
        "        # Start anywhere except the terminal states\n",
        "        start_state = np.random.randint(0, 16)\n",
        "        while start_state in self.terminal_states:\n",
        "            start_state = np.random.randint(0, 16)\n",
        "        return start_state\n",
        "\n",
        "# 2. THE POLICY (Random)\n",
        "def generate_episode(env):\n",
        "    episode = []\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    while not done:\n",
        "        # Random Policy: 25% chance for any direction\n",
        "        action = np.random.choice(env.actions)\n",
        "        next_state, reward, done = env.step(state, action)\n",
        "        episode.append((state, action, reward))\n",
        "        state = next_state\n",
        "    return episode\n",
        "\n",
        "# 3. THE ALGORITHM (First-Visit Monte Carlo Policy Evaluation)\n",
        "def mc_policy_evaluation(env, num_episodes=5000):\n",
        "    # Initialize Values to 0\n",
        "    V = np.zeros(env.grid_size * env.grid_size)\n",
        "\n",
        "    # Store all returns for every state\n",
        "    returns = {s: [] for s in range(env.grid_size * env.grid_size)}\n",
        "\n",
        "    for _ in range(num_episodes):\n",
        "        episode = generate_episode(env)\n",
        "        G = 0\n",
        "\n",
        "        # Work backwards from the end of the episode\n",
        "        for idx in range(len(episode) - 1, -1, -1):\n",
        "            state, action, reward = episode[idx]\n",
        "            G = G + reward # Gamma is 1.0, so G = G + R\n",
        "\n",
        "            # \"First-Visit\" Check:\n",
        "            # Only count the return if this was the first time\n",
        "            # we visited this state in this specific episode.\n",
        "            previous_states = [x[0] for x in episode[:idx]]\n",
        "            if state not in previous_states:\n",
        "                returns[state].append(G)\n",
        "                V[state] = np.mean(returns[state]) # Average the returns\n",
        "    return V\n",
        "\n",
        "# --- Main Driver ---\n",
        "if __name__ == \"__main__\":\n",
        "    env = GridWorld()\n",
        "    print(\"Running Monte Carlo Policy Evaluation (5000 Episodes)...\\n\")\n",
        "\n",
        "    print(\"wait wait wait ....\")\n",
        "\n",
        "    # Run the algorithm\n",
        "    values = mc_policy_evaluation(env)\n",
        "\n",
        "    print(\"\\nResulting Value Function (V):\")\n",
        "    # Reshape to 4x4 for easy reading\n",
        "    print(np.round(values.reshape(4, 4), 1))"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MX2zycu9Ga0-"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ecjz2pcQGaqT"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "prUgX9kbGahS"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def value_iteration_grid_world():\n",
        "    # --- 1. SETUP THE ENVIRONMENT ---\n",
        "    grid_size = 4\n",
        "    gamma = 1.0  # Discount factor (1.0 for shortest path)\n",
        "    theta = 1e-4 # Convergence threshold\n",
        "\n",
        "    # Terminal states (Top-Left and Bottom-Right)\n",
        "    terminal_states = [0, 15]\n",
        "    actions = ['UP', 'DOWN', 'LEFT', 'RIGHT']\n",
        "\n",
        "    # Initialize Value Function V(s) to zeros\n",
        "    V = np.zeros(grid_size * grid_size)\n",
        "\n",
        "    print(\"Starting Value Iteration...\")\n",
        "    iteration = 0\n",
        "\n",
        "    # --- 2. THE ALGORITHM (Value Iteration) ---\n",
        "    while True:\n",
        "        delta = 0\n",
        "        # Create a copy for synchronous updates\n",
        "        V_new = np.copy(V)\n",
        "\n",
        "        # Loop over every state in the world\n",
        "        for s in range(grid_size * grid_size):\n",
        "            if s in terminal_states:\n",
        "                continue # Value of terminal state is always 0\n",
        "\n",
        "            # Calculate coordinates\n",
        "            row, col = divmod(s, grid_size)\n",
        "\n",
        "            # Find the max value among all possible actions\n",
        "            action_values = []\n",
        "\n",
        "            for action in actions:\n",
        "                # --- MODEL LOGIC (Simulating the move) ---\n",
        "                next_r, next_c = row, col\n",
        "\n",
        "                if action == 'UP':    next_r = max(row - 1, 0)\n",
        "                elif action == 'DOWN':  next_r = min(row + 1, grid_size - 1)\n",
        "                elif action == 'LEFT':  next_c = max(col - 1, 0)\n",
        "                elif action == 'RIGHT': next_c = min(col + 1, grid_size - 1)\n",
        "\n",
        "                next_state = next_r * grid_size + next_c\n",
        "\n",
        "                # Standard Reward is -1 per step\n",
        "                reward = -1\n",
        "\n",
        "                # Bellman Optimality Equation: R + gamma * V(s')\n",
        "                val = reward + gamma * V[next_state]\n",
        "                action_values.append(val)\n",
        "\n",
        "            # Update V(s) with the BEST possible action (Greedy)\n",
        "            best_value = max(action_values)\n",
        "            V_new[s] = best_value\n",
        "\n",
        "            # Check how much the value changed\n",
        "            delta = max(delta, abs(best_value - V[s]))\n",
        "\n",
        "        V = V_new\n",
        "        iteration += 1\n",
        "\n",
        "        # Stop if converged\n",
        "        if delta < theta:\n",
        "            print(f\"Converged after {iteration} iterations.\")\n",
        "            break\n",
        "\n",
        "    # --- 3. DISPLAY RESULTS ---\n",
        "    print(\"\\nOptimal Value Function (V*):\")\n",
        "    print(np.round(V.reshape(4, 4), 1))\n",
        "\n",
        "    # Optional: Display the Optimal Policy (Arrows)\n",
        "    print(\"\\nOptimal Policy (Planning Result):\")\n",
        "    arrows = {0: '↑', 1: '↓', 2: '←', 3: '→'}\n",
        "    policy_grid = []\n",
        "\n",
        "    for s in range(grid_size * grid_size):\n",
        "        if s in terminal_states:\n",
        "            policy_grid.append(\" T \")\n",
        "            continue\n",
        "\n",
        "        row, col = divmod(s, grid_size)\n",
        "        best_action_idx = -1\n",
        "        best_val = -float('inf')\n",
        "\n",
        "        # Check neighbors again to find which one gave that best value\n",
        "        for i, action in enumerate(actions):\n",
        "            next_r, next_c = row, col\n",
        "            if action == 'UP':    next_r = max(row - 1, 0)\n",
        "            elif action == 'DOWN':  next_r = min(row + 1, grid_size - 1)\n",
        "            elif action == 'LEFT':  next_c = max(col - 1, 0)\n",
        "            elif action == 'RIGHT': next_c = min(col + 1, grid_size - 1)\n",
        "\n",
        "            val = -1 + gamma * V[next_r * grid_size + next_c]\n",
        "            if val > best_val:\n",
        "                best_val = val\n",
        "                best_action_idx = i\n",
        "\n",
        "        policy_grid.append(f\" {arrows[best_action_idx]} \")\n",
        "\n",
        "    # Print Policy Grid\n",
        "    print(\"-\" * 17)\n",
        "    for i in range(0, 16, 4):\n",
        "        print(\"|\".join(policy_grid[i:i+4]))\n",
        "        print(\"-\" * 17)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    value_iteration_grid_world()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pqjh13sgDtnj",
        "outputId": "d525a37e-408b-4d00-a548-7fc41a56b0c5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Value Iteration...\n",
            "Converged after 4 iterations.\n",
            "\n",
            "Optimal Value Function (V*):\n",
            "[[ 0. -1. -2. -3.]\n",
            " [-1. -2. -3. -2.]\n",
            " [-2. -3. -2. -1.]\n",
            " [-3. -2. -1.  0.]]\n",
            "\n",
            "Optimal Policy (Planning Result):\n",
            "-----------------\n",
            " T | ← | ← | ↓ \n",
            "-----------------\n",
            " ↑ | ↑ | ↑ | ↓ \n",
            "-----------------\n",
            " ↑ | ↑ | ↓ | ↓ \n",
            "-----------------\n",
            " ↑ | → | → | T \n",
            "-----------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UhmuuABLGbo_"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mKr9wGwoGq9C"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5SgM2KsAGqx7"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class MDPGridWorld:\n",
        "    def __init__(self):\n",
        "        self.grid_size = 4\n",
        "        # For \"Rollout from (0,0)\", we treat (0,0) as Start\n",
        "        # and (3,3) [index 15] as the ONLY Goal/Terminal state.\n",
        "        self.terminal_states = [15]\n",
        "        self.actions = ['UP', 'DOWN', 'LEFT', 'RIGHT']\n",
        "        self.gamma = 1.0 # Discount factor\n",
        "\n",
        "    def get_next_state_reward(self, state, action):\n",
        "        if state in self.terminal_states:\n",
        "            return state, 0\n",
        "\n",
        "        row, col = divmod(state, self.grid_size)\n",
        "\n",
        "        # Move Logic\n",
        "        if action == 'UP':    row = max(row - 1, 0)\n",
        "        elif action == 'DOWN':  row = min(row + 1, self.grid_size - 1)\n",
        "        elif action == 'LEFT':  col = max(col - 1, 0)\n",
        "        elif action == 'RIGHT': col = min(col + 1, self.grid_size - 1)\n",
        "\n",
        "        next_state = row * self.grid_size + col\n",
        "        reward = -1 # Cost per step\n",
        "        return next_state, reward\n",
        "\n",
        "    def value_iteration(self):\n",
        "        \"\"\"1. PLAN: Calculate V* to find the best policy.\"\"\"\n",
        "        V = np.zeros(self.grid_size * self.grid_size)\n",
        "        theta = 1e-4\n",
        "\n",
        "        while True:\n",
        "            delta = 0\n",
        "            V_new = np.copy(V)\n",
        "            for s in range(self.grid_size * self.grid_size):\n",
        "                if s in self.terminal_states: continue\n",
        "\n",
        "                action_values = []\n",
        "                for a in self.actions:\n",
        "                    ns, r = self.get_next_state_reward(s, a)\n",
        "                    action_values.append(r + self.gamma * V[ns])\n",
        "\n",
        "                new_val = max(action_values)\n",
        "                V_new[s] = new_val\n",
        "                delta = max(delta, abs(new_val - V[s]))\n",
        "            V = V_new\n",
        "            if delta < theta: break\n",
        "        return V\n",
        "\n",
        "    def rollout(self, start_state, V):\n",
        "        \"\"\"2. ACT: Execute the policy from the start state.\"\"\"\n",
        "        print(f\"\\n--- Rolling out Optimal Policy from State {start_state} (0,0) ---\")\n",
        "\n",
        "        curr_state = start_state\n",
        "        steps = 0\n",
        "        path = [curr_state]\n",
        "\n",
        "        while curr_state not in self.terminal_states:\n",
        "            row, col = divmod(curr_state, self.grid_size)\n",
        "\n",
        "            # Find Best Action using V\n",
        "            best_action = None\n",
        "            best_val = -float('inf')\n",
        "\n",
        "            for action in self.actions:\n",
        "                ns, r = self.get_next_state_reward(curr_state, action)\n",
        "                val = r + self.gamma * V[ns]\n",
        "\n",
        "                if val > best_val:\n",
        "                    best_val = val\n",
        "                    best_action = action\n",
        "\n",
        "            # Execute the Move\n",
        "            next_state, _ = self.get_next_state_reward(curr_state, best_action)\n",
        "\n",
        "            print(f\"Step {steps+1}: At {curr_state} ({row},{col}) -> Action: {best_action} -> New State: {next_state}\")\n",
        "\n",
        "            curr_state = next_state\n",
        "            path.append(curr_state)\n",
        "            steps += 1\n",
        "\n",
        "            if steps > 20: # Safety break\n",
        "                print(\"Stuck in loop!\")\n",
        "                break\n",
        "\n",
        "        print(f\"\\nGoal Reached at State {curr_state}!\")\n",
        "        print(f\"Total Path: {path}\")\n",
        "\n",
        "# --- Main Execution ---\n",
        "if __name__ == \"__main__\":\n",
        "    world = MDPGridWorld()\n",
        "\n",
        "    # 1. First, we need the plan (Value Iteration)\n",
        "    print(\"Computing Optimal Policy...\")\n",
        "    optimal_values = world.value_iteration()\n",
        "\n",
        "    # 2. Now, we Roll Out (Simulate) the plan from (0,0)\n",
        "    world.rollout(start_state=0, V=optimal_values)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gAhjz_14Gqk6",
        "outputId": "cb3f178e-2fd9-4a1e-d98d-356c9cbe4565"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computing Optimal Policy...\n",
            "\n",
            "--- Rolling out Optimal Policy from State 0 (0,0) ---\n",
            "Step 1: At 0 (0,0) -> Action: DOWN -> New State: 4\n",
            "Step 2: At 4 (1,0) -> Action: DOWN -> New State: 8\n",
            "Step 3: At 8 (2,0) -> Action: DOWN -> New State: 12\n",
            "Step 4: At 12 (3,0) -> Action: RIGHT -> New State: 13\n",
            "Step 5: At 13 (3,1) -> Action: RIGHT -> New State: 14\n",
            "Step 6: At 14 (3,2) -> Action: RIGHT -> New State: 15\n",
            "\n",
            "Goal Reached at State 15!\n",
            "Total Path: [0, 4, 8, 12, 13, 14, 15]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aEc61rqvGrvZ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LFwxkxJSG180"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "k9D2KeFGG12x"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# --- 1. THE ENVIRONMENT (Unknown to the agent initially) ---\n",
        "class GridWorld:\n",
        "    def __init__(self):\n",
        "        self.grid_size = 4\n",
        "        self.terminal_states = [0, 15] # Top-Left, Bottom-Right\n",
        "        self.actions = [0, 1, 2, 3] # 0:UP, 1:DOWN, 2:LEFT, 3:RIGHT\n",
        "\n",
        "    def step(self, state, action):\n",
        "        if state in self.terminal_states:\n",
        "            return state, 0, True\n",
        "\n",
        "        row, col = divmod(state, self.grid_size)\n",
        "\n",
        "        # Action Logic\n",
        "        if action == 0:   row = max(row - 1, 0) # UP\n",
        "        elif action == 1: row = min(row + 1, self.grid_size - 1) # DOWN\n",
        "        elif action == 2: col = max(col - 1, 0) # LEFT\n",
        "        elif action == 3: col = min(col + 1, self.grid_size - 1) # RIGHT\n",
        "\n",
        "        next_state = row * self.grid_size + col\n",
        "        reward = -1 # Penalty for each step\n",
        "        done = next_state in self.terminal_states\n",
        "\n",
        "        return next_state, reward, done\n",
        "\n",
        "    def reset(self):\n",
        "        start_state = np.random.randint(0, 16)\n",
        "        while start_state in self.terminal_states:\n",
        "            start_state = np.random.randint(0, 16)\n",
        "        return start_state\n",
        "\n",
        "# --- 2. THE ALGORITHM (Q-Learning) ---\n",
        "def q_learning():\n",
        "    env = GridWorld()\n",
        "\n",
        "    # Parameters\n",
        "    num_episodes = 5000\n",
        "    alpha = 0.1   # Learning Rate (How fast we accept new info)\n",
        "    gamma = 0.99  # Discount Factor (Importance of future rewards)\n",
        "    epsilon = 0.1 # Exploration Rate (Chance to try random move)\n",
        "\n",
        "    # Initialize Q-Table: 16 States x 4 Actions\n",
        "    # Q[s, a] stores the value of taking action 'a' in state 's'\n",
        "    Q = np.zeros((16, 4))\n",
        "\n",
        "    print(\"Training with Q-Learning (5000 Episodes)...\")\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            # A. Choose Action (Epsilon-Greedy)\n",
        "            if np.random.rand() < epsilon:\n",
        "                action = np.random.choice(env.actions) # Explore (Random)\n",
        "            else:\n",
        "                action = np.argmax(Q[state]) # Exploit (Best known action)\n",
        "\n",
        "            # B. Take Action & Observe\n",
        "            next_state, reward, done = env.step(state, action)\n",
        "\n",
        "            # C. Update Q-Value (The Bellman Update Rule)\n",
        "            # Old Value\n",
        "            old_value = Q[state, action]\n",
        "            # Best possible value from next state\n",
        "            next_max = np.max(Q[next_state])\n",
        "\n",
        "            # Formula: Q(s,a) = Q(s,a) + alpha * [Reward + gamma * max(Q(s')) - Q(s,a)]\n",
        "            new_value = old_value + alpha * (reward + gamma * next_max - old_value)\n",
        "\n",
        "            Q[state, action] = new_value\n",
        "\n",
        "            state = next_state\n",
        "\n",
        "    return Q\n",
        "\n",
        "# --- 3. DISPLAY RESULTS ---\n",
        "def print_results(Q):\n",
        "    actions_map = {0: '↑', 1: '↓', 2: '←', 3: '→'}\n",
        "    print(\"\\nLearned Policy (from Q-Table):\")\n",
        "    print(\"-\" * 17)\n",
        "\n",
        "    grid_output = []\n",
        "    for s in range(16):\n",
        "        if s in [0, 15]: # Terminal\n",
        "            grid_output.append(\" T \")\n",
        "            continue\n",
        "\n",
        "        # The best action is the one with the highest Q-value for this state\n",
        "        best_action_idx = np.argmax(Q[s])\n",
        "        grid_output.append(f\" {actions_map[best_action_idx]} \")\n",
        "\n",
        "    # Print nicely\n",
        "    for i in range(0, 16, 4):\n",
        "        print(\"|\".join(grid_output[i:i+4]))\n",
        "        print(\"-\" * 17)\n",
        "\n",
        "    print(\"\\nExample Q-Values for State 1 (Next to Top-Left Goal):\")\n",
        "    print(f\"UP: {Q[1,0]:.2f}, DOWN: {Q[1,1]:.2f}, LEFT: {Q[1,2]:.2f}, RIGHT: {Q[1,3]:.2f}\")\n",
        "    print(\"(Notice 'LEFT' should have the highest value because it leads to the goal)\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    final_Q = q_learning()\n",
        "    print_results(final_Q)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AL6JJaYiG1ub",
        "outputId": "a952f329-8bac-41e9-f8fa-921ceed2a736"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training with Q-Learning (5000 Episodes)...\n",
            "\n",
            "Learned Policy (from Q-Table):\n",
            "-----------------\n",
            " T | ← | ← | ↓ \n",
            "-----------------\n",
            " ↑ | ↑ | ↑ | ↓ \n",
            "-----------------\n",
            " ↑ | ↓ | → | ↓ \n",
            "-----------------\n",
            " → | → | → | T \n",
            "-----------------\n",
            "\n",
            "Example Q-Values for State 1 (Next to Top-Left Goal):\n",
            "UP: -1.95, DOWN: -2.89, LEFT: -1.00, RIGHT: -2.90\n",
            "(Notice 'LEFT' should have the highest value because it leads to the goal)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pIHSqqbQG1lY"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pzyg7zYDHJ6k"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jp9vTBLuHJx6"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# --- 1. ENVIRONMENT (4x4 Grid World) ---\n",
        "class GridWorld:\n",
        "    def __init__(self):\n",
        "        self.grid_size = 4\n",
        "        self.terminal_states = [0, 15] # Top-Left, Bottom-Right\n",
        "        self.actions = ['UP', 'DOWN', 'LEFT', 'RIGHT']\n",
        "\n",
        "    def step(self, state, action):\n",
        "        if state in self.terminal_states:\n",
        "            return state, 0, True\n",
        "\n",
        "        row, col = divmod(state, self.grid_size)\n",
        "\n",
        "        if action == 'UP':    row = max(row - 1, 0)\n",
        "        elif action == 'DOWN':  row = min(row + 1, self.grid_size - 1)\n",
        "        elif action == 'LEFT':  col = max(col - 1, 0)\n",
        "        elif action == 'RIGHT': col = min(col + 1, self.grid_size - 1)\n",
        "\n",
        "        next_state = row * self.grid_size + col\n",
        "        reward = -1\n",
        "        done = next_state in self.terminal_states\n",
        "        return next_state, reward, done\n",
        "\n",
        "    def reset(self):\n",
        "        start_state = np.random.randint(0, 16)\n",
        "        while start_state in self.terminal_states:\n",
        "            start_state = np.random.randint(0, 16)\n",
        "        return start_state\n",
        "\n",
        "# --- 2. THE ALGORITHM: TD(0) Prediction ---\n",
        "def td_zero_learning(env, num_episodes=5000, alpha=0.1, gamma=1.0):\n",
        "    \"\"\"\n",
        "    TD(0) evaluates the value of states V(s) under a random policy.\n",
        "    It updates V(s) after every single step.\n",
        "    \"\"\"\n",
        "    # Initialize V(s) to 0\n",
        "    V = np.zeros(env.grid_size * env.grid_size)\n",
        "\n",
        "    print(f\"Running TD(0) Learning for {num_episodes} episodes...\")\n",
        "\n",
        "    for _ in range(num_episodes):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            # Policy: Random Walk (Standard for evaluation)\n",
        "            action = np.random.choice(env.actions)\n",
        "\n",
        "            # Take step\n",
        "            next_state, reward, done = env.step(state, action)\n",
        "\n",
        "            # --- THE TD(0) UPDATE RULE ---\n",
        "            # 1. Prediction: What we thought V(s) was\n",
        "            current_value = V[state]\n",
        "\n",
        "            # 2. Target: Reward + Discounted Value of Next State\n",
        "            # (If next state is terminal, its value is 0)\n",
        "            next_state_value = 0 if done else V[next_state]\n",
        "            td_target = reward + gamma * next_state_value\n",
        "\n",
        "            # 3. Update: Nudge current value towards the target\n",
        "            # V(s) = V(s) + alpha * [ R + gamma*V(s') - V(s) ]\n",
        "            V[state] = current_value + alpha * (td_target - current_value)\n",
        "\n",
        "            state = next_state\n",
        "\n",
        "    return V\n",
        "\n",
        "# --- 3. EXECUTION ---\n",
        "if __name__ == \"__main__\":\n",
        "    env = GridWorld()\n",
        "\n",
        "    # Run TD(0)\n",
        "    v_values = td_zero_learning(env)\n",
        "\n",
        "    print(\"\\nState-Value Function V(s) learned via TD(0):\")\n",
        "    print(\"-\" * 30)\n",
        "\n",
        "    # Reshape and print\n",
        "    print(np.round(v_values.reshape(4, 4), 2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KZGEhVX4HJoS",
        "outputId": "87776e27-dba3-4149-8b39-7e6559998824"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running TD(0) Learning for 5000 episodes...\n",
            "\n",
            "State-Value Function V(s) learned via TD(0):\n",
            "------------------------------\n",
            "[[  0.   -12.72 -19.53 -23.33]\n",
            " [-11.67 -18.15 -21.02 -21.98]\n",
            " [-19.48 -20.58 -18.86 -14.97]\n",
            " [-22.69 -21.02 -10.74   0.  ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ps_rqjdRHkjl"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kVTEUN5mHwrf"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jIt_vcJMHxBK"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# --- 1. ENVIRONMENT ---\n",
        "class GridWorld:\n",
        "    def __init__(self):\n",
        "        self.grid_size = 4\n",
        "        self.terminal_states = [0, 15]\n",
        "        self.actions = ['UP', 'DOWN', 'LEFT', 'RIGHT']\n",
        "\n",
        "    def step(self, state, action):\n",
        "        if state in self.terminal_states:\n",
        "            return state, 0, True\n",
        "\n",
        "        row, col = divmod(state, self.grid_size)\n",
        "\n",
        "        if action == 'UP':    row = max(row - 1, 0)\n",
        "        elif action == 'DOWN':  row = min(row + 1, self.grid_size - 1)\n",
        "        elif action == 'LEFT':  col = max(col - 1, 0)\n",
        "        elif action == 'RIGHT': col = min(col + 1, self.grid_size - 1)\n",
        "\n",
        "        next_state = row * self.grid_size + col\n",
        "        reward = -1\n",
        "        done = next_state in self.terminal_states\n",
        "        return next_state, reward, done\n",
        "\n",
        "    def reset(self):\n",
        "        start_state = np.random.randint(0, 16)\n",
        "        while start_state in self.terminal_states:\n",
        "            start_state = np.random.randint(0, 16)\n",
        "        return start_state\n",
        "\n",
        "# --- 2. THE ALGORITHM: TD(Lambda) ---\n",
        "def td_lambda_learning(env, lam=0.5, num_episodes=5000, alpha=0.1, gamma=1.0):\n",
        "    \"\"\"\n",
        "    TD(Lambda) uses 'Eligibility Traces' (E) to update past states.\n",
        "    lam (lambda): 0 = TD(0), 1 = Monte Carlo.\n",
        "    \"\"\"\n",
        "    # Initialize V(s)\n",
        "    V = np.zeros(env.grid_size * env.grid_size)\n",
        "\n",
        "    print(f\"Running TD({lam}) Learning...\")\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        # Reset Eligibility Traces at start of every episode\n",
        "        E = np.zeros(env.grid_size * env.grid_size)\n",
        "\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            # Policy: Random Walk\n",
        "            action = np.random.choice(env.actions)\n",
        "            next_state, reward, done = env.step(state, action)\n",
        "\n",
        "            # 1. Calculate TD Error (delta)\n",
        "            # The difference between what we expect and what we got\n",
        "            target = reward + (0 if done else gamma * V[next_state])\n",
        "            delta = target - V[state]\n",
        "\n",
        "            # 2. Increment Trace for CURRENT state\n",
        "            # \"I was just here, so I deserve credit/blame for this reward\"\n",
        "            E[state] += 1\n",
        "\n",
        "            # 3. Update V for ALL states based on their Trace\n",
        "            # V(s) = V(s) + alpha * delta * E(s)\n",
        "            # (Vectorized update for efficiency)\n",
        "            V += alpha * delta * E\n",
        "\n",
        "            # 4. Decay Traces for ALL states\n",
        "            # Memories fade over time: E = gamma * lambda * E\n",
        "            E *= gamma * lam\n",
        "\n",
        "            state = next_state\n",
        "\n",
        "    return V\n",
        "\n",
        "# --- 3. EXECUTION ---\n",
        "if __name__ == \"__main__\":\n",
        "    env = GridWorld()\n",
        "\n",
        "    # Run TD(Lambda) with lambda = 0.5\n",
        "    v_values = td_lambda_learning(env, lam=0.5)\n",
        "\n",
        "    print(f\"\\nState-Value Function V(s) via TD(0.5):\")\n",
        "    print(\"-\" * 30)\n",
        "    print(np.round(v_values.reshape(4, 4), 2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YauxqkXuHxSY",
        "outputId": "13b127fa-6a0e-40cd-a7d4-09603def4832"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running TD(0.5) Learning...\n",
            "\n",
            "State-Value Function V(s) via TD(0.5):\n",
            "------------------------------\n",
            "[[  0.   -11.75 -16.5  -18.17]\n",
            " [-13.   -15.96 -17.9  -16.02]\n",
            " [-17.77 -17.79 -15.79  -9.58]\n",
            " [-20.24 -18.68 -14.54   0.  ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nxrAk7k2HyOh"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-tH6Fc1jIA38"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "v-lDUp9KIAvB"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# --- 1. ENVIRONMENT ---\n",
        "class GridWorld:\n",
        "    def __init__(self):\n",
        "        self.grid_size = 4\n",
        "        self.terminal_states = [0, 15]\n",
        "        self.actions = [0, 1, 2, 3] # UP, DOWN, LEFT, RIGHT\n",
        "\n",
        "    def step(self, state, action):\n",
        "        if state in self.terminal_states:\n",
        "            return state, 0, True\n",
        "\n",
        "        row, col = divmod(state, self.grid_size)\n",
        "\n",
        "        if action == 0:   row = max(row - 1, 0) # UP\n",
        "        elif action == 1: row = min(row + 1, self.grid_size - 1) # DOWN\n",
        "        elif action == 2: col = max(col - 1, 0) # LEFT\n",
        "        elif action == 3: col = min(col + 1, self.grid_size - 1) # RIGHT\n",
        "\n",
        "        next_state = row * self.grid_size + col\n",
        "        reward = -1\n",
        "        done = next_state in self.terminal_states\n",
        "        return next_state, reward, done\n",
        "\n",
        "    def reset(self):\n",
        "        start_state = np.random.randint(0, 16)\n",
        "        while start_state in self.terminal_states:\n",
        "            start_state = np.random.randint(0, 16)\n",
        "        return start_state\n",
        "\n",
        "# --- 2. THE ALGORITHM: SARSA ---\n",
        "def sarsa_learning():\n",
        "    env = GridWorld()\n",
        "\n",
        "    # Parameters\n",
        "    num_episodes = 5000\n",
        "    alpha = 0.1   # Learning Rate\n",
        "    gamma = 1.0   # Discount Factor\n",
        "    epsilon = 0.1 # Exploration Rate\n",
        "\n",
        "    # Initialize Q-Table (16 States x 4 Actions)\n",
        "    Q = np.zeros((16, 4))\n",
        "\n",
        "    print(\"Training with SARSA (5000 Episodes)...\")\n",
        "\n",
        "    for _ in range(num_episodes):\n",
        "        state = env.reset()\n",
        "\n",
        "        # SARSA Step 1: Choose Action A (Epsilon-Greedy) BEFORE the loop\n",
        "        if np.random.rand() < epsilon:\n",
        "            action = np.random.choice(env.actions)\n",
        "        else:\n",
        "            action = np.argmax(Q[state])\n",
        "\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            # SARSA Step 2: Take Action A, observe R, S'\n",
        "            next_state, reward, done = env.step(state, action)\n",
        "\n",
        "            # SARSA Step 3: Choose Next Action A' (Epsilon-Greedy) based on S'\n",
        "            # Note: We pick the next action NOW, before updating\n",
        "            if np.random.rand() < epsilon:\n",
        "                next_action = np.random.choice(env.actions)\n",
        "            else:\n",
        "                next_action = np.argmax(Q[next_state])\n",
        "\n",
        "            # SARSA Step 4: Update Q(S, A) using Q(S', A')\n",
        "            # Formula: Q(s,a) = Q(s,a) + alpha * [ R + gamma * Q(s',a') - Q(s,a) ]\n",
        "\n",
        "            # Value of next state (0 if terminal)\n",
        "            q_next = 0 if done else Q[next_state, next_action]\n",
        "\n",
        "            target = reward + gamma * q_next\n",
        "            Q[state, action] += alpha * (target - Q[state, action])\n",
        "\n",
        "            # SARSA Step 5: Move to next state pair\n",
        "            state = next_state\n",
        "            action = next_action\n",
        "\n",
        "    return Q\n",
        "\n",
        "# --- 3. EXECUTION & RESULTS ---\n",
        "def print_policy(Q):\n",
        "    actions_map = {0: '↑', 1: '↓', 2: '←', 3: '→'}\n",
        "    print(\"\\nFinal Policy (SARSA):\")\n",
        "    print(\"-\" * 17)\n",
        "\n",
        "    grid_output = []\n",
        "    for s in range(16):\n",
        "        if s in [0, 15]:\n",
        "            grid_output.append(\" T \")\n",
        "            continue\n",
        "        best_action = np.argmax(Q[s])\n",
        "        grid_output.append(f\" {actions_map[best_action]} \")\n",
        "\n",
        "    for i in range(0, 16, 4):\n",
        "        print(\"|\".join(grid_output[i:i+4]))\n",
        "        print(\"-\" * 17)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    q_table = sarsa_learning()\n",
        "    print_policy(q_table)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1i-hiR3EIAjS",
        "outputId": "ce152293-c1c0-4280-e033-4e65d558f87a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training with SARSA (5000 Episodes)...\n",
            "\n",
            "Final Policy (SARSA):\n",
            "-----------------\n",
            " T | ← | ← | ← \n",
            "-----------------\n",
            " ↑ | ↑ | ← | ↓ \n",
            "-----------------\n",
            " ↑ | ↑ | → | ↓ \n",
            "-----------------\n",
            " → | → | → | T \n",
            "-----------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pIIn7URcIDMV"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8FliVyp0IHz4"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rlIuwq0EIHqa"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# --- 1. ENVIRONMENT (4x4 Grid World) ---\n",
        "class GridWorld:\n",
        "    def __init__(self):\n",
        "        self.grid_size = 4\n",
        "        self.terminal_states = [0, 15] # Top-Left, Bottom-Right\n",
        "        self.actions = [0, 1, 2, 3] # UP, DOWN, LEFT, RIGHT\n",
        "\n",
        "    def step(self, state, action):\n",
        "        if state in self.terminal_states:\n",
        "            return state, 0, True\n",
        "\n",
        "        row, col = divmod(state, self.grid_size)\n",
        "\n",
        "        # Move Logic\n",
        "        if action == 0:   row = max(row - 1, 0) # UP\n",
        "        elif action == 1: row = min(row + 1, self.grid_size - 1) # DOWN\n",
        "        elif action == 2: col = max(col - 1, 0) # LEFT\n",
        "        elif action == 3: col = min(col + 1, self.grid_size - 1) # RIGHT\n",
        "\n",
        "        next_state = row * self.grid_size + col\n",
        "        reward = -1\n",
        "        done = next_state in self.terminal_states\n",
        "        return next_state, reward, done\n",
        "\n",
        "    def reset(self):\n",
        "        start_state = np.random.randint(0, 16)\n",
        "        while start_state in self.terminal_states:\n",
        "            start_state = np.random.randint(0, 16)\n",
        "        return start_state\n",
        "\n",
        "# --- 2. THE ALGORITHM: Q-Learning ---\n",
        "def q_learning():\n",
        "    env = GridWorld()\n",
        "\n",
        "    # Parameters\n",
        "    num_episodes = 5000\n",
        "    alpha = 0.1   # Learning Rate\n",
        "    gamma = 1.0   # Discount Factor (1.0 because we want shortest path)\n",
        "    epsilon = 0.1 # Exploration Rate\n",
        "\n",
        "    # Initialize Q-Table (16 States x 4 Actions)\n",
        "    Q = np.zeros((16, 4))\n",
        "\n",
        "    print(\"Training with Q-Learning (5000 Episodes)...\")\n",
        "\n",
        "    for _ in range(num_episodes):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            # 1. Choose Action (Epsilon-Greedy)\n",
        "            if np.random.rand() < epsilon:\n",
        "                action = np.random.choice(env.actions) # Explore\n",
        "            else:\n",
        "                action = np.argmax(Q[state]) # Exploit\n",
        "\n",
        "            # 2. Take Action\n",
        "            next_state, reward, done = env.step(state, action)\n",
        "\n",
        "            # 3. Update Q-Value (Off-Policy)\n",
        "            # We use max(Q[next_state]) regardless of what action we actually take next\n",
        "            best_next_action_val = 0 if done else np.max(Q[next_state])\n",
        "\n",
        "            # Update Rule: Q(S,A) = Q(S,A) + alpha * [ R + gamma * max_a Q(S',a) - Q(S,A) ]\n",
        "            td_target = reward + gamma * best_next_action_val\n",
        "            Q[state, action] += alpha * (td_target - Q[state, action])\n",
        "\n",
        "            state = next_state\n",
        "\n",
        "    return Q\n",
        "\n",
        "# --- 3. EXECUTION & RESULTS ---\n",
        "def print_policy(Q):\n",
        "    actions_map = {0: '↑', 1: '↓', 2: '←', 3: '→'}\n",
        "    print(\"\\nFinal Optimal Policy (Q-Learning):\")\n",
        "    print(\"-\" * 17)\n",
        "\n",
        "    grid_output = []\n",
        "    for s in range(16):\n",
        "        if s in [0, 15]:\n",
        "            grid_output.append(\" T \")\n",
        "            continue\n",
        "        # Simply pick the best action from the learned table\n",
        "        best_action = np.argmax(Q[s])\n",
        "        grid_output.append(f\" {actions_map[best_action]} \")\n",
        "\n",
        "    for i in range(0, 16, 4):\n",
        "        print(\"|\".join(grid_output[i:i+4]))\n",
        "        print(\"-\" * 17)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    q_table = q_learning()\n",
        "    print_policy(q_table)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O0A-rqhfIHf-",
        "outputId": "6ca2c465-07c8-49db-c242-58a1a8871600"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training with Q-Learning (5000 Episodes)...\n",
            "\n",
            "Final Optimal Policy (Q-Learning):\n",
            "-----------------\n",
            " T | ← | ← | ← \n",
            "-----------------\n",
            " ↑ | ↑ | ↑ | ↓ \n",
            "-----------------\n",
            " ↑ | ← | ↓ | ↓ \n",
            "-----------------\n",
            " → | → | → | T \n",
            "-----------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pD98dVi3ITP4"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "p_PiPrYwIhcZ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "e6Z4lWYfIhT-"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import random\n",
        "from collections import deque\n",
        "\n",
        "# --- 1. ENVIRONMENT ---\n",
        "class GridWorld:\n",
        "    def __init__(self):\n",
        "        self.grid_size = 4\n",
        "        self.terminal_states = [0, 15]\n",
        "        self.actions = [0, 1, 2, 3] # UP, DOWN, LEFT, RIGHT\n",
        "\n",
        "    def step(self, state, action):\n",
        "        if state in self.terminal_states:\n",
        "            return state, 0, True\n",
        "\n",
        "        row, col = divmod(state, self.grid_size)\n",
        "\n",
        "        if action == 0:   row = max(row - 1, 0)\n",
        "        elif action == 1: row = min(row + 1, self.grid_size - 1)\n",
        "        elif action == 2: col = max(col - 1, 0)\n",
        "        elif action == 3: col = min(col + 1, self.grid_size - 1)\n",
        "\n",
        "        next_state = row * self.grid_size + col\n",
        "        reward = -1\n",
        "        done = next_state in self.terminal_states\n",
        "        return next_state, reward, done\n",
        "\n",
        "    def reset(self):\n",
        "        start_state = np.random.randint(0, 16)\n",
        "        while start_state in self.terminal_states:\n",
        "            start_state = np.random.randint(0, 16)\n",
        "        return start_state\n",
        "\n",
        "# --- 2. THE NEURAL NETWORK ---\n",
        "class QNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(QNetwork, self).__init__()\n",
        "        # Input: 16 (One-hot encoding of the state)\n",
        "        # Hidden: 128 neurons\n",
        "        # Output: 4 (Q-values for UP, DOWN, LEFT, RIGHT)\n",
        "        self.fc1 = nn.Linear(16, 128)\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        self.fc3 = nn.Linear(64, 4)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        return self.fc3(x)\n",
        "\n",
        "# --- 3. HELPER: ONE-HOT ENCODING ---\n",
        "def state_to_tensor(state):\n",
        "    # Converts state integer (e.g., 5) to one-hot vector [0,0,0,0,0,1,0...]\n",
        "    v = torch.zeros(16)\n",
        "    v[state] = 1.0\n",
        "    return v.unsqueeze(0) # Add batch dimension\n",
        "\n",
        "# --- 4. THE ALGORITHM: DQN Training ---\n",
        "def train_dqn():\n",
        "    env = GridWorld()\n",
        "\n",
        "    # Hyperparameters\n",
        "    episodes = 1000\n",
        "    gamma = 0.99\n",
        "    epsilon = 1.0\n",
        "    epsilon_decay = 0.995\n",
        "    epsilon_min = 0.1\n",
        "    learning_rate = 0.001\n",
        "    batch_size = 32\n",
        "\n",
        "    # Initialize Networks\n",
        "    policy_net = QNetwork()\n",
        "    optimizer = optim.Adam(policy_net.parameters(), lr=learning_rate)\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    # Replay Buffer (Memory)\n",
        "    memory = deque(maxlen=2000)\n",
        "\n",
        "    print(\"Training DQN (this may take a moment)...\")\n",
        "\n",
        "    for episode in range(episodes):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            state_tensor = state_to_tensor(state)\n",
        "\n",
        "            # A. Select Action (Epsilon-Greedy)\n",
        "            if random.random() < epsilon:\n",
        "                action = random.choice(env.actions)\n",
        "            else:\n",
        "                with torch.no_grad():\n",
        "                    q_values = policy_net(state_tensor)\n",
        "                    action = torch.argmax(q_values).item()\n",
        "\n",
        "            # B. Step\n",
        "            next_state, reward, done = env.step(state, action)\n",
        "\n",
        "            # C. Store in Memory\n",
        "            memory.append((state, action, reward, next_state, done))\n",
        "            state = next_state\n",
        "\n",
        "            # D. Train (Experience Replay)\n",
        "            if len(memory) > batch_size:\n",
        "                minibatch = random.sample(memory, batch_size)\n",
        "\n",
        "                # Prepare batch data\n",
        "                states_b = torch.cat([state_to_tensor(x[0]) for x in minibatch])\n",
        "                next_states_b = torch.cat([state_to_tensor(x[3]) for x in minibatch])\n",
        "\n",
        "                # Get current Q values\n",
        "                q_preds = policy_net(states_b)\n",
        "\n",
        "                # Calculate Target Q values\n",
        "                with torch.no_grad():\n",
        "                    q_next = policy_net(next_states_b)\n",
        "\n",
        "                target_q_values = q_preds.clone()\n",
        "\n",
        "                for i, (s, a, r, ns, d) in enumerate(minibatch):\n",
        "                    # Bellman Update: R + gamma * max(Q(s'))\n",
        "                    target = r\n",
        "                    if not d:\n",
        "                        target += gamma * torch.max(q_next[i]).item()\n",
        "                    target_q_values[i][a] = target\n",
        "\n",
        "                # Gradient Descent\n",
        "                loss = criterion(q_preds, target_q_values)\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "        # Decay Epsilon\n",
        "        if epsilon > epsilon_min:\n",
        "            epsilon *= epsilon_decay\n",
        "\n",
        "        if (episode+1) % 200 == 0:\n",
        "            print(f\"Episode {episode+1}/{episodes} completed.\")\n",
        "\n",
        "    return policy_net\n",
        "\n",
        "# --- 5. TEST THE TRAINED MODEL ---\n",
        "if __name__ == \"__main__\":\n",
        "    trained_model = train_dqn()\n",
        "\n",
        "    print(\"\\nVisualizing DQN Policy:\")\n",
        "    print(\"-\" * 17)\n",
        "    actions_map = {0: '↑', 1: '↓', 2: '←', 3: '→'}\n",
        "    env = GridWorld()\n",
        "\n",
        "    output_grid = []\n",
        "    for s in range(16):\n",
        "        if s in [0, 15]:\n",
        "            output_grid.append(\" T \")\n",
        "            continue\n",
        "\n",
        "        st = state_to_tensor(s)\n",
        "        with torch.no_grad():\n",
        "            q = trained_model(st)\n",
        "            best_a = torch.argmax(q).item()\n",
        "        output_grid.append(f\" {actions_map[best_a]} \")\n",
        "\n",
        "    for i in range(0, 16, 4):\n",
        "        print(\"|\".join(output_grid[i:i+4]))\n",
        "        print(\"-\" * 17)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gZc2tCplIhKH",
        "outputId": "902552ff-0e9b-4e1f-def1-bf5e1eef964a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training DQN (this may take a moment)...\n",
            "Episode 200/1000 completed.\n",
            "Episode 400/1000 completed.\n",
            "Episode 600/1000 completed.\n",
            "Episode 800/1000 completed.\n",
            "Episode 1000/1000 completed.\n",
            "\n",
            "Visualizing DQN Policy:\n",
            "-----------------\n",
            " T | ← | ← | ← \n",
            "-----------------\n",
            " ↑ | ↑ | ← | ↓ \n",
            "-----------------\n",
            " ↑ | ↑ | ↓ | ↓ \n",
            "-----------------\n",
            " ↑ | → | → | T \n",
            "-----------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "B_VyTuAoIiWa"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cpuwmpr3I5Db"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MO1tIiBVI49F"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f5c2e0c6",
        "outputId": "26598add-f8fc-4ee5-8b62-e35dc0a0d41d"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# --- 1. ENVIRONMENT (4x4 Grid World) ---\n",
        "class GridWorld:\n",
        "    def __init__(self):\n",
        "        self.grid_size = 4\n",
        "        self.terminal_states = [0, 15]\n",
        "        self.actions = [0, 1, 2, 3] # UP, DOWN, LEFT, RIGHT\n",
        "\n",
        "    def step(self, state, action):\n",
        "        if state in self.terminal_states:\n",
        "            return state, 0, True\n",
        "\n",
        "        row, col = divmod(state, self.grid_size)\n",
        "        if action == 0:   row = max(row - 1, 0)\n",
        "        elif action == 1: row = min(row + 1, self.grid_size - 1)\n",
        "        elif action == 2: col = max(col - 1, 0)\n",
        "        elif action == 3: col = min(col + 1, self.grid_size - 1)\n",
        "\n",
        "        next_state = row * self.grid_size + col\n",
        "        reward = -1\n",
        "        done = next_state in self.terminal_states\n",
        "        return next_state, reward, done\n",
        "\n",
        "    def reset(self):\n",
        "        start_state = np.random.randint(0, 16)\n",
        "        while start_state in self.terminal_states:\n",
        "            start_state = np.random.randint(0, 16)\n",
        "        return start_state\n",
        "\n",
        "# --- 2. NEURAL NETWORKS ---\n",
        "\n",
        "class PolicyNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(PolicyNetwork, self).__init__()\n",
        "        self.fc1 = nn.Linear(16, 128)\n",
        "        self.fc2 = nn.Linear(128, 4)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        # Numerical stability fix: prevents NaN in softmax\n",
        "        return F.softmax(x, dim=-1)\n",
        "\n",
        "class ValueNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ValueNetwork, self).__init__()\n",
        "        self.fc1 = nn.Linear(16, 128)\n",
        "        self.fc2 = nn.Linear(128, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        return self.fc2(x)\n",
        "\n",
        "def state_to_tensor(state):\n",
        "    v = torch.zeros(16)\n",
        "    v[state] = 1.0\n",
        "    return v.unsqueeze(0)\n",
        "\n",
        "# --- 3. REINFORCE ALGORITHM ---\n",
        "def train_reinforce_baseline():\n",
        "    env = GridWorld()\n",
        "\n",
        "    policy_net = PolicyNetwork()\n",
        "    value_net = ValueNetwork()\n",
        "\n",
        "    # Reduced learning rate slightly for stability\n",
        "    policy_optimizer = optim.Adam(policy_net.parameters(), lr=0.0005)\n",
        "    value_optimizer = optim.Adam(value_net.parameters(), lr=0.0005)\n",
        "\n",
        "    num_episodes = 2000\n",
        "    gamma = 0.99\n",
        "\n",
        "    print(\"Training REINFORCE with Baseline\")\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "\n",
        "        log_probs = []\n",
        "        values = []\n",
        "        rewards = []\n",
        "\n",
        "        # A. Generate Episode\n",
        "        while not done:\n",
        "            state_t = state_to_tensor(state)\n",
        "\n",
        "            probs = policy_net(state_t)\n",
        "            value = value_net(state_t)\n",
        "\n",
        "            # Create distribution\n",
        "            dist = torch.distributions.Categorical(probs)\n",
        "            action = dist.sample()\n",
        "\n",
        "            # Step\n",
        "            next_state, reward, done = env.step(state, action.item())\n",
        "\n",
        "            log_probs.append(dist.log_prob(action))\n",
        "            values.append(value)\n",
        "            rewards.append(reward)\n",
        "\n",
        "            state = next_state\n",
        "\n",
        "            # Safety break if agent gets stuck in a loop\n",
        "            if len(rewards) > 100:\n",
        "                break\n",
        "\n",
        "        # B. Calculate Returns\n",
        "        returns = []\n",
        "        G = 0\n",
        "        for r in reversed(rewards):\n",
        "            G = r + gamma * G\n",
        "            returns.insert(0, G)\n",
        "        returns = torch.tensor(returns, dtype=torch.float32)\n",
        "\n",
        "        # C. Normalize Returns (Safe Mode)\n",
        "        # Only normalize if we have more than 1 step, otherwise std is NaN\n",
        "        if len(returns) > 1:\n",
        "            returns = (returns - returns.mean()) / (returns.std() + 1e-9)\n",
        "        else:\n",
        "            returns = returns - returns.mean()\n",
        "\n",
        "        # D. Calculate Losses\n",
        "        policy_loss = []\n",
        "        value_loss = []\n",
        "\n",
        "        for log_prob, value, G_t in zip(log_probs, values, returns):\n",
        "            advantage = G_t - value.item()\n",
        "\n",
        "            # Policy Loss\n",
        "            policy_loss.append(-log_prob * advantage)\n",
        "\n",
        "            # Value Loss (Fixing the warning by using detach/clone logic if needed)\n",
        "            # We target the actual scalar G_t\n",
        "            target = torch.tensor([G_t], dtype=torch.float32)\n",
        "            value_loss.append(F.mse_loss(value.view(-1), target))\n",
        "\n",
        "        policy_optimizer.zero_grad()\n",
        "        value_optimizer.zero_grad()\n",
        "\n",
        "        # Check if lists are not empty (in case of immediate termination)\n",
        "        if policy_loss:\n",
        "            loss_p = torch.stack(policy_loss).sum()\n",
        "            loss_v = torch.stack(value_loss).sum()\n",
        "\n",
        "            loss_p.backward()\n",
        "            loss_v.backward()\n",
        "\n",
        "            # GRADIENT CLIPPING (The Fix for Exploding Gradients)\n",
        "            torch.nn.utils.clip_grad_norm_(policy_net.parameters(), 1.0)\n",
        "            torch.nn.utils.clip_grad_norm_(value_net.parameters(), 1.0)\n",
        "\n",
        "            policy_optimizer.step()\n",
        "            value_optimizer.step()\n",
        "\n",
        "        if (episode + 1) % 500 == 0:\n",
        "            print(f\"Episode {episode + 1}/{num_episodes} completed.\")\n",
        "\n",
        "    return policy_net\n",
        "\n",
        "# --- 4. VISUALIZE ---\n",
        "if __name__ == \"__main__\":\n",
        "    trained_policy = train_reinforce_baseline()\n",
        "\n",
        "    print(\"\\nFinal Policy (REINFORCE):\")\n",
        "    actions_map = {0: '↑', 1: '↓', 2: '←', 3: '→'}\n",
        "\n",
        "    output_grid = []\n",
        "    for s in range(16):\n",
        "        if s in [0, 15]:\n",
        "            output_grid.append(\" T \")\n",
        "            continue\n",
        "        st = state_to_tensor(s)\n",
        "        with torch.no_grad():\n",
        "            probs = trained_policy(st)\n",
        "            best_a = torch.argmax(probs).item()\n",
        "        output_grid.append(f\" {actions_map[best_a]} \")\n",
        "\n",
        "    print(\"-\" * 17)\n",
        "    for i in range(0, 16, 4):\n",
        "        print(\"|\".join(output_grid[i:i+4]))\n",
        "        print(\"-\" * 17)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training REINFORCE with Baseline\n",
            "Episode 500/2000 completed.\n",
            "Episode 1000/2000 completed.\n",
            "Episode 1500/2000 completed.\n",
            "Episode 2000/2000 completed.\n",
            "\n",
            "Final Policy (REINFORCE):\n",
            "-----------------\n",
            " T | ← | ← | ← \n",
            "-----------------\n",
            " ↑ | ← | ↑ | ↓ \n",
            "-----------------\n",
            " ↑ | ← | ↓ | ↓ \n",
            "-----------------\n",
            " ↑ | → | → | T \n",
            "-----------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OkDQQnF6I5wH"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Yxt_VJ9cL1yI"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-xNHri6bL1ol"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# --- 1. ENVIRONMENT (4x4 Grid World) ---\n",
        "class GridWorld:\n",
        "    def __init__(self):\n",
        "        self.grid_size = 4\n",
        "        self.terminal_states = [0, 15]\n",
        "        self.actions = [0, 1, 2, 3] # UP, DOWN, LEFT, RIGHT\n",
        "\n",
        "    def step(self, state, action):\n",
        "        if state in self.terminal_states:\n",
        "            return state, 0, True\n",
        "\n",
        "        row, col = divmod(state, self.grid_size)\n",
        "        if action == 0:   row = max(row - 1, 0)\n",
        "        elif action == 1: row = min(row + 1, self.grid_size - 1)\n",
        "        elif action == 2: col = max(col - 1, 0)\n",
        "        elif action == 3: col = min(col + 1, self.grid_size - 1)\n",
        "\n",
        "        next_state = row * self.grid_size + col\n",
        "        reward = -1\n",
        "        done = next_state in self.terminal_states\n",
        "        return next_state, reward, done\n",
        "\n",
        "    def reset(self):\n",
        "        start_state = np.random.randint(0, 16)\n",
        "        while start_state in self.terminal_states:\n",
        "            start_state = np.random.randint(0, 16)\n",
        "        return start_state\n",
        "\n",
        "# --- 2. NEURAL NETWORKS ---\n",
        "class PolicyNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(PolicyNetwork, self).__init__()\n",
        "        self.fc1 = nn.Linear(16, 128)\n",
        "        self.fc2 = nn.Linear(128, 4)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return F.softmax(x, dim=-1)\n",
        "\n",
        "class ValueNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ValueNetwork, self).__init__()\n",
        "        self.fc1 = nn.Linear(16, 128)\n",
        "        self.fc2 = nn.Linear(128, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        return self.fc2(x)\n",
        "\n",
        "def state_to_tensor(state):\n",
        "    v = torch.zeros(16)\n",
        "    v[state] = 1.0\n",
        "    return v.unsqueeze(0)\n",
        "\n",
        "# --- 3. REINFORCE WITH ADVANTAGE ALGORITHM ---\n",
        "def train_reinforce_advantage():\n",
        "    env = GridWorld()\n",
        "\n",
        "    # Initialize Actor (Policy) and Critic (Value)\n",
        "    policy_net = PolicyNetwork()\n",
        "    value_net = ValueNetwork()\n",
        "\n",
        "    # Use small learning rate and gradient clipping for stability\n",
        "    policy_optimizer = optim.Adam(policy_net.parameters(), lr=0.0005)\n",
        "    value_optimizer = optim.Adam(value_net.parameters(), lr=0.0005)\n",
        "\n",
        "    num_episodes = 2000\n",
        "    gamma = 0.99\n",
        "\n",
        "    print(\"Training REINFORCE using Advantage Function...\")\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "\n",
        "        log_probs = []\n",
        "        values = []\n",
        "        rewards = []\n",
        "\n",
        "        # --- A. Collect Trajectory (Monte Carlo) ---\n",
        "        while not done:\n",
        "            state_t = state_to_tensor(state)\n",
        "\n",
        "            # 1. Get Policy prob and Value estimate\n",
        "            probs = policy_net(state_t)\n",
        "            value = value_net(state_t)\n",
        "\n",
        "            # 2. Sample Action\n",
        "            dist = torch.distributions.Categorical(probs)\n",
        "            action = dist.sample()\n",
        "\n",
        "            # 3. Take Step\n",
        "            next_state, reward, done = env.step(state, action.item())\n",
        "\n",
        "            log_probs.append(dist.log_prob(action))\n",
        "            values.append(value)\n",
        "            rewards.append(reward)\n",
        "\n",
        "            state = next_state\n",
        "            if len(rewards) > 100: break # Safety break\n",
        "\n",
        "        # --- B. Calculate Returns (G_t) ---\n",
        "        returns = []\n",
        "        G = 0\n",
        "        for r in reversed(rewards):\n",
        "            G = r + gamma * G\n",
        "            returns.insert(0, G)\n",
        "        returns = torch.tensor(returns, dtype=torch.float32)\n",
        "\n",
        "        # Normalize returns for numerical stability\n",
        "        if len(returns) > 1:\n",
        "            returns = (returns - returns.mean()) / (returns.std() + 1e-9)\n",
        "        else:\n",
        "            returns = returns - returns.mean()\n",
        "\n",
        "        # --- C. Calculate Advantage & Update ---\n",
        "        policy_loss = []\n",
        "        value_loss = []\n",
        "\n",
        "        for log_prob, value, G_t in zip(log_probs, values, returns):\n",
        "            # THE ADVANTAGE FUNCTION: A(s,a) = G_t - V(s)\n",
        "            # We detach() value because we don't want to update the ValueNet based on Policy loss\n",
        "            advantage = G_t - value.item()\n",
        "\n",
        "            # Policy Update: Increase prob of actions with positive Advantage\n",
        "            policy_loss.append(-log_prob * advantage)\n",
        "\n",
        "            # Value Update: Make V(s) closer to actual G_t\n",
        "            target = torch.tensor([G_t], dtype=torch.float32)\n",
        "            value_loss.append(F.mse_loss(value.view(-1), target))\n",
        "\n",
        "        # Backpropagation\n",
        "        policy_optimizer.zero_grad()\n",
        "        value_optimizer.zero_grad()\n",
        "\n",
        "        if policy_loss:\n",
        "            loss_p = torch.stack(policy_loss).sum()\n",
        "            loss_v = torch.stack(value_loss).sum()\n",
        "\n",
        "            loss_p.backward()\n",
        "            loss_v.backward()\n",
        "\n",
        "            # Gradient Clipping (Prevents Exploding Gradients/NaNs)\n",
        "            torch.nn.utils.clip_grad_norm_(policy_net.parameters(), 1.0)\n",
        "            torch.nn.utils.clip_grad_norm_(value_net.parameters(), 1.0)\n",
        "\n",
        "            policy_optimizer.step()\n",
        "            value_optimizer.step()\n",
        "\n",
        "        if (episode + 1) % 500 == 0:\n",
        "            print(f\"Episode {episode + 1}/{num_episodes} completed.\")\n",
        "\n",
        "    return policy_net\n",
        "\n",
        "# --- 4. VISUALIZE ---\n",
        "if __name__ == \"__main__\":\n",
        "    trained_policy = train_reinforce_advantage()\n",
        "\n",
        "    print(\"\\nFinal Policy (Advantage Method):\")\n",
        "    actions_map = {0: '↑', 1: '↓', 2: '←', 3: '→'}\n",
        "\n",
        "    output_grid = []\n",
        "    for s in range(16):\n",
        "        if s in [0, 15]:\n",
        "            output_grid.append(\" T \")\n",
        "            continue\n",
        "        st = state_to_tensor(s)\n",
        "        with torch.no_grad():\n",
        "            probs = trained_policy(st)\n",
        "            best_a = torch.argmax(probs).item()\n",
        "        output_grid.append(f\" {actions_map[best_a]} \")\n",
        "\n",
        "    print(\"-\" * 17)\n",
        "    for i in range(0, 16, 4):\n",
        "        print(\"|\".join(output_grid[i:i+4]))\n",
        "        print(\"-\" * 17)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X9EvZsJJL1dB",
        "outputId": "a316861c-9d35-40d6-c23f-6301618b4d08"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training REINFORCE using Advantage Function...\n",
            "Episode 500/2000 completed.\n",
            "Episode 1000/2000 completed.\n",
            "Episode 1500/2000 completed.\n",
            "Episode 2000/2000 completed.\n",
            "\n",
            "Final Policy (Advantage Method):\n",
            "-----------------\n",
            " T | ← | ← | ← \n",
            "-----------------\n",
            " ↑ | ← | ↑ | ↓ \n",
            "-----------------\n",
            " ↑ | → | ↓ | ↓ \n",
            "-----------------\n",
            " ↑ | → | → | T \n",
            "-----------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IcXNZx9FL41K"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5gy_bT5kMJdO"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Y2E2fBSqMJVi"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.distributions import Normal\n",
        "\n",
        "# --- 1. CONTINUOUS ENVIRONMENT (Simple 1D Target Seeking) ---\n",
        "class ContinuousTargetEnv:\n",
        "    def __init__(self):\n",
        "        # State: Position on a line (Start at -2.0)\n",
        "        # Goal: Reach 0.0\n",
        "        self.state = np.array([-2.0], dtype=np.float32)\n",
        "        self.max_steps = 200\n",
        "        self.current_step = 0\n",
        "\n",
        "    def reset(self):\n",
        "        # Start at random position between -2 and -1\n",
        "        self.state = np.array([np.random.uniform(-2, -1)], dtype=np.float32)\n",
        "        self.current_step = 0\n",
        "        return self.state\n",
        "\n",
        "    def step(self, action):\n",
        "        # Action is a continuous force [-1, 1]\n",
        "        force = np.clip(action, -1.0, 1.0)\n",
        "\n",
        "        # Dynamics: Position += Force * speed\n",
        "        self.state[0] += force * 0.1\n",
        "\n",
        "        # Calculate Reward (Negative distance to goal 0.0)\n",
        "        dist = abs(self.state[0] - 0.0)\n",
        "        reward = -dist\n",
        "\n",
        "        # Check Done\n",
        "        self.current_step += 1\n",
        "        done = dist < 0.1 or self.current_step >= self.max_steps\n",
        "\n",
        "        # Bonus reward for finishing\n",
        "        if dist < 0.1:\n",
        "            reward += 10.0\n",
        "\n",
        "        return self.state, reward, done\n",
        "\n",
        "# --- 2. ACTOR-CRITIC NETWORK ---\n",
        "class ActorCritic(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ActorCritic, self).__init__()\n",
        "        # Common layer\n",
        "        self.fc1 = nn.Linear(1, 128)\n",
        "\n",
        "        # ACTOR HEAD (Outputs Mean `mu` and Std Dev `sigma`)\n",
        "        # Used to create a Normal Distribution (Gaussian)\n",
        "        self.mu_head = nn.Linear(128, 1)\n",
        "        self.sigma_head = nn.Linear(128, 1)\n",
        "\n",
        "        # CRITIC HEAD (Outputs Value V(s))\n",
        "        self.value_head = nn.Linear(128, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "\n",
        "        # Actor outputs\n",
        "        mu = torch.tanh(self.mu_head(x)) # Output between -1 and 1\n",
        "        sigma = F.softplus(self.sigma_head(x)) + 1e-5 # Always positive\n",
        "\n",
        "        # Critic output\n",
        "        value = self.value_head(x)\n",
        "\n",
        "        return mu, sigma, value\n",
        "\n",
        "# --- 3. A2C ALGORITHM (Continuous) ---\n",
        "def train_a2c_continuous():\n",
        "    env = ContinuousTargetEnv()\n",
        "    model = ActorCritic()\n",
        "\n",
        "    # We update both heads with one optimizer\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    episodes = 1000\n",
        "    gamma = 0.99\n",
        "\n",
        "    print(\"Training A2C for Continuous Control...\")\n",
        "\n",
        "    for episode in range(episodes):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        total_reward = 0\n",
        "\n",
        "        while not done:\n",
        "            state_t = torch.FloatTensor(state)\n",
        "\n",
        "            # 1. Forward pass\n",
        "            mu, sigma, value = model(state_t)\n",
        "\n",
        "            # 2. Sample Continuous Action from Normal Distribution\n",
        "            dist = Normal(mu, sigma)\n",
        "            action = dist.sample()\n",
        "\n",
        "            # Clip action to valid range for environment\n",
        "            action_numpy = action.detach().numpy()[0]\n",
        "\n",
        "            # 3. Take Step\n",
        "            next_state, reward, done = env.step(action_numpy)\n",
        "            total_reward += reward\n",
        "\n",
        "            # 4. Calculate Target (TD Target)\n",
        "            next_state_t = torch.FloatTensor(next_state)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                _, _, next_value = model(next_state_t)\n",
        "                # If done, next value is 0\n",
        "                target_value = reward + (0 if done else gamma * next_value.item())\n",
        "\n",
        "            # 5. Calculate Advantage\n",
        "            # Advantage = Target - Current_Prediction\n",
        "            advantage = target_value - value\n",
        "\n",
        "            # 6. Calculate Losses\n",
        "\n",
        "            # Critic Loss: MSE(Target, Predicted)\n",
        "            critic_loss = advantage.pow(2)\n",
        "\n",
        "            # Actor Loss: -log_prob * advantage\n",
        "            # (We detach advantage so we don't backprop through critic here)\n",
        "            log_prob = dist.log_prob(action)\n",
        "            actor_loss = -log_prob * advantage.detach()\n",
        "\n",
        "            # Total Loss\n",
        "            loss = actor_loss + critic_loss\n",
        "\n",
        "            # 7. Update\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            state = next_state\n",
        "\n",
        "        if (episode + 1) % 100 == 0:\n",
        "            print(f\"Episode {episode + 1}/episodes: Total Reward = {total_reward:.2f}\")\n",
        "\n",
        "    return model\n",
        "\n",
        "# --- 4. TEST ---\n",
        "if __name__ == \"__main__\":\n",
        "    trained_model = train_a2c_continuous()\n",
        "\n",
        "    print(\"\\nTesting Trained Policy (Moving from -2.0 to 0.0):\")\n",
        "    env = ContinuousTargetEnv()\n",
        "    state = env.reset()\n",
        "\n",
        "    for i in range(10):\n",
        "        state_t = torch.FloatTensor(state)\n",
        "        with torch.no_grad():\n",
        "            mu, sigma, _ = trained_model(state_t)\n",
        "            # In testing, we just use the Mean (mu) - no randomness\n",
        "            action = mu.item()\n",
        "\n",
        "        next_state, _, done = env.step(action)\n",
        "        print(f\"Step {i+1}: Pos {state[0]:.2f} -> Action {action:.2f} -> New Pos {next_state[0]:.2f}\")\n",
        "        state = next_state\n",
        "        if done: break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uJrppVnPMJLK",
        "outputId": "87b46810-f4a1-473c-adb4-32965375229d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training A2C for Continuous Control...\n",
            "Episode 100/episodes: Total Reward = -4.85\n",
            "Episode 200/episodes: Total Reward = -1.69\n",
            "Episode 300/episodes: Total Reward = 0.61\n",
            "Episode 400/episodes: Total Reward = -0.43\n",
            "Episode 500/episodes: Total Reward = 4.89\n",
            "Episode 600/episodes: Total Reward = 1.66\n",
            "Episode 700/episodes: Total Reward = 0.76\n",
            "Episode 800/episodes: Total Reward = 2.81\n",
            "Episode 900/episodes: Total Reward = -4.71\n",
            "Episode 1000/episodes: Total Reward = -3.22\n",
            "\n",
            "Testing Trained Policy (Moving from -2.0 to 0.0):\n",
            "Step 1: Pos -1.17 -> Action 1.00 -> New Pos -1.17\n",
            "Step 2: Pos -1.07 -> Action 1.00 -> New Pos -1.07\n",
            "Step 3: Pos -0.97 -> Action 1.00 -> New Pos -0.97\n",
            "Step 4: Pos -0.87 -> Action 1.00 -> New Pos -0.87\n",
            "Step 5: Pos -0.77 -> Action 1.00 -> New Pos -0.77\n",
            "Step 6: Pos -0.67 -> Action 1.00 -> New Pos -0.67\n",
            "Step 7: Pos -0.57 -> Action 1.00 -> New Pos -0.57\n",
            "Step 8: Pos -0.47 -> Action 0.99 -> New Pos -0.47\n",
            "Step 9: Pos -0.37 -> Action 0.99 -> New Pos -0.37\n",
            "Step 10: Pos -0.27 -> Action 0.98 -> New Pos -0.27\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1gMcO4oHMQ0v"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AsMHA8hrM6Uy"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3PVK2ljqM6MG"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# --- 1. MULTI-AGENT ENVIRONMENT ---\n",
        "class MultiAgentGridWorld:\n",
        "    def __init__(self):\n",
        "        self.grid_size = 4\n",
        "        self.actions = [0, 1, 2, 3] # UP, DOWN, LEFT, RIGHT\n",
        "\n",
        "        # Goals\n",
        "        self.goal_A = (3, 3) # Bottom-Right\n",
        "        self.goal_B = (0, 3) # Top-Right\n",
        "\n",
        "    def reset(self):\n",
        "        # Start positions\n",
        "        self.pos_A = (0, 0)\n",
        "        self.pos_B = (3, 0)\n",
        "\n",
        "        # Return combined state: (rowA, colA, rowB, colB)\n",
        "        return self.pos_A + self.pos_B\n",
        "\n",
        "    def step(self, action_A, action_B):\n",
        "        # 1. Calculate Proposed New Positions\n",
        "        new_pos_A = self._move(self.pos_A, action_A)\n",
        "        new_pos_B = self._move(self.pos_B, action_B)\n",
        "\n",
        "        reward_A = -1\n",
        "        reward_B = -1\n",
        "        done_A = False\n",
        "        done_B = False\n",
        "\n",
        "        # 2. Check for Collisions (Agents hitting each other)\n",
        "        if new_pos_A == new_pos_B:\n",
        "            # Crash! Both stay in place and get big penalty\n",
        "            reward_A = -10\n",
        "            reward_B = -10\n",
        "            new_pos_A = self.pos_A\n",
        "            new_pos_B = self.pos_B\n",
        "        else:\n",
        "            # 3. Check for Goals\n",
        "            if new_pos_A == self.goal_A:\n",
        "                reward_A = 100\n",
        "                done_A = True\n",
        "\n",
        "            if new_pos_B == self.goal_B:\n",
        "                reward_B = 100\n",
        "                done_B = True\n",
        "\n",
        "        # Update positions (if not done)\n",
        "        if not done_A: self.pos_A = new_pos_A\n",
        "        if not done_B: self.pos_B = new_pos_B\n",
        "\n",
        "        next_state = self.pos_A + self.pos_B\n",
        "\n",
        "        # Global Done: When BOTH finished\n",
        "        # (For simplicity in this simulation, we reset if ONE finishes to keep them training together,\n",
        "        # or we could wait. Here we'll treat episode as done if EITHER finishes for faster training cycles)\n",
        "        done = done_A or done_B\n",
        "\n",
        "        return next_state, reward_A, reward_B, done\n",
        "\n",
        "    def _move(self, pos, action):\n",
        "        r, c = pos\n",
        "        if action == 0:   r = max(r - 1, 0) # UP\n",
        "        elif action == 1: r = min(r + 1, self.grid_size - 1) # DOWN\n",
        "        elif action == 2: c = max(c - 1, 0) # LEFT\n",
        "        elif action == 3: c = min(c + 1, self.grid_size - 1) # RIGHT\n",
        "        return (r, c)\n",
        "\n",
        "# --- 2. INDEPENDENT Q-LEARNING ---\n",
        "def train_marl():\n",
        "    env = MultiAgentGridWorld()\n",
        "\n",
        "    # State Space: 4x4 for Agent A * 4x4 for Agent B = 256 states\n",
        "    # We map state tuple (r1, c1, r2, c2) to an index 0-255\n",
        "    def get_state_idx(state_tuple):\n",
        "        r1, c1, r2, c2 = state_tuple\n",
        "        # Flattening 4D coordinate to 1D index\n",
        "        return r1*64 + c1*16 + r2*4 + c2\n",
        "\n",
        "    # Two Independent Q-Tables\n",
        "    Q_A = np.zeros((256, 4))\n",
        "    Q_B = np.zeros((256, 4))\n",
        "\n",
        "    # Hyperparameters\n",
        "    episodes = 5000\n",
        "    alpha = 0.1\n",
        "    gamma = 0.95\n",
        "    epsilon = 0.1\n",
        "\n",
        "    print(\"Training Multi-Agent System (Agents A & B)...\")\n",
        "\n",
        "    for episode in range(episodes):\n",
        "        state = env.reset()\n",
        "        state_idx = get_state_idx(state)\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            # --- ACTION SELECTION (Epsilon-Greedy) ---\n",
        "            if np.random.rand() < epsilon:\n",
        "                act_A = np.random.choice(env.actions)\n",
        "            else:\n",
        "                act_A = np.argmax(Q_A[state_idx])\n",
        "\n",
        "            if np.random.rand() < epsilon:\n",
        "                act_B = np.random.choice(env.actions)\n",
        "            else:\n",
        "                act_B = np.argmax(Q_B[state_idx])\n",
        "\n",
        "            # --- STEP ---\n",
        "            next_state, rA, rB, done = env.step(act_A, act_B)\n",
        "            next_state_idx = get_state_idx(next_state)\n",
        "\n",
        "            # --- UPDATE Q-TABLES SEPARATELY ---\n",
        "\n",
        "            # Update Agent A\n",
        "            old_val_A = Q_A[state_idx, act_A]\n",
        "            next_max_A = np.max(Q_A[next_state_idx])\n",
        "            Q_A[state_idx, act_A] = old_val_A + alpha * (rA + gamma * next_max_A - old_val_A)\n",
        "\n",
        "            # Update Agent B\n",
        "            old_val_B = Q_B[state_idx, act_B]\n",
        "            next_max_B = np.max(Q_B[next_state_idx])\n",
        "            Q_B[state_idx, act_B] = old_val_B + alpha * (rB + gamma * next_max_B - old_val_B)\n",
        "\n",
        "            state_idx = next_state_idx\n",
        "\n",
        "            # Safety break\n",
        "            if rA == 100 or rB == 100:\n",
        "                break\n",
        "\n",
        "    return Q_A, Q_B, env\n",
        "\n",
        "# --- 3. ANALYSE / TEST ---\n",
        "if __name__ == \"__main__\":\n",
        "    qa, qb, env = train_marl()\n",
        "\n",
        "    print(\"\\n--- Testing MARL Interaction ---\")\n",
        "    print(\"Agent A: (0,0) -> (3,3)\")\n",
        "    print(\"Agent B: (3,0) -> (0,3)\")\n",
        "\n",
        "    state = env.reset()\n",
        "    state_idx = 0 # Calculated manually for (0,0,3,0)\n",
        "\n",
        "    # Helper to print grid\n",
        "    def print_grid(pos_a, pos_b):\n",
        "        grid = [[' . ' for _ in range(4)] for _ in range(4)]\n",
        "        grid[pos_a[0]][pos_a[1]] = ' A '\n",
        "        grid[pos_b[0]][pos_b[1]] = ' B '\n",
        "        if pos_a == pos_b: grid[pos_a[0]][pos_a[1]] = ' X ' # Collision\n",
        "        for row in grid:\n",
        "            print(\"\".join(row))\n",
        "        print(\"-\" * 12)\n",
        "\n",
        "    pos_A = (0,0)\n",
        "    pos_B = (3,0)\n",
        "\n",
        "    print_grid(pos_A, pos_B)\n",
        "\n",
        "    for step in range(8):\n",
        "        # Calculate state index from current positions\n",
        "        idx = pos_A[0]*64 + pos_A[1]*16 + pos_B[0]*4 + pos_B[1]\n",
        "\n",
        "        # Choose best actions\n",
        "        act_A = np.argmax(qa[idx])\n",
        "        act_B = np.argmax(qb[idx])\n",
        "\n",
        "        move_map = {0:'UP', 1:'DOWN', 2:'LEFT', 3:'RIGHT'}\n",
        "        print(f\"Step {step+1}: Agent A goes {move_map[act_A]}, Agent B goes {move_map[act_B]}\")\n",
        "\n",
        "        # Execute (using internal env logic manually to show steps)\n",
        "        # Note: In test, we assume they learned to avoid collision\n",
        "        new_state, _, _, _ = env.step(act_A, act_B)\n",
        "\n",
        "        # Extract positions from tuple (rA, cA, rB, cB)\n",
        "        pos_A = (new_state[0], new_state[1])\n",
        "        pos_B = (new_state[2], new_state[3])\n",
        "\n",
        "        print_grid(pos_A, pos_B)\n",
        "\n",
        "        if pos_A == (3,3) and pos_B == (0,3):\n",
        "            print(\"Both Agents Reached Goals!\")\n",
        "            break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hGKw31pAM6DH",
        "outputId": "9bc15f2b-83c5-45e7-d4a7-14a108c0db11"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Multi-Agent System (Agents A & B)...\n",
            "\n",
            "--- Testing MARL Interaction ---\n",
            "Agent A: (0,0) -> (3,3)\n",
            "Agent B: (3,0) -> (0,3)\n",
            " A  .  .  . \n",
            " .  .  .  . \n",
            " .  .  .  . \n",
            " B  .  .  . \n",
            "------------\n",
            "Step 1: Agent A goes DOWN, Agent B goes RIGHT\n",
            " .  .  .  . \n",
            " A  .  .  . \n",
            " .  .  .  . \n",
            " .  B  .  . \n",
            "------------\n",
            "Step 2: Agent A goes RIGHT, Agent B goes UP\n",
            " .  .  .  . \n",
            " .  A  .  . \n",
            " .  B  .  . \n",
            " .  .  .  . \n",
            "------------\n",
            "Step 3: Agent A goes DOWN, Agent B goes RIGHT\n",
            " .  .  .  . \n",
            " .  .  .  . \n",
            " .  A  B  . \n",
            " .  .  .  . \n",
            "------------\n",
            "Step 4: Agent A goes RIGHT, Agent B goes RIGHT\n",
            " .  .  .  . \n",
            " .  .  .  . \n",
            " .  .  A  B \n",
            " .  .  .  . \n",
            "------------\n",
            "Step 5: Agent A goes RIGHT, Agent B goes UP\n",
            " .  .  .  . \n",
            " .  .  .  B \n",
            " .  .  .  A \n",
            " .  .  .  . \n",
            "------------\n",
            "Step 6: Agent A goes DOWN, Agent B goes UP\n",
            " .  .  .  . \n",
            " .  .  .  B \n",
            " .  .  .  A \n",
            " .  .  .  . \n",
            "------------\n",
            "Step 7: Agent A goes DOWN, Agent B goes UP\n",
            " .  .  .  . \n",
            " .  .  .  B \n",
            " .  .  .  A \n",
            " .  .  .  . \n",
            "------------\n",
            "Step 8: Agent A goes DOWN, Agent B goes UP\n",
            " .  .  .  . \n",
            " .  .  .  B \n",
            " .  .  .  A \n",
            " .  .  .  . \n",
            "------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CLcYhAOPM7Q7"
      },
      "execution_count": 13,
      "outputs": []
    }
  ]
}