{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "THZFuR5x7SFN",
        "outputId": "4bde4a2e-5aa6-482c-a3b9-da5da86d2523"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Value Iteration...\n",
            "Converged after 4 iterations.\n",
            "\n",
            "Optimal Value Function (V*):\n",
            "[[ 0. -1. -2. -3.]\n",
            " [-1. -2. -3. -2.]\n",
            " [-2. -3. -2. -1.]\n",
            " [-3. -2. -1.  0.]]\n",
            "\n",
            "Optimal Policy (Planning Result):\n",
            "-----------------\n",
            " T | ← | ← | ↓ \n",
            "-----------------\n",
            " ↑ | ↑ | ↑ | ↓ \n",
            "-----------------\n",
            " ↑ | ↑ | ↓ | ↓ \n",
            "-----------------\n",
            " ↑ | → | → | T \n",
            "-----------------\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "def value_iteration_grid_world():\n",
        "    # --- 1. SETUP THE ENVIRONMENT ---\n",
        "    grid_size = 4\n",
        "    gamma = 1.0  # Discount factor (1.0 for shortest path)\n",
        "    theta = 1e-4 # Convergence threshold\n",
        "\n",
        "    # Terminal states (Top-Left and Bottom-Right)\n",
        "    terminal_states = [0, 15]\n",
        "    actions = ['UP', 'DOWN', 'LEFT', 'RIGHT']\n",
        "\n",
        "    # Initialize Value Function V(s) to zeros\n",
        "    V = np.zeros(grid_size * grid_size)\n",
        "\n",
        "    print(\"Starting Value Iteration...\")\n",
        "    iteration = 0\n",
        "\n",
        "    # --- 2. THE ALGORITHM (Value Iteration) ---\n",
        "    while True:\n",
        "        delta = 0\n",
        "        # Create a copy for synchronous updates\n",
        "        V_new = np.copy(V)\n",
        "\n",
        "        # Loop over every state in the world\n",
        "        for s in range(grid_size * grid_size):\n",
        "            if s in terminal_states:\n",
        "                continue # Value of terminal state is always 0\n",
        "\n",
        "            # Calculate coordinates\n",
        "            row, col = divmod(s, grid_size)\n",
        "\n",
        "            # Find the max value among all possible actions\n",
        "            action_values = []\n",
        "\n",
        "            for action in actions:\n",
        "                # --- MODEL LOGIC (Simulating the move) ---\n",
        "                next_r, next_c = row, col\n",
        "\n",
        "                if action == 'UP':    next_r = max(row - 1, 0)\n",
        "                elif action == 'DOWN':  next_r = min(row + 1, grid_size - 1)\n",
        "                elif action == 'LEFT':  next_c = max(col - 1, 0)\n",
        "                elif action == 'RIGHT': next_c = min(col + 1, grid_size - 1)\n",
        "\n",
        "                next_state = next_r * grid_size + next_c\n",
        "\n",
        "                # Standard Reward is -1 per step\n",
        "                reward = -1\n",
        "\n",
        "                # Bellman Optimality Equation: R + gamma * V(s')\n",
        "                val = reward + gamma * V[next_state]\n",
        "                action_values.append(val)\n",
        "\n",
        "            # Update V(s) with the BEST possible action (Greedy)\n",
        "            best_value = max(action_values)\n",
        "            V_new[s] = best_value\n",
        "\n",
        "            # Check how much the value changed\n",
        "            delta = max(delta, abs(best_value - V[s]))\n",
        "\n",
        "        V = V_new\n",
        "        iteration += 1\n",
        "\n",
        "        # Stop if converged\n",
        "        if delta < theta:\n",
        "            print(f\"Converged after {iteration} iterations.\")\n",
        "            break\n",
        "\n",
        "    # --- 3. DISPLAY RESULTS ---\n",
        "    print(\"\\nOptimal Value Function (V*):\")\n",
        "    print(np.round(V.reshape(4, 4), 1))\n",
        "\n",
        "    # Optional: Display the Optimal Policy (Arrows)\n",
        "    print(\"\\nOptimal Policy (Planning Result):\")\n",
        "    arrows = {0: '↑', 1: '↓', 2: '←', 3: '→'}\n",
        "    policy_grid = []\n",
        "\n",
        "    for s in range(grid_size * grid_size):\n",
        "        if s in terminal_states:\n",
        "            policy_grid.append(\" T \")\n",
        "            continue\n",
        "\n",
        "        row, col = divmod(s, grid_size)\n",
        "        best_action_idx = -1\n",
        "        best_val = -float('inf')\n",
        "\n",
        "        # Check neighbors again to find which one gave that best value\n",
        "        for i, action in enumerate(actions):\n",
        "            next_r, next_c = row, col\n",
        "            if action == 'UP':    next_r = max(row - 1, 0)\n",
        "            elif action == 'DOWN':  next_r = min(row + 1, grid_size - 1)\n",
        "            elif action == 'LEFT':  next_c = max(col - 1, 0)\n",
        "            elif action == 'RIGHT': next_c = min(col + 1, grid_size - 1)\n",
        "\n",
        "            val = -1 + gamma * V[next_r * grid_size + next_c]\n",
        "            if val > best_val:\n",
        "                best_val = val\n",
        "                best_action_idx = i\n",
        "\n",
        "        policy_grid.append(f\" {arrows[best_action_idx]} \")\n",
        "\n",
        "    # Print Policy Grid\n",
        "    print(\"-\" * 17)\n",
        "    for i in range(0, 16, 4):\n",
        "        print(\"|\".join(policy_grid[i:i+4]))\n",
        "        print(\"-\" * 17)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    value_iteration_grid_world()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a601a1e0"
      },
      "source": [
        "This Python code implements the **Value Iteration algorithm** to find the optimal value function and policy for a simple **Grid World** environment, a classic problem in Reinforcement Learning.\n",
        "\n",
        "Here's a breakdown of the code:\n",
        "\n",
        "1.  **Environment Setup (`--- 1. SETUP THE ENVIRONMENT ---`)**:\n",
        "    *   `grid_size = 4`: Defines a 4x4 grid, resulting in 16 possible states (0 to 15).\n",
        "    *   `gamma = 1.0`: This is the **discount factor**. A value of 1.0 means future rewards are valued equally to immediate rewards, which is typical for shortest path problems where the goal is to reach a terminal state with minimum steps.\n",
        "    *   `theta = 1e-4`: The **convergence threshold**. The algorithm stops when the maximum change in the value function (`delta`) between iterations falls below this small value, indicating that the values have stabilized.\n",
        "    *   `terminal_states = [0, 15]`: These are the states (top-left and bottom-right corners) where the agent receives a reward and the episode ends. Their value is fixed at 0.\n",
        "    *   `actions = ['UP', 'DOWN', 'LEFT', 'RIGHT']`: The possible actions an agent can take from any non-terminal state.\n",
        "    *   `V = np.zeros(grid_size * grid_size)`: Initializes the **value function** `V(s)` for all states `s` to zero. This `V` array will eventually store the maximum expected cumulative reward for reaching a terminal state from each state, following an optimal policy.\n",
        "\n",
        "2.  **The Algorithm (Value Iteration) (`--- 2. THE ALGORITHM (Value Iteration) ---`)**:\n",
        "    *   The `while True:` loop continues iteratively until the value function converges.\n",
        "    *   `delta = 0`: This variable tracks the largest change in any state's value during an iteration. It's used to check for convergence.\n",
        "    *   `V_new = np.copy(V)`: A copy of the current value function is made for synchronous updates. This means that all calculations within an iteration use the values from the *previous* iteration, preventing a state update from immediately affecting the value calculation for another state in the same iteration.\n",
        "    *   **State Iteration**: The code iterates through each state `s` in the grid.\n",
        "        *   If `s` is a `terminal_state`, its value is already 0, so it's skipped.\n",
        "        *   For non-terminal states, it calculates `row` and `col` coordinates from the state index.\n",
        "    *   **Action Value Calculation**: For each state, it considers all possible actions (`UP`, `DOWN`, `LEFT`, `RIGHT`).\n",
        "        *   `next_r, next_c`: Calculates the coordinates of the next state after taking an action. Boundary conditions (e.g., hitting a wall) are handled by `max(row - 1, 0)` or `min(row + 1, grid_size - 1)` to keep the agent within the grid.\n",
        "        *   `next_state`: Converts the `next_r, next_c` back to a single state index.\n",
        "        *   `reward = -1`: A standard setup where each step taken incurs a cost (negative reward) of -1. The goal is often to minimize steps to a terminal state.\n",
        "        *   **Bellman Optimality Equation**: `val = reward + gamma * V[next_state]` calculates the value of taking a specific action from the current state. This equation is central to Value Iteration: the value of a state via a specific action is the immediate reward received plus the discounted value of the resulting next state.\n",
        "        *   `action_values.append(val)`: Stores the calculated value for each possible action.\n",
        "    *   **Value Update**: `best_value = max(action_values)`: The value of the current state `s` is updated to the maximum possible value achievable by taking the *best* action from that state (`V_new[s] = best_value`). This greedy choice ensures that the algorithm moves towards an optimal policy.\n",
        "    *   **Convergence Check**: `delta = max(delta, abs(best_value - V[s]))`: `delta` is updated to be the maximum absolute change in the value of any state during the current iteration. If `delta` falls below `theta`, it signifies that the value function has stabilized sufficiently, and the loop breaks.\n",
        "    *   `V = V_new`: The updated values become the current values for the next iteration.\n",
        "\n",
        "3.  **Display Results (`--- 3. DISPLAY RESULTS ---`)**:\n",
        "    *   **Optimal Value Function (V*)**: After convergence, the final `V` array (which represents `V*`, the optimal value function) is printed, reshaped into the 4x4 grid format. Each number indicates the maximum expected reward (or minimum cost, as rewards are negative) for reaching a terminal state from that particular cell, assuming optimal actions are taken.\n",
        "    *   **Optimal Policy (Planning Result)**:\n",
        "        *   The optimal policy dictates which action to take in each state. It's derived directly from the converged `V*`.\n",
        "        *   For each non-terminal state, the code again considers all possible actions. It calculates the value of taking each action (`-1 + gamma * V[next_state]`) and selects the action that leads to the highest value. This chosen action is the `best_action_idx`.\n",
        "        *   `arrows`: A dictionary maps numeric action indices to directional arrow symbols (↑, ↓, ←, →).\n",
        "        *   `policy_grid`: A list is constructed, where each element is an arrow representing the optimal action for that state, or ' T ' for terminal states.\n",
        "        *   Finally, the `policy_grid` is printed in a 4x4 grid format, visually showing the optimal direction to move from each state to follow the optimal path to a terminal state."
      ]
    }
  ]
}